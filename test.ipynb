{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import langchain\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc(directory):\n",
    "    file_loader=PyPDFDirectoryLoader(directory)\n",
    "    documents=file_loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=read_doc('documents/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(docs,chunk_size=800,chunk_overlap=50):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "    docs=text_splitter.split_documents(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Antti Laaksonen\\nGuide to Competitive\\nProgramming\\n123Learning and Improving Algorithms\\nThrough Contests', metadata={'source': 'documents\\\\doc.pdf', 'page': 0}),\n",
       " Document(page_content='Antti Laaksonen\\nDepartment of Computer Science\\nUniversity of Helsinki\\nHelsinki\\nFinland\\nISSN 1863-7310 ISSN 2197-1781 (electronic)\\nUndergraduate Topics in Computer Science\\nISBN 978-3-319-72546-8 ISBN 978-3-319-72547-5 (eBook)\\nhttps://doi.org/10.1007/978-3-319-72547-5\\nLibrary of Congress Control Number: 2017960923\\n©Springer International Publishing AG, part of Springer Nature 2017', metadata={'source': 'documents\\\\doc.pdf', 'page': 1}),\n",
       " Document(page_content='Preface\\nThe purpose of this book is to give you a comprehensive introduction to modern\\ncompetitive programming. It is assumed that you already know the basics of pro-\\ngramming, but previous background in algorithm design or programming contests\\nis not necessary. Since the book covers a wide range of topics of various dif ﬁculty,\\nit suits both for beginners and more experienced readers.\\nProgramming contests already have a quite long history. The International\\nCollegiate Programming Contest for university students was started during the\\n1970s, and the ﬁrstInternational Olympiad in Informatics for secondary school\\nstudents was organized in 1989. Both competitions are now established events with\\na large number of participants from all around the world.\\nToday, competitive programming is more popular than ever. The Internet has\\nplayed a signi ﬁcant role in this progress. There is now an active online community\\nof competitive programmers, and many contests are organized every week. At the\\nsame time, the dif ﬁculty of contests is increasing. Techniques that only the very best\\nparticipants mastered some years ago are now standard tools known by a large\\nnumber of people.\\nCompetitive programming has its roots in the scienti ﬁc study of algorithms.\\nHowever, while a computer scientist writes a proof to show that their algorithm\\nworks, a competitive programmer implements their algorithm and submits it to a\\ncontest system. Then, the algorithm is tested using a set of test cases, and if it passes\\nall of them, it is accepted. This is an essential element in competitive programming,\\nbecause it provides a way to automatically get strong evidence that an algorithm\\nworks. In fact, competitive programming has proved to be an excellent way to learn\\nalgorithms, because it encourages to design algorithms that really work, instead of\\nsketching ideas that may work or not.\\nAnother bene ﬁt of competitive programming is that contest problems require\\nthinking . In particular, there are no spoilers in problem statements. This is actually a\\nsevere problem in many algorithms courses. You are given a nice problem to solve,\\nbut then the last sentence says, for example: “Hint: modify Dijkstra ’s algorithm to\\nsolve the problem. ”After reading this, there is not much thinking needed, because\\nyou already know how to solve the problem. This never happens in competitive', metadata={'source': 'documents\\\\doc.pdf', 'page': 2}),\n",
       " Document(page_content='programming. Instead, you have a full set of tools available, and you have to ﬁgure\\noutyourself which of them to use.\\nSolving competitive programming problems also improves one ’s programming\\nand debugging skills. Typically, a solution is awarded points only if it correctly\\nsolves all test cases, so a successful competitive programmer has to be able to\\nimplement programs that do not have bugs. This is a valuable skill in software\\nengineering, and it is not a coincidence that IT companies are interested in people\\nwho have background in competitive programming.\\nIt takes a long time to become a good competitive programmer, but it is also an\\nopportunity to learn a lot. You can be sure that you will get a good general\\nunderstanding of algorithms if you spend time reading the book, solving problems,\\nand taking part in contests.\\nIf you have any feedback, I would like to hear it! You can always send me a\\nmessage to ahslaaks@cs.helsinki. ﬁ.\\nI am very grateful to a large number of people who have sent me feedback on\\ndraft versions of this book. This feedback has greatly improved the quality of the\\nbook. I especially thank Mikko Ervasti, Janne Junnila, Janne Kokkala, Tuukka\\nKorhonen, Patric Östergård, and Roope Salmi for giving detailed feedback on the\\nmanuscript. I also thank Simon Rees and Wayne Wheeler for excellent collabo-\\nration when publishing this book with Springer.\\nHelsinki, Finland Antti Laaksonen\\nOctober 2017', metadata={'source': 'documents\\\\doc.pdf', 'page': 3}),\n",
       " Document(page_content='Contents\\n1 Introduction ............................................. 1\\n1.1 What is Competitive Programming? ...................... 1\\n1.1.1 Programming Contests .......................... 2\\n1.1.2 Tips for Practicing ............................. 3\\n1.2 About This Book .................................... 3\\n1.3 CSES Problem Set ................................... 5\\n1.4 Other Resources ..................................... 7\\n2 Programming Techniques .................................. 9\\n2.1 Language Features ................................... 9\\n2.1.1 Input and Output .............................. 10\\n2.1.2 Working with Numbers ......................... 12\\n2.1.3 Shortening Code .............................. 14\\n2.2 Recursive Algorithms ................................. 15\\n2.2.1 Generating Subsets ............................ 15\\n2.2.2 Generating Permutations ........................ 16\\n2.2.3 Backtracking ................................. 18\\n2.3 Bit Manipulation ..................................... 20\\n2.3.1 Bit Operations ................................ 21\\n2.3.2 Representing Sets ............................. 23\\n3E f ﬁciency ............................................... 27\\n3.1 Time Complexity .................................... 27\\n3.1.1 Calculation Rules ............................. 27\\n3.1.2 Common Time Complexities ..................... 30\\n3.1.3 Estimating Ef ﬁciency ........................... 31\\n3.1.4 Formal De ﬁnitions ............................. 32\\n3.2 Examples .......................................... 32\\n3.2.1 Maximum Subarray Sum ........................ 32\\n3.2.2 Two Queens Problem .......................... 35\\n4 Sorting and Searching ..................................... 37\\n4.1 Sorting Algorithms ................................... 37\\n4.1.1 Bubble Sort .................................. 38', metadata={'source': 'documents\\\\doc.pdf', 'page': 4}),\n",
       " Document(page_content='4.1.2 Merge Sort .................................. 39\\n4.1.3 Sorting Lower Bound .......................... 40\\n4.1.4 Counting Sort ................................ 41\\n4.1.5 Sorting in Practice ............................. 41\\n4.2 Solving Problems by Sorting ........................... 43\\n4.2.1 Sweep Line Algorithms ......................... 44\\n4.2.2 Scheduling Events ............................. 45\\n4.2.3 Tasks and Deadlines ........................... 45\\n4.3 Binary Search ....................................... 46\\n4.3.1 Implementing the Search ........................ 47\\n4.3.2 Finding Optimal Solutions ....................... 48\\n5 Data Structures .......................................... 51\\n5.1 Dynamic Arrays ..................................... 51\\n5.1.1 Vectors ..................................... 52\\n5.1.2 Iterators and Ranges ........................... 53\\n5.1.3 Other Structures ............................... 54\\n5.2 Set Structures ....................................... 55\\n5.2.1 Sets and Multisets ............................. 55\\n5.2.2 Maps ....................................... 57\\n5.2.3 Priority Queues ............................... 58\\n5.2.4 Policy-Based Sets ............................. 59\\n5.3 Experiments ........................................ 60\\n5.3.1 Set Versus Sorting ............................. 60\\n5.3.2 Map Versus Array ............................. 61\\n5.3.3 Priority Queue Versus Multiset ................... 62\\n6 Dynamic Programming .................................... 63\\n6.1 Basic Concepts ...................................... 63\\n6.1.1 When Greedy Fails ............................ 63\\n6.1.2 Finding an Optimal Solution ..................... 64\\n6.1.3 Counting Solutions ............................ 67\\n6.2 Further Examples .................................... 68\\n6.2.1 Longest Increasing Subsequence .................. 69\\n6.2.2 Paths in a Grid ............................... 70\\n6.2.3 Knapsack Problems ............................ 71\\n6.2.4 From Permutations to Subsets .................... 72\\n6.2.5 Counting Tilings .............................. 74\\n7 Graph Algorithms ........................................ 77\\n7.1 Basics of Graphs ..................................... 78\\n7.1.1 Graph Terminology ............................ 78\\n7.1.2 Graph Representation .......................... 80\\n7.2 Graph Traversal ..................................... 83\\n7.2.1 Depth-First Search ............................. 83', metadata={'source': 'documents\\\\doc.pdf', 'page': 5}),\n",
       " Document(page_content='7.2.2 Breadth-First Search ........................... 85\\n7.2.3 Applications ................................. 86\\n7.3 Shortest Paths ....................................... 87\\n7.3.1 Bellman –Ford Algorithm ........................ 88\\n7.3.2 Dijkstra ’s Algorithm ........................... 89\\n7.3.3 Floyd –Warshall Algorithm ...................... 92\\n7.4 Directed Acyclic Graphs ............................... 94\\n7.4.1 Topological Sorting ............................ 94\\n7.4.2 Dynamic Programming ......................... 96\\n7.5 Successor Graphs .................................... 97\\n7.5.1 Finding Successors ............................ 98\\n7.5.2 Cycle Detection ............................... 99\\n7.6 Minimum Spanning Trees .............................. 100\\n7.6.1 Kruskal ’s Algorithm ........................... 101\\n7.6.2 Union-Find Structure ........................... 103\\n7.6.3 Prim ’s Algorithm .............................. 106\\n8 Algorithm Design Topics ................................... 107\\n8.1 Bit-Parallel Algorithms ................................ 107\\n8.1.1 Hamming Distances ............................ 107\\n8.1.2 Counting Subgrids ............................. 108\\n8.1.3 Reachability in Graphs ......................... 110\\n8.2 Amortized Analysis .................................. 111\\n8.2.1 Two Pointers Method .......................... 111\\n8.2.2 Nearest Smaller Elements ....................... 113\\n8.2.3 Sliding Window Minimum ...................... 114\\n8.3 Finding Minimum Values .............................. 115\\n8.3.1 Ternary Search ............................... 115\\n8.3.2 Convex Functions ............................. 116\\n8.3.3 Minimizing Sums ............................. 117\\n9 Range Queries ........................................... 119\\n9.1 Queries on Static Arrays ............................... 119\\n9.1.1 Sum Queries ................................. 120\\n9.1.2 Minimum Queries ............................. 121\\n9.2 Tree Structures ...................................... 122\\n9.2.1 Binary Indexed Trees .......................... 122\\n9.2.2 Segment Trees ................................ 125\\n9.2.3 Additional Techniques .......................... 128\\n10 Tree Algorithms ......................................... 131\\n10.1 Basic Techniques .................................... 131\\n10.1.1 Tree Traversal ................................ 132\\n10.1.2 Calculating Diameters .......................... 134\\n10.1.3 All Longest Paths ............................. 135', metadata={'source': 'documents\\\\doc.pdf', 'page': 6}),\n",
       " Document(page_content='10.2 Tree Queries ........................................ 137\\n10.2.1 Finding Ancestors ............................. 137\\n10.2.2 Subtrees and Paths ............................. 138\\n10.2.3 Lowest Common Ancestors ...................... 140\\n10.2.4 Merging Data Structures ........................ 142\\n10.3 Advanced Techniques ................................. 144\\n10.3.1 Centroid Decomposition ........................ 144\\n10.3.2 Heavy-Light Decomposition ..................... 145\\n11 Mathematics ............................................. 147\\n11.1 Number Theory ..................................... 147\\n11.1.1 Primes and Factors ............................ 148\\n11.1.2 Sieve of Eratosthenes .......................... 150\\n11.1.3 Euclid ’s Algorithm ............................ 151\\n11.1.4 Modular Exponentiation ........................ 153\\n11.1.5 Euler ’s Theorem .............................. 153\\n11.1.6 Solving Equations ............................. 155\\n11.2 Combinatorics ....................................... 156\\n11.2.1 Binomial Coef ﬁcients .......................... 157\\n11.2.2 Catalan Numbers .............................. 159\\n11.2.3 Inclusion-Exclusion ............................ 161\\n11.2.4 Burnside ’s Lemma ............................. 163\\n11.2.5 Cayley ’s Formula ............................. 164\\n11.3 Matrices ........................................... 164\\n11.3.1 Matrix Operations ............................. 165\\n11.3.2 Linear Recurrences ............................ 167\\n11.3.3 Graphs and Matrices ........................... 169\\n11.3.4 Gaussian Elimination ........................... 170\\n11.4 Probability ......................................... 173\\n11.4.1 Working with Events ........................... 174\\n11.4.2 Random Variables ............................. 175\\n11.4.3 Markov Chains ............................... 178\\n11.4.4 Randomized Algorithms ........................ 179\\n11.5 Game Theory ....................................... 181\\n11.5.1 Game States ................................. 181\\n11.5.2 Nim Game ................................... 182\\n11.5.3 Sprague –Grundy Theorem ....................... 184\\n12 Advanced Graph Algorithms ............................... 189\\n12.1 Strong Connectivity .................................. 189\\n12.1.1 Kosaraju ’s Algorithm .......................... 190\\n12.1.2 2SAT Problem ................................ 192\\n12.2 Complete Paths ...................................... 193\\n12.2.1 Eulerian Paths ................................ 194', metadata={'source': 'documents\\\\doc.pdf', 'page': 7}),\n",
       " Document(page_content='12.2.2 Hamiltonian Paths ............................. 195\\n12.2.3 Applications ................................. 196\\n12.3 Maximum Flows ..................................... 198\\n12.3.1 Ford –Fulkerson Algorithm ....................... 199\\n12.3.2 Disjoint Paths ................................ 202\\n12.3.3 Maximum Matchings ........................... 203\\n12.3.4 Path Covers .................................. 205\\n12.4 Depth-First Search Trees .............................. 207\\n12.4.1 Biconnectivity ................................ 207\\n12.4.2 Eulerian Subgraphs ............................ 209\\n13 Geometry ............................................... 211\\n13.1 Geometric Techniques ................................ 211\\n13.1.1 Complex Numbers ............................. 211\\n13.1.2 Points and Lines .............................. 213\\n13.1.3 Polygon Area ................................ 216\\n13.1.4 Distance Functions ............................ 218\\n13.2 Sweep Line Algorithms ............................... 220\\n13.2.1 Intersection Points ............................. 220\\n13.2.2 Closest Pair Problem ........................... 221\\n13.2.3 Convex Hull Problem .......................... 224\\n14 String Algorithms ........................................ 225\\n14.1 Basic Topics ........................................ 225\\n14.1.1 Trie Structure ................................ 226\\n14.1.2 Dynamic Programming ......................... 227\\n14.2 String Hashing ...................................... 228\\n14.2.1 Polynomial Hashing ........................... 228\\n14.2.2 Applications ................................. 229\\n14.2.3 Collisions and Parameters ....................... 230\\n14.3 Z-Algorithm ........................................ 231\\n14.3.1 Constructing the Z-Array ........................ 232\\n14.3.2 Applications ................................. 233\\n14.4 Suf ﬁx Arrays ....................................... 234\\n14.4.1 Pre ﬁx Doubling Method ........................ 235\\n14.4.2 Finding Patterns ............................... 236\\n14.4.3 LCP Arrays .................................. 236\\n15 Additional Topics ........................................ 239\\n15.1 Square Root Techniques ............................... 239\\n15.1.1 Data Structures ............................... 240\\n15.1.2 Subalgorithms ................................ 241\\n15.1.3 Integer Partitions .............................. 243\\n15.1.4 Mo ’s Algorithm ............................... 244', metadata={'source': 'documents\\\\doc.pdf', 'page': 8}),\n",
       " Document(page_content='15.2 Segment Trees Revisited ............................... 245\\n15.2.1 Lazy Propagation .............................. 246\\n15.2.2 Dynamic Trees ............................... 249\\n15.2.3 Data Structures in Nodes ........................ 251\\n15.2.4 Two-Dimensional Trees ......................... 253\\n15.3 Treaps ............................................. 253\\n15.3.1 Splitting and Merging .......................... 253\\n15.3.2 Implementation ............................... 255\\n15.3.3 Additional Techniques .......................... 257\\n15.4 Dynamic Programming Optimization ..................... 258\\n15.4.1 Convex Hull Trick ............................. 258\\n15.4.2 Divide and Conquer Optimization ................. 260\\n15.4.3 Knuth ’s Optimization .......................... 261\\n15.5 Miscellaneous ....................................... 262\\n15.5.1 Meet in the Middle ............................ 263\\n15.5.2 Counting Subsets .............................. 263\\n15.5.3 Parallel Binary Search .......................... 265\\n15.5.4 Dynamic Connectivity .......................... 266\\nAppendix A: Mathematical Background .......................... 269\\nReferences .................................................. 277\\nIndex ...................................................... 279', metadata={'source': 'documents\\\\doc.pdf', 'page': 9}),\n",
       " Document(page_content='Introduction\\nThis chapter shows what competitive programming is about, outlines the contents of\\nthe book, and discusses additional learning resources.\\nSection 1.1goes through the elements of competitive programming, introduces\\na selection of popular programming contests, and gives advice on how to practice\\ncompetitive programming.\\nSection 1.2discusses the goals and topics of this book, and brieﬂy describes the\\ncontents of each chapter.\\nSection 1.3presents the CSES Problem Set, which contains a collection of practice\\nproblems. Solving the problems while reading the book is a good way to learn\\ncompetitive programming.\\nSection 1.4 discusses other books related to competitive programming and the\\ndesign of algorithms.\\n1.1 What is Competitive Programming?\\nCompetitive programming combines two topics: the design of algorithms and the\\nimplementation of algorithms.\\nDesign of Algorithms The core of competitive programming is about inventing\\nefﬁcient algorithms that solve well-deﬁned computational problems. The design of\\nalgorithms requires problem solving and mathematical skills. Often a solution to a\\nproblem is a combination of well-known methods and new insights.\\nMathematics plays an important role in competitive programming. Actually, there\\nare no clear boundaries between algorithm design and mathematics. This book has\\nbeen written so that not much background in mathematics is needed. The appendix\\nof the book reviews some mathematical concepts that are used throughout the book,', metadata={'source': 'documents\\\\doc.pdf', 'page': 10}),\n",
       " Document(page_content='2 1 Introduction\\nsuch as sets, logic, and functions, and the appendix can be used as a reference when\\nreading the book.\\nImplementation of Algorithms In competitive programming, the solutions to prob-\\nlems are evaluated by testing an implemented algorithm using a set of test cases.\\nThus, after coming up with an algorithm that solves the problem, the next step is to\\ncorrectly implement it, which requires good programming skills. Competitive pro-\\ngramming greatly differs from traditional software engineering: programs are short\\n(usually at most some hundreds of lines), they should be written quickly, and it is\\nnot needed to maintain them after the contest.\\nAt the moment, the most popular programming languages used in contests are\\nC++, Python, and Java. For example, in Google Code Jam 2017, among the best\\n3,000 participants, 79% used C++, 16% used Python, and 8% used Java. Many\\npeople regard C++ as the best choice for a competitive programmer. The beneﬁts of\\nusing C++ are that it is a very efﬁcient language and its standard library contains a\\nlarge collection of data structures and algorithms.\\nAll example programs in this book are written in C++, and the standard library’s\\ndata structures and algorithms are often used. The programs follow the C++11 stan-\\ndard, which can be used in most contests nowadays. If you cannot program in C++\\nyet, now is a good time to start learning.\\n1.1.1 Programming Contests\\nIOI The International Olympiad in Informatics is an annual programming contest for\\nsecondary school students. Each country is allowed to send a team of four students\\nto the contest. There are usually about 300 participants from 80 countries.\\nThe IOI consists of two ﬁve-hour long contests. In both contests, the participants\\nare asked to solve three difﬁcult programming tasks. The tasks are divided into\\nsubtasks, each of which has an assigned score. While the contestants are divided into\\nteams, they compete as individuals.\\nParticipants for the IOI are selected through national contests. Before the IOI,\\nmany regional contests are organized, such as the Baltic Olympiad in Informatics\\n(BOI), the Central European Olympiad in Informatics (CEOI), and the Asia-Paciﬁc\\nInformatics Olympiad (APIO).\\nICPC The International Collegiate Programming Contest is an annual programming\\ncontest for university students. Each team in the contest consists of three students,\\nand unlike in the IOI, the students work together; there is only one computer available\\nfor each team.\\nThe ICPC consists of several stages, and ﬁnally the best teams are invited to the\\nWorld Finals. While there are tens of thousands of participants in the contest, there\\nare only a small number1of ﬁnal slots available, so even advancing to the ﬁnals is a\\ngreat achievement.\\n1The exact number of ﬁnal slots varies from year to year; in 2017, there were 133 ﬁnal slots.', metadata={'source': 'documents\\\\doc.pdf', 'page': 11}),\n",
       " Document(page_content='1.1 What is Competitive Programming? 3\\nIn each ICPC contest, the teams have ﬁve hours of time to solve about ten algorithm\\nproblems. A solution to a problem is accepted only if it solves all test cases efﬁciently.\\nDuring the contest, competitors may view the results of other teams, but for the last\\nhour the scoreboard is frozen and it is not possible to see the results of the last\\nsubmissions.\\nOnline Contests There are also many online contests that are open for everybody.\\nAt the moment, the most active contest site is Codeforces, which organizes contests\\nabout weekly. Other popular contest sites include AtCoder, CodeChef, CS Academy,\\nHackerRank, and Topcoder.\\nSome companies organize online contests with onsite ﬁnals. Examples of such\\ncontests are Facebook Hacker Cup, Google Code Jam, and Yandex.Algorithm. Of\\ncourse, companies also use those contests for recruiting: performing well in a contest\\nis a good way to prove one’s skills in programming.\\n1.1.2 Tips for Practicing\\nLearning competitive programming requires a great amount of work. However, there\\nare many ways to practice, and some of them are better than others.\\nWhen solving problems, one should keep in mind that the number of solved\\nproblems is not so important that the quality of the problems. It is tempting to select\\nproblems that look nice and easy and solve them, and skip problems that look hard\\nand tedious. However, the way to really improve one’s skills is to focus on the latter\\ntype of problems.\\nAnother important observation is that most programming contest problems can\\nbe solved using simple and short algorithms, but the difﬁcult part is to invent the\\nalgorithm. Competitive programming is not about learning complex and obscure\\nalgorithms by heart, but rather about learning problem solving and ways to approach\\ndifﬁcult problems using simple tools.\\nFinally, some people despise the implementation of algorithms: it is fun to design\\nalgorithms but boring to implement them. However, the ability to quickly and cor-\\nrectly implement algorithms is an important asset, and this skill can be practiced. It\\nis a bad idea to spend most of the contest time for writing code and ﬁnding bugs,\\ninstead of thinking of how to solve problems.\\n1.2 About This Book\\nThe IOI Syllabus [15] regulates the topics that may appear at the International\\nOlympiad in Informatics, and the syllabus has been a starting point when select-\\ning topics for this book. However, the book also discusses some advanced topics that\\nare (as of 2017) excluded from the IOI but may appear in other contests. Examples\\nof such topics are maximum ﬂows, nim theory, and sufﬁx arrays.', metadata={'source': 'documents\\\\doc.pdf', 'page': 12}),\n",
       " Document(page_content='4 1 Introduction\\nWhile many competitive programming topics are discussed in standard algorithms\\ntextbooks, there are also differences. For example, many textbooks focus on imple-\\nmenting sorting algorithms and fundamental data structures from scratch, but this\\nknowledge is not very relevant in competitive programming, because standard li-\\nbrary functionality can be used. Then, there are topics that are well known in the\\ncompetitive programming community but rarely discussed in textbooks. An example\\nof such a topic is the segment tree data structure that can be used to solve a large\\nnumber of problems that would otherwise require tricky algorithms.\\nOne of the purposes of this book has been to document competitive programming\\ntechniques that are usually only discussed in online forums and blog posts. When-\\never possible, scientiﬁc references have been given for methods that are speciﬁc to\\ncompetitive programming. However, this has not often been possible, because many\\ntechniques are now part of competitive programming folklore and nobody knows\\nwho has originally discovered them.\\nThe structure of the book is as follows:\\n•Chapter 2reviews features of the C++ programming language, and then discusses\\nrecursive algorithms and bit manipulation.\\n•Chapter 3focuses on efﬁciency: how to create algorithms that can quickly process\\nlarge data sets.\\n•Chapter 4discusses sorting algorithms and binary search, focusing on their ap-\\nplications in algorithm design.\\n•Chapter 5goes through a selection of data structures of the C++ standard library,\\nsuch as vectors, sets, and maps.\\n•Chapter 6introduces an algorithm design technique called dynamic programming,\\nand presents examples of problems that can be solved using it.\\n•Chapter 7discusses elementary graph algorithms, such as ﬁnding shortest paths\\nand minimum spanning trees.\\n•Chapter 8deals with some advanced algorithm design topics, such as bit-\\nparallelism and amortized analysis.\\n•Chapter 9focuses on efﬁciently processing array range queries, such as calculating\\nsums of values and determining minimum values.\\n•Chapter 10presents specialized algorithms for trees, including methods for\\nprocessing tree queries.\\n•Chapter 11discusses mathematical topics that are relevant in competitive pro-\\ngramming.\\n•Chapter 12presents advanced graph techniques, such as strongly connected com-\\nponents and maximum ﬂows.\\n•Chapter 13focuses on geometric algorithms and presents techniques using which\\ngeometric problems can be solved conveniently.\\n•Chapter 14deals with string techniques, such as string hashing, the Z-algorithm,\\nand using sufﬁx arrays.\\n•Chapter 15discusses a selection of more advanced topics, such as square root\\nalgorithms and dynamic programming optimization.', metadata={'source': 'documents\\\\doc.pdf', 'page': 13}),\n",
       " Document(page_content='1.3 CSES Problem Set 5\\n1.3 CSES Problem Set\\nThe CSES Problem Set provides a collection of problems that can be used to practice\\ncompetitive programming. The problems have been arranged in order of difﬁculty,\\nand all techniques needed for solving the problems are discussed in this book. The\\nproblem set is available at the following address:\\nhttps://cses.ﬁ/problemset/\\nLet us see how to solve the ﬁrst problem in the problem set, called W eird Algorithm .\\nThe problem statement is as follows:\\nConsider an algorithm that takes as input a positive integer n.I f nis even, the algorithm\\ndivides it by two, and if nis odd, the algorithm multiplies it by three and adds one. The\\nalgorithm repeats this, until nis one. For example, the sequence for n=3 is as follows:\\n3→10→5→16→8→4→2→1\\nY our task is to simulate the execution of the algorithm for a given value of n.\\nInput\\nThe only input line contains an integer n.\\nOutput\\nPrint a line that contains all values of nduring the algorithm.\\nConstraints\\n•1≤n≤106\\nExample\\nInput:\\n3\\nOutput:\\n31 051 68421\\nThis problem is connected to the famous Collatz conjecture which states that the\\nabove algorithm terminates for every value of n. However, nobody has been able to\\nprove it. In this problem, however, we know that the initial value of nwill be at most\\none million, which makes the problem much easier to solve.\\nThis problem is a simple simulation problem, which does not require much think-\\ning. Here is a possible way to solve the problem in C++:', metadata={'source': 'documents\\\\doc.pdf', 'page': 14}),\n",
       " Document(page_content='6 1 Introduction\\n#include <iostream>\\nusing namespace std;\\nint main() {\\nint n;\\ncin >> n;\\nwhile (true ){\\ncout << n << \" \";\\nif(n == 1) break ;\\nif(n%2 == 0) n /= 2;\\nelse n = n*3+1;\\n}\\ncout << \"\\\\n\";\\n}\\nThe code ﬁrst reads in the input number n, and then simulates the algorithm and\\nprints the value of nafter each step. It is easy to test that the algorithm correctly\\nhandles the example case n=3 given in the problem statement.\\nNow is time to submit the code to CSES. Then the code will be compiled and\\ntested using a set of test cases. For each test case, CSES will tell us whether our code\\npassed it or not, and we can also examine the input, the expected output, and the\\noutput produced by our code.\\nAfter testing our code, CSES gives the following report:\\ntest verdict time (s)\\n#1 ACCEPTED 0.06 / 1.00\\n#2 ACCEPTED 0.06 / 1.00\\n#3 ACCEPTED 0.07 / 1.00\\n#4 ACCEPTED 0.06 / 1.00\\n#5 ACCEPTED 0.06 / 1.00\\n#6 TIME LIMIT EXCEEDED – / 1.00\\n#7 TIME LIMIT EXCEEDED – / 1.00\\n#8 WRONG ANSWER 0.07 / 1.00\\n#9 TIME LIMIT EXCEEDED – / 1.00\\n#10 ACCEPTED 0.06 / 1.00\\nThis means that our code passed some of the test cases (ACCEPTED), was some-\\ntimes too slow (TIME LIMIT EXCEEDED), and also produced an incorrect output\\n(WRONG ANSWER). This is quite surprising!\\nThe ﬁrst test case that fails has n=138367. If we test our code locally using this\\ninput, it turns out that the code is indeed slow. In fact, it never terminates.\\nThe reason why our code fails is that ncan become quite large during the simula-\\ntion. In particular, it can become larger than the upper limit of an int variable. To', metadata={'source': 'documents\\\\doc.pdf', 'page': 15}),\n",
       " Document(page_content='1.3 CSES Problem Set 7\\nﬁx the problem, it sufﬁces to change our code so that the type of nislong long .\\nThen we will get the desired result:\\ntest verdict time (s)\\n#1 ACCEPTED 0.05 / 1.00\\n#2 ACCEPTED 0.06 / 1.00\\n#3 ACCEPTED 0.07 / 1.00\\n#4 ACCEPTED 0.06 / 1.00\\n#5 ACCEPTED 0.06 / 1.00\\n#6 ACCEPTED 0.05 / 1.00\\n#7 ACCEPTED 0.06 / 1.00\\n#8 ACCEPTED 0.05 / 1.00\\n#9 ACCEPTED 0.07 / 1.00\\n#10 ACCEPTED 0.06 / 1.00\\nAs this example shows, even very simple algorithms may contain subtle bugs.\\nCompetitive programming teaches how to write algorithms that really work.\\n1.4 Other Resources\\nBesides this book, there are already several other books on competitive programming.\\nSkiena’s and Revilla’s Programming Challenges [28] is a pioneering book in the ﬁeld\\npublished in 2003. A more recent book is Competitive Programming 3 [14] by Halim\\nand Halim. Both the above books are intended for readers with no background in\\ncompetitive programming.\\nLooking for a Challenge? [7] is an advanced book, which present a collection of\\ndifﬁcult problems from Polish programming contests. The most interesting feature\\nof the book is that it provides detailed analyses of how to solve the problems. The\\nbook is intended for experienced competitive programmers.\\nOf course, general algorithms books are also good reads for competitive program-\\nmers. The most comprehensive of them is Introduction to Algorithms [6] written by\\nCormen, Leiserson, Rivest, and Stein, also called the CLRS . This book is a good re-\\nsource if you want to check all details concerning an algorithm and how to rigorously\\nprove that it is correct.\\nKleinberg’s and Tardos’s Algorithm Design [19] focuses on algorithm design tech-\\nniques, and thoroughly discusses the divide and conquer method, greedy algorithms,\\ndynamic programming, and maximum ﬂow algorithms. Skiena’s The Algorithm De-\\nsign Manual [27] is a more practical book which includes a large catalogue of\\ncomputational problems and describes ways how to solve them.', metadata={'source': 'documents\\\\doc.pdf', 'page': 16}),\n",
       " Document(page_content='Programming T echniques\\nThis chapter presents some of the features of the C++ programming language that\\nare useful in competitive programming, and gives examples of how to use recursion\\nand bit operations in programming.\\nSection 2.1 discusses a selection of topics related to C++, including input and\\noutput methods, working with numbers, and how to shorten code.\\nSection 2.2 focuses on recursive algorithms. First we will learn an elegant way\\nto generate all subsets and permutations of a set using recursion. After this, we will\\nuse backtracking to count the number of ways to place nnon-attacking queens on\\nann×nchessboard.\\nSection 2.3 discusses the basics of bit operations and shows how to use them to\\nrepresent subsets of sets.\\n2.1 Language Features\\nA typical C++ code template for competitive programming looks like this:\\n#include <bits/stdc++.h>\\nusing namespace std;\\nint main() {\\n//solution comes here\\n}\\nThe#include line at the beginning of the code is a feature of the g++ compiler\\nthat allows us to include the entire standard library. Thus, it is not needed to separately', metadata={'source': 'documents\\\\doc.pdf', 'page': 17}),\n",
       " Document(page_content='10 2 Programming Techniques\\ninclude libraries such as iostream ,vector , and algorithm , but rather they\\nare available automatically.\\nTheusing line declares that the classes and functions of the standard library can\\nbe used directly in the code. Without the using line we would have to write, for\\nexample, std::cout , but now it sufﬁces to write cout .\\nThe code can be compiled using the following command:\\ng++ -std=c++11 -O2 -Wall test.cpp -o test\\nThis command produces a binary ﬁle test from the source code test.cpp . The\\ncompiler follows the C++11 standard ( -std=c++11 ), optimizes the code ( -O2 ),\\nand shows warnings about possible errors ( -Wall ).\\n2.1.1 Input and Output\\nIn most contests, standard streams are used for reading input and writing output. In\\nC++, the standard streams are cin for input and cout for output. Also C functions,\\nsuch as scanf andprintf , can be used.\\nThe input for the program usually consists of numbers and strings separated with\\nspaces and newlines. They can be read from the cin stream as follows:\\nint a, b;\\nstring x;\\nc i n> >a> >b> >x ;\\nThis kind of code always works, assuming that there is at least one space or\\nnewline between each element in the input. For example, the above code can read\\nboth the following inputs:\\n123 456 monkey\\n123 456\\nmonkey\\nThecout stream is used for output as follows:\\nint a = 123, b = 456;\\nstring x = \"monkey\";\\ncout << a < <\"\"< <b< <\"\"< <x< <\" \\\\ n \" ;\\nInput and output is sometimes a bottleneck in the program. The following lines\\nat the beginning of the code make input and output more efﬁcient:', metadata={'source': 'documents\\\\doc.pdf', 'page': 18}),\n",
       " Document(page_content='2.1 Language Features 11\\nios::sync_with_stdio(0);\\ncin.tie(0);\\nNote that the newline \"\\\\n\" works faster than endl , because endl always causes\\na ﬂush operation.\\nThe C functions scanf andprintf are an alternative to the C++ standard\\nstreams. They are usually slightly faster, but also more difﬁcult to use. The following\\ncode reads two integers from the input:\\nint a, b;\\nscanf(\"%d %d\", &a, &b);\\nThe following code prints two integers:\\nint a = 123, b = 456;\\nprintf(\"%d %d\\\\n\", a, b);\\nSometimes the program should read a whole input line, possibly containing spaces.\\nThis can be accomplished by using the getline function:\\nstring s;\\ngetline(cin, s);\\nIf the amount of data is unknown, the following loop is useful:\\nwhile (cin >> x) {\\n// code\\n}\\nThis loop reads elements from the input one after another, until there is no more\\ndata available in the input.\\nIn some contest systems, ﬁles are used for input and output. An easy solution for\\nthis is to write the code as usual using standard streams, but add the following lines\\nto the beginning of the code:\\nfreopen(\"input.txt\", \"r\", stdin);\\nfreopen(\"output.txt\", \"w\", stdout);\\nAfter this, the program reads the input from the ﬁle “input.txt” and writes the\\noutput to the ﬁle “output.txt”.', metadata={'source': 'documents\\\\doc.pdf', 'page': 19}),\n",
       " Document(page_content='12 2 Programming Techniques\\n2.1.2 Working with Numbers\\nIntegers The most used integer type in competitive programming is int , which is\\na 32-bit type1with a value range of −231...231−1 (about −2·109...2·109). If\\nthe type int is not enough, the 64-bit type long long can be used. It has a value\\nrange of −263...263−1 (about −9·1018...9·1018).\\nThe following code deﬁnes a long long variable:\\nlong long x = 123456789123456789LL;\\nThe sufﬁx LLmeans that the type of the number is long long .\\nA common mistake when using the type long long is that the type int is still\\nused somewhere in the code. For example, the following code contains a subtle error:\\nint a = 123456789;\\nlong long b = a*a;\\ncout << b << \"\\\\n\"; // -1757895751\\nEven though the variable bis of type long long , both numbers in the expression\\na*a are of type int , and the result is also of type int . Because of this, the variable\\nbwill have a wrong result. The problem can be solved by changing the type of ato\\nlong long or by changing the expression to (long long)a*a .\\nUsually contest problems are set so that the type long long is enough. Still, it\\nis good to know that the g++ compiler also provides a 128-bit type __int128_t\\nwith a value range of −2127...2127−1 (about −1038...1038). However, this type\\nis not available in all contest systems.\\nModular Arithmetic Sometimes, the answer to a problem is a very large number,\\nbut it is enough to output it “modulo m”, i.e., the remainder when the answer is\\ndivided by m(e.g., “modulo 109+7”). The idea is that even if the actual answer is\\nvery large, it sufﬁces to use the types int andlong long .\\nWe denote by xmod mthe remainder when xis divided by m. For example,\\n17 mod 5 =2, because 17 =3·5+2. An important property of remainders is that\\nthe following formulas hold:\\n(a+b)mod m=(amod m+bmod m)mod m\\n(a−b)mod m=(amod m−bmod m)mod m\\n(a·b)mod m=(amod m·bmod m)mod m\\nThus, we can take the remainder after every operation, and the numbers will never\\nbecome too large.\\n1In fact, the C++ standard does not exactly specify the sizes of the number types, and the bounds\\ndepend on the compiler and platform. The sizes given in this section are those you will very likely\\nsee when using modern systems.', metadata={'source': 'documents\\\\doc.pdf', 'page': 20}),\n",
       " Document(page_content='2.1 Language Features 13\\nFor example, the following code calculates n!, the factorial of n, modulo m:\\nlong long x=1 ;\\nfor (int i = 1; i <= n; i++) {\\nx = (x*i)%m;\\n}\\ncout << x << \"\\\\n\";\\nUsually we want the remainder to always be between 0 ...m−1. However, in C++\\nand other languages, the remainder of a negative number is either zero or negative.\\nAn easy way to make sure there are no negative remainders is to ﬁrst calculate the\\nremainder as usual and then add mif the result is negative:\\nx=x % m ;\\nif( x<0 )x+ =m ;\\nHowever, this is only needed when there are subtractions in the code, and the\\nremainder may become negative.\\nFloating Point Numbers In most competitive programming problems, it sufﬁces\\nto use integers, but sometimes ﬂoating point numbers are needed. The most useful\\nﬂoating point types in C++ are the 64-bit double and, as an extension in the g++\\ncompiler, the 80-bit long double . In most cases, double is enough, but long\\ndouble is more accurate.\\nThe required precision of the answer is usually given in the problem statement.\\nAn easy way to output the answer is to use the printf function and give the number\\nof decimal places in the formatting string. For example, the following code prints\\nthe value of xwith 9 decimal places:\\nprintf(\"%.9f\\\\n\", x);\\nA difﬁculty when using ﬂoating point numbers is that some numbers cannot be\\nrepresented accurately as ﬂoating point numbers, and there will be rounding errors.\\nFor example, in the following code, the value of xis slightly smaller than 1, while\\nthe correct value would be 1.\\ndouble x = 0.3*3+0.1;\\nprintf(\"%.20f\\\\n\", x); // 0.99999999999999988898\\nIt is risky to compare ﬂoating point numbers with the ==operator, because it is\\npossible that the values should be equal but they are not because of precision errors.\\nA better way to compare ﬂoating point numbers is to assume that two numbers are\\nequal if the difference between them is less than ε, where εis a small number. For\\nexample, in the following code ε=10−9:', metadata={'source': 'documents\\\\doc.pdf', 'page': 21}),\n",
       " Document(page_content='14 2 Programming Techniques\\nif(abs(a-b) < 1e-9) {\\n// a and b are equal\\n}\\nNote that while ﬂoating point numbers are inaccurate, integers up to a certain\\nlimit can still be represented accurately. For example, using double , it is possible\\nto accurately represent all integers whose absolute value is at most 253.\\n2.1.3 Shortening Code\\nType Names The command typedef can be used to give a short name to a data\\ntype. For example, the name long long is long, so we can deﬁne a short name\\nllas follows:\\ntypedef long long ll;\\nAfter this, the code\\nlong long a = 123456789;\\nlong long b = 987654321;\\ncout << a*b << \"\\\\n\";\\ncan be shortened as follows:\\nll a = 123456789;\\nll b = 987654321;\\ncout << a*b << \"\\\\n\";\\nThe command typedef can also be used with more complex types. For example,\\nthe following code gives the name vifor a vector of integers, and the name pifor\\na pair that contains two integers.\\ntypedef vector< int>v i ;\\ntypedef pair< int,int>p i ;\\nMacros Another way to shorten code is to deﬁne macros . A macro speciﬁes that\\ncertain strings in the code will be changed before the compilation. In C++, macros\\nare deﬁned using the #define keyword.\\nFor example, we can deﬁne the following macros:\\n#define F first\\n#define S second\\n#define PB push_back\\n#define MP make_pair', metadata={'source': 'documents\\\\doc.pdf', 'page': 22}),\n",
       " Document(page_content='2.1 Language Features 15\\nAfter this, the code\\nv.push_back(make_pair(y1,x1));\\nv.push_back(make_pair(y2,x2));\\nint d = v[i].first+v[i].second;\\ncan be shortened as follows:\\nv.PB(MP(y1,x1));\\nv.PB(MP(y2,x2));\\nint d = v[i].F+v[i].S;\\nA macro can also have parameters, which makes it possible to shorten loops and\\nother structures. For example, we can deﬁne the following macro:\\n#define REP(i,a,b) for (int i=a ;i< =b ;i + + )\\nAfter this, the code\\nfor (int i = 1; i <= n; i++) {\\nsearch(i);\\n}\\ncan be shortened as follows:\\nREP(i,1,n) {\\nsearch(i);\\n}\\n2.2 Recursive Algorithms\\nRecursion often provides an elegant way to implement an algorithm. In this section,\\nwe discuss recursive algorithms that systematically go through candidate solutions to\\na problem. First, we focus on generating subsets and permutations and then discuss\\nthe more general backtracking technique.\\n2.2.1 Generating Subsets\\nOur ﬁrst application of recursion is generating all subsets of a set of nelements. For\\nexample, the subsets of {1,2,3}are∅,{1},{2},{3},{1,2},{1,3},{2,3}, and{1,2,3}.\\nThe following recursive function search can be used to generate the subsets. The\\nfunction maintains a vector', metadata={'source': 'documents\\\\doc.pdf', 'page': 23}),\n",
       " Document(page_content='16 2 Programming Techniques\\nvector< int> subset;\\nthat will contain the elements of each subset. The search begins when the function\\nis called with parameter 1.\\nvoid search( int k) {\\nif(k == n+1) {\\n// process subset\\n}else {\\n// include k in the subset\\nsubset.push_back(k);\\nsearch(k+1);\\nsubset.pop_back();\\n// don’t include k in the subset\\nsearch(k+1);\\n}\\n}\\nWhen the function search is called with parameter k, it decides whether to\\ninclude the element kin the subset or not, and in both cases, then calls itself with\\nparameter k+1. Then, if k=n+1, the function notices that all elements have been\\nprocessed and a subset has been generated.\\nFigure 2.1illustrates the generation of subsets when n=3. At each function call,\\neither the upper branch ( kis included in the subset) or the lower branch ( kis not\\nincluded in the subset) is chosen.\\n2.2.2 Generating Permutations\\nNext we consider the problem of generating all permutations of a set of nelements.\\nFor example, the permutations of {1,2,3}are(1,2,3),(1,3,2),(2,1,3),(2,3,1),\\n(3,1,2), and(3,2,1). Again, we can use recursion to perform the search. The fol-\\nlowing function search maintains a vector\\nFig. 2.1 The recursion tree\\nwhen generating the subsets\\nof the set {1,2,3}', metadata={'source': 'documents\\\\doc.pdf', 'page': 24}),\n",
       " Document(page_content='2.2 Recursive Algorithms 17\\nvector< int> permutation;\\nthat will contain each permutation, and an array\\nbool chosen[n+1];\\nwhich indicates for each element if it has been included in the permutation. The\\nsearch begins when the function is called without parameters.\\nvoid search() {\\nif(permutation.size() == n) {\\n// process permutation\\n}else {\\nfor (int i = 1; i <= n; i++) {\\nif(chosen[i]) continue ;\\nchosen[i] = true ;\\npermutation.push_back(i);\\nsearch();\\nchosen[i] = false ;\\npermutation.pop_back();\\n}\\n}\\n}\\nEach function call appends a new element to permutation and records that it\\nhas been included in chosen . If the size of permutation equals the size of the\\nset, a permutation has been generated.\\nNote that the C++ standard library also has the function next_permutation\\nthat can be used to generate permutations. The function is given a permutation, and\\nit produces the next permutation in lexicographic order. The following code goes\\nthrough the permutations of {1,2,..., n}:\\nfor (int i = 1; i <= n; i++) {\\npermutation.push_back(i);\\n}\\ndo{\\n// process permutation\\n}while (next_permutation(permutation.begin(),\\npermutation.end()));', metadata={'source': 'documents\\\\doc.pdf', 'page': 25}),\n",
       " Document(page_content='18 2 Programming Techniques\\n2.2.3 Backtracking\\nAbacktracking algorithm begins with an empty solution and extends the solution\\nstep by step. The search recursively goes through all different ways how a solution\\ncan be constructed.\\nAs an example, consider the problem of calculating the number of ways nqueens\\ncan be placed on an n×nchessboard so that no two queens attack each other. For\\nexample, Fig. 2.2shows the two possible solutions for n=4.\\nThe problem can be solved using backtracking by placing queens on the board\\nrow by row. More precisely, exactly one queen will be placed on each row so that no\\nqueen attacks any of the queens placed before. A solution has been found when all\\nnqueens have been placed on the board.\\nFor example, Fig. 2.3shows some partial solutions generated by the backtracking\\nalgorithm when n=4. At the bottom level, the three ﬁrst conﬁgurations are illegal,\\nbecause the queens attack each other. However, the fourth conﬁguration is valid, and\\nit can be extended to a complete solution by placing two more queens on the board.\\nThere is only one way to place the two remaining queens.\\nFig. 2.2 The possible ways to place 4 queens on a 4 ×4 chessboard\\nFig. 2.3 Partial solutions to the queen problem using backtracking', metadata={'source': 'documents\\\\doc.pdf', 'page': 26}),\n",
       " Document(page_content='2.2 Recursive Algorithms 19\\nThe algorithm can be implemented as follows:\\nvoid search( int y) {\\nif(y == n) {\\ncount++;\\nreturn ;\\n}\\nfor (int x=0 ;x<n ; x++) {\\nif(col[x] || diag1[x+y] || diag2[x-y+n-1]) continue ;\\ncol[x] = diag1[x+y] = diag2[x-y+n-1] = 1;\\nsearch(y+1);\\ncol[x] = diag1[x+y] = diag2[x-y+n-1] = 0;\\n}\\n}\\nThe search begins by calling search(0) . The size of the board is n, and the\\ncode calculates the number of solutions to count . The code assumes that the rows\\nand columns of the board are numbered from 0 to n−1. When search is called\\nwith parameter y, it places a queen on row yand then calls itself with parameter\\ny+1. Then, if y=n, a solution has been found, and the value of count is increased\\nby one.\\nThe array col keeps track of the columns that contain a queen, and the arrays\\ndiag1 anddiag2 keep track of the diagonals. It is not allowed to add another\\nqueen to a column or diagonal that already contains a queen. For example, Fig. 2.4\\nshows the numbering of columns and diagonals of the 4 ×4 board.\\nThe above backtracking algorithm tells us that there are 92 ways to place 8 queens\\non the 8 ×8 board. When nincreases, the search quickly becomes slow, because\\nthe number of solutions grows exponentially. For example, it takes already about a\\nminute on a modern computer to calculate that there are 14772512 ways to place 16\\nqueens on the 16 ×16 board.\\nIn fact, nobody knows an efﬁcient way to count the number of queen combinations\\nfor larger values of n. Currently, the largest value of nfor which the result is known is\\n27: there are 234907967154122528 combinations in this case. This was discovered\\nin 2016 by a group of researchers who used a cluster of computers to calculate the\\nresult [ 25].\\nFig. 2.4 Numbering of the\\narrays when counting the\\ncombinations on the 4 ×4\\nboard', metadata={'source': 'documents\\\\doc.pdf', 'page': 27}),\n",
       " Document(page_content='20 2 Programming Techniques\\n2.3 Bit Manipulation\\nIn programming, an n-bit integer is internally stored as a binary number that consists\\nofnbits. For example, the C++ type int is a 32-bit type, which means that every\\nint number consists of 32 bits. For example, the bit representation of the int\\nnumber 43 is\\n00000000000000000000000000101011 .\\nThe bits in the representation are indexed from right to left. To convert a bit repre-\\nsentation bk...b2b1b0into a number, the formula\\nbk2k+···+ b222+b121+b020.\\ncan be used. For example,\\n1·25+1·23+1·21+1·20=43.\\nThe bit representation of a number is either signed orunsigned . Usually a signed\\nrepresentation is used, which means that both negative and positive numbers can be\\nrepresented. A signed variable of nbits can contain any integer between −2n−1and\\n2n−1−1. For example, the int type in C++ is a signed type, so an int variable\\ncan contain any integer between −231and 231−1.\\nThe ﬁrst bit in a signed representation is the sign of the number (0 for nonnegative\\nnumbers and 1 for negative numbers), and the remaining n−1 bits contain the\\nmagnitude of the number. Two’s complement is used, which means that the opposite\\nnumber of a number is calculated by ﬁrst inverting all the bits in the number and\\nthen increasing the number by one. For example, the bit representation of the int\\nnumber −43 is\\n11111111111111111111111111010101 .\\nIn an unsigned representation, only nonnegative numbers can be used, but the\\nupper bound for the values is larger. An unsigned variable of nbits can contain any\\ninteger between 0 and 2n−1. For example, in C++, an unsigned int variable\\ncan contain any integer between 0 and 232−1.\\nThere is a connection between the representations: a signed number −xequals\\nan unsigned number 2n−x. For example, the following code shows that the signed\\nnumber x=− 43 equals the unsigned number y=232−43:\\nint x = -43;\\nunsigned int y=x ;\\ncout << x << \"\\\\n\"; // -43\\ncout << y << \"\\\\n\"; // 4294967253\\nIf a number is larger than the upper bound of the bit representation, the number\\nwill overﬂow. In a signed representation, the next number after 2n−1−1i s−2n−1,', metadata={'source': 'documents\\\\doc.pdf', 'page': 28}),\n",
       " Document(page_content='2.3 Bit Manipulation 21\\nand in an unsigned representation, the next number after 2n−1 is 0. For example,\\nconsider the following code:\\nint x = 2147483647\\ncout << x << \"\\\\n\"; // 2147483647\\nx++;\\ncout << x << \"\\\\n\"; // -2147483648\\nInitially, the value of xis 231−1. This is the largest value that can be stored in\\nanint variable, so the next number after 231−1i s−231.\\n2.3.1 Bit Operations\\nAnd Operation The and operation x&yproduces a number that has one bits in\\npositions where both xand yhave one bits. For example, 22 & 26 =18, because\\n10110 (22)\\n& 11010 (26)\\n=10010 (18).\\nUsing the and operation, we can check if a number xis even because x&1=0\\nifxis even, and x&1=1i fxis odd. More generally, xis divisible by 2kexactly\\nwhen x&(2k−1)=0.\\nOr Operation Theoroperation x|yproduces a number that has one bits in positions\\nwhere at least one of xand yhave one bits. For example, 22 |26=30, because\\n10110 (22)\\n|11010 (26)\\n=11110 (30).\\nXor Operation The xor operation xˆyproduces a number that has one bits in\\npositions where exactly one of xand yhave one bits. For example, 22 ˆ 26 =12,\\nbecause\\n10110 (22)\\nˆ 11010 (26)\\n=01100 (12).\\nNot Operation The not operation~xproduces a number where all the bits of xhave\\nbeen inverted. The formula~x=− x−1 holds, for example,~29=− 30. The result\\nof the not operation at the bit level depends on the length of the bit representation,', metadata={'source': 'documents\\\\doc.pdf', 'page': 29}),\n",
       " Document(page_content='22 2 Programming Techniques\\nbecause the operation inverts all bits. For example, if the numbers are 32-bit int\\nnumbers, the result is as follows:\\nx= 29 00000000000000000000000000011101\\n~x=− 30 11111111111111111111111111100010\\nBit Shifts Theleft bit shift x << kappends kzero bits to the number, and the right bit\\nshift x >> kremoves the klast bits from the number. For example, 14 << 2=56,\\nbecause 14 and 56 correspond to 1110 and 111000. Similarly, 49 >> 3=6, because\\n49 and 6 correspond to 110001 and 110. Note that x<< kcorresponds to multiplying\\nxby 2k, and x>> kcorresponds to dividing xby 2krounded down to an integer.\\nBit Masks Abit mask of the form 1 << khas a one bit in position k, and all other\\nbits are zero, so we can use such masks to access single bits of numbers. In particular,\\nthekth bit of a number is one exactly when x&(1<< k)is not zero. The following\\ncode prints the bit representation of an int number x:\\nfor (int k=3 1 ;k> =0 ;k - - ){\\nif(x&(1<<k)) cout << \"1\";\\nelse cout << \"0\";\\n}\\nIt is also possible to modify single bits of numbers using similar ideas. The formula\\nx|(1<< k)sets the kth bit of xto one, the formula x&~(1<< k)sets the kth bit\\nofxto zero, and the formula xˆ(1<< k)inverts the kth bit of x. Then, the formula\\nx&(x−1)sets the last one bit of xto zero, and the formula x&−xsets all the one\\nbits to zero, except for the last one bit. The formula x|(x−1)inverts all the bits\\nafter the last one bit. Finally, a positive number xis a power of two exactly when x\\n&(x−1)=0.\\nOne pitfall when using bit masks is that 1<<k is always an int bit mask. An\\neasy way to create a long long bit mask is 1LL<<k .\\nAdditional Functions The g++ compiler also provides the following functions for\\ncounting bits:\\n•__builtin_clz (x): the number of zeros at the beginning of the bit represen-\\ntation\\n•__builtin_ctz (x): the number of zeros at the end of the bit representation\\n•__builtin_popcount (x): the number of ones in the bit representation\\n•__builtin_parity (x): the parity (even or odd) of the number of ones in the\\nbit representation\\nThe functions can be used as follows:', metadata={'source': 'documents\\\\doc.pdf', 'page': 30}),\n",
       " Document(page_content='2.3 Bit Manipulation 23\\nint x = 5328; // 00000000000000000001010011010000\\ncout << __builtin_clz(x) << \"\\\\n\"; // 19\\ncout << __builtin_ctz(x) << \"\\\\n\"; // 4\\ncout << __builtin_popcount(x) << \"\\\\n\"; // 5\\ncout << __builtin_parity(x) << \"\\\\n\"; // 1\\nNote that the above functions only support int numbers, but there are also long\\nlong versions of the functions available with the sufﬁx ll.\\n2.3.2 Representing Sets\\nEvery subset of a set {0,1,2,..., n−1}can be represented as an nbit integer\\nwhose one bits indicate which elements belong to the subset. This is an efﬁcient way\\nto represent sets, because every element requires only one bit of memory, and set\\noperations can be implemented as bit operations.\\nFor example, since int is a 32-bit type, an int number can represent any subset\\nof the set {0,1,2,..., 31}. The bit representation of the set {1,3,4,8}is\\n00000000000000000000000100011010 ,\\nwhich corresponds to the number 28+24+23+21=282.\\nThe following code declares an int variable xthat can contain a subset of\\n{0,1,2,..., 31}. After this, the code adds the elements 1, 3, 4, and 8 to the set\\nand prints the size of the set.\\nint x=0 ;\\nx |= (1<<1);\\nx |= (1<<3);\\nx |= (1<<4);\\nx |= (1<<8);\\ncout << __builtin_popcount(x) << \"\\\\n\"; // 4\\nThen, the following code prints all elements that belong to the set:\\nfor (int i = 0; i < 32; i++) {\\nif(x&(1<<i)) cout << i << \" \";\\n}\\n// output :1348\\nSet Operations Table 2.1 shows how set operations can be implemented as bit\\noperations. For example, the following code ﬁrst constructs the sets x={1,3,4,8}\\nand y={3,6,8,9}and then constructs the set z=x∪y={1,3,4,6,8,9}:', metadata={'source': 'documents\\\\doc.pdf', 'page': 31}),\n",
       " Document(page_content='24 2 Programming Techniques\\nTable 2.1 Implementing set operations as bit operations\\nOperation Set syntax Bit syntax\\nIntersection a∩b a&b\\nUnion a∪b a|b\\nComplement ¯a~a\\nDifference a\\\\b a&(~b)\\nint x = (1<<1)|(1<<3)|(1<<4)|(1<<8);\\nint y = (1<<3)|(1<<6)|(1<<8)|(1<<9);\\nint z=x | y ;\\ncout << __builtin_popcount(z) << \"\\\\n\"; // 6\\nThe following code goes through the subsets of {0,1,..., n−1}:\\nfor (int b = 0; b < (1<<n); b++) {\\n// process subset b\\n}\\nThen, the following code goes through the subsets with exactly kelements:\\nfor (int b = 0; b < (1<<n); b++) {\\nif(__builtin_popcount(b) == k) {\\n// process subset b\\n}\\n}\\nFinally, the following code goes through the subsets of a set x:\\nint b=0 ;\\ndo{\\n// process subset b\\n}while (b=(b-x)&x);\\nC++ Bitsets The C++ standard library also provides the bitset structure, which\\ncorresponds to an array whose each value is either 0 or 1. For example, the following\\ncode creates a bitset of 10 elements:', metadata={'source': 'documents\\\\doc.pdf', 'page': 32}),\n",
       " Document(page_content='2.3 Bit Manipulation 25\\nbitset<10> s;\\ns[1] = 1;\\ns[3] = 1;\\ns[4] = 1;\\ns[7] = 1;\\ncout << s[4] << \"\\\\n\"; // 1\\ncout << s[5] << \"\\\\n\"; // 0\\nThe function count returns the number of one bits in the bitset:\\ncout << s.count() << \"\\\\n\"; // 4\\nAlso bit operations can be directly used to manipulate bitsets:\\nbitset<10> a, b;\\n// ...\\nbitset<10> c = a&b;\\nbitset<10> d = a|b;\\nbitset<10> e = a^b;', metadata={'source': 'documents\\\\doc.pdf', 'page': 33}),\n",
       " Document(page_content='Efﬁciency\\nThe efﬁciency of algorithms plays a central role in competitive programming. In this\\nchapter, we learn tools that make it easier to design efﬁcient algorithms.\\nSection 3.1introduces the concept of time complexity, which allows us to estimate\\nrunning times of algorithms without implementing them. The time complexity of an\\nalgorithm shows how quickly its running time increases when the size of the input\\ngrows.\\nSection 3.2 presents two example problems which can be solved in many ways.\\nIn both problems, we can easily design a slow brute force solution, but it turns out\\nthat we can also create much more efﬁcient algorithms.\\n3.1 Time Complexity\\nThe time complexity of an algorithm estimates how much time the algorithm will use\\nfor a given input. By calculating the time complexity, we can often ﬁnd out whether\\nthe algorithm is fast enough for solving a problem—without implementing it.\\nA time complexity is denoted O(···)where the three dots represent some func-\\ntion. Usually, the variable ndenotes the input size. For example, if the input is an\\narray of numbers, nwill be the size of the array, and if the input is a string, nwill be\\nthe length of the string.\\n3.1.1 Calculation Rules\\nIf a code consists of single commands, its time complexity is O(1). For example,\\nthe time complexity of the following code is O(1).', metadata={'source': 'documents\\\\doc.pdf', 'page': 34}),\n",
       " Document(page_content='28 3E f ﬁ c i e n c y\\na++;\\nb++;\\nc=a + b ;\\nThe time complexity of a loop estimates the number of times the code inside the\\nloop is executed. For example, the time complexity of the following code is O(n),\\nbecause the code inside the loop is executed ntimes. We assume that “ ... ” denotes\\na code whose time complexity is O(1).\\nfor (int i = 1; i <= n; i++) {\\n...\\n}\\nThen, the time complexity of the following code is O(n2):\\nfor (int i = 1; i <= n; i++) {\\nfor (int j=1 ;j< =n ;j + + ){\\n...\\n}\\n}\\nIn general, if there are knested loops and each loop goes through nvalues, the\\ntime complexity is O(nk).\\nA time complexity does not tell us the exact number of times the code inside a\\nloop is executed, because it only shows the order of growth and ignores the constant\\nfactors. In the following examples, the code inside the loop is executed 3 n,n+5,\\nand⌈n/2⌉times, but the time complexity of each code is O(n).\\nfor (int i = 1; i <= 3*n; i++) {\\n...\\n}\\nfor (int i = 1; i <= n+5; i++) {\\n...\\n}\\nfor (int i=1 ;i< =n ;i+ =2 ){\\n...\\n}\\nAs another example, the time complexity of the following code is O(n2), because\\nthe code inside the loop is executed 1 +2+...+n=1\\n2(n2+n)times.', metadata={'source': 'documents\\\\doc.pdf', 'page': 35}),\n",
       " Document(page_content='3.1 Time Complexity 29\\nfor (int i = 1; i <= n; i++) {\\nfor (int j=1 ;j< =i ;j + + ){\\n...\\n}\\n}\\nIf an algorithm consists of consecutive phases, the total time complexity is the\\nlargest time complexity of a single phase. The reason for this is that the slowest\\nphase is the bottleneck of the algorithm. For example, the following code consists\\nof three phases with time complexities O(n),O(n2), and O(n). Thus, the total time\\ncomplexity is O(n2).\\nfor (int i = 1; i <= n; i++) {\\n...\\n}\\nfor (int i = 1; i <= n; i++) {\\nfor (int j=1 ;j< =n ;j + + ){\\n...\\n}\\n}\\nfor (int i = 1; i <= n; i++) {\\n...\\n}\\nSometimes the time complexity depends on several factors, and the time com-\\nplexity formula contains several variables. For example, the time complexity of the\\nfollowing code is O(nm):\\nfor (int i = 1; i <= n; i++) {\\nfor (int j=1 ;j< =m ;j + + ){\\n...\\n}\\n}\\nThe time complexity of a recursive function depends on the number of times the\\nfunction is called and the time complexity of a single call. The total time complexity\\nis the product of these values. For example, consider the following function:\\nvoid f(int n) {\\nif(n == 1) return ;\\nf(n-1);\\n}\\nThe call f(n)causes nfunction calls, and the time complexity of each call is\\nO(1), so the total time complexity is O(n).\\nAs another example, consider the following function:', metadata={'source': 'documents\\\\doc.pdf', 'page': 36}),\n",
       " Document(page_content='30 3E f ﬁ c i e n c y\\nvoid g(int n) {\\nif(n == 1) return ;\\ng(n-1);\\ng(n-1);\\n}\\nWhat happens when the function is called with a parameter n? First, there are two\\ncalls with parameter n−1, then four calls with parameter n−2, then eight calls with\\nparameter n−3, and so on. In general, there will be 2kcalls with parameter n−k\\nwhere k=0,1,..., n−1. Thus, the time complexity is\\n1+2+4+···+ 2n−1=2n−1=O(2n).\\n3.1.2 Common Time Complexities\\nThe following list contains common time complexities of algorithms:\\nO(1) The running time of a constant-time algorithm does not depend on the input\\nsize. A typical constant-time algorithm is a direct formula that calculates the\\nanswer.\\nO(log n) Alogarithmic algorithm often halves the input size at each step. The\\nrunning time of such an algorithm is logarithmic, because log2nequals the number\\nof times nmust be divided by 2 to get 1. Note that the base of the logarithm is not\\nshown in the time complexity.\\nO(√n) Asquare root algorithm is slower than O(log n)but faster than O(n).A\\nspecial property of square roots is that√n=n/√n,s o nelements can be divided\\ninto O(√n)blocks of O(√n)elements.\\nO(n) Alinear algorithm goes through the input a constant number of times. This\\nis often the best possible time complexity, because it is usually necessary to access\\neach input element at least once before reporting the answer.\\nO(nlog n) This time complexity often indicates that the algorithm sorts the input,\\nbecause the time complexity of efﬁcient sorting algorithms is O(nlog n). Another\\npossibility is that the algorithm uses a data structure where each operation takes\\nO(log n)time.\\nO(n2) Aquadratic algorithm often contains two nested loops. It is possible to go\\nthrough all pairs of the input elements in O(n2)time.\\nO(n3) Acubic algorithm often contains three nested loops. It is possible to go\\nthrough all triplets of the input elements in O(n3)time.\\nO(2n) This time complexity often indicates that the algorithm iterates through all\\nsubsets of the input elements. For example, the subsets of {1,2,3}are∅,{1},{2},\\n{3},{1,2},{1,3},{2,3}, and{1,2,3}.\\nO(n!) This time complexity often indicates that the algorithm iterates through all\\npermutations of the input elements. For example, the permutations of {1,2,3}are\\n(1,2,3),(1,3,2),(2,1,3),(2,3,1),(3,1,2), and(3,2,1).', metadata={'source': 'documents\\\\doc.pdf', 'page': 37}),\n",
       " Document(page_content='3.1 Time Complexity 31\\nAn algorithm is polynomial if its time complexity is at most O(nk)where kis a\\nconstant. All the above time complexities except O(2n)and O(n!)are polynomial. In\\npractice, the constant kis usually small, and therefore a polynomial time complexity\\nroughly means that the algorithm can process large inputs.\\nMost algorithms in this book are polynomial. Still, there are many important\\nproblems for which no polynomial algorithm is known, i.e., nobody knows how to\\nsolve them efﬁciently. NP-hard problems are an important set of problems, for which\\nno polynomial algorithm is known.\\n3.1.3 Estimating Efﬁciency\\nBy calculating the time complexity of an algorithm, it is possible to check, before\\nimplementing the algorithm, that it is efﬁcient enough for solving a problem. The\\nstarting point for estimations is the fact that a modern computer can perform some\\nhundreds of millions of simple operations in a second.\\nFor example, assume that the time limit for a problem is one second and the\\ninput size is n=105. If the time complexity is O(n2), the algorithm will perform\\nabout(105)2=1010operations. This should take at least some tens of seconds, so\\nthe algorithm seems to be too slow for solving the problem. However, if the time\\ncomplexity is O(nlog n), there will be only about 105log 105≈1.6·106operations,\\nand the algorithm will surely ﬁt the time limit.\\nOn the other hand, given the input size, we can try to guess the required time\\ncomplexity of the algorithm that solves the problem. Table 3.1contains some useful\\nestimates assuming a time limit of one second.\\nFor example, if the input size is n=105, it is probably expected that the time\\ncomplexity of the algorithm is O(n)orO(nlog n). This information makes it easier\\nto design the algorithm, because it rules out approaches that would yield an algorithm\\nwith a worse time complexity.\\nStill, it is important to remember that a time complexity is only an estimate of\\nefﬁciency, because it hides the constant factors. For example, an algorithm that runs\\ninO(n)time may perform n/2o r5 noperations, which has an important effect on\\nthe actual running time of the algorithm.\\nTable 3.1 Estimating time complexity from input size\\nInput size Expected time complexity\\nn≤10 O(n!)\\nn≤20 O(2n)\\nn≤500 O(n3)\\nn≤5000 O(n2)\\nn≤106O(nlog n)orO(n)\\nnis large O(1)orO(log n)', metadata={'source': 'documents\\\\doc.pdf', 'page': 38}),\n",
       " Document(page_content='32 3E f ﬁ c i e n c y\\n3.1.4 Formal Deﬁnitions\\nWhat does it exactly mean that an algorithm works in O(f(n))time? It means\\nthat there are constants cand n0such that the algorithm performs at most c f (n)\\noperations for all inputs where n≥n0. Thus, the Onotation gives an upper bound\\nfor the running time of the algorithm for sufﬁciently large inputs.\\nFor example, it is technically correct to say that the time complexity of the fol-\\nlowing algorithm is O(n2).\\nfor (int i = 1; i <= n; i++) {\\n...\\n}\\nHowever, a better bound is O(n), and it would be very misleading to give the\\nbound O(n2), because everybody actually assumes that the Onotation is used to\\ngive an accurate estimate of the time complexity.\\nThere are also two other common notations. The Ωnotation gives a lower bound\\nfor the running time of an algorithm. The time complexity of an algorithm is Ω( f(n)),\\nif there are constants cand n0such that the algorithm performs at least c f (n)\\noperations for all inputs where n≥n0. Finally, the Θnotation gives an exact bound :\\nthe time complexity of an algorithm is Θ( f(n))if it is both O(f(n))andΩ( f(n)).\\nFor example, since the time complexity of the above algorithm is both O(n)and\\nΩ(n), it is also Θ(n).\\nWe can use the above notations in many situations, not only for referring to time\\ncomplexities of algorithms. For example, we might say that an array contains O(n)\\nvalues, or that an algorithm consists of O(log n)rounds.\\n3.2 Examples\\nIn this section we discuss two algorithm design problems that can be solved in several\\ndifferent ways. We start with simple brute force algorithms, and then create more\\nefﬁcient solutions by using various algorithm design ideas.\\n3.2.1 Maximum Subarray Sum\\nGiven an array of nnumbers, our ﬁrst task is to calculate the maximum subarray sum ,\\ni.e., the largest possible sum of a sequence of consecutive values in the array. The\\nproblem is interesting when there may be negative values in the array. For example,\\nFig. 3.1shows an array and its maximum-sum subarray.', metadata={'source': 'documents\\\\doc.pdf', 'page': 39}),\n",
       " Document(page_content='3.2 Examples 33\\nFig. 3.1 The maximum-sum\\nsubarray of this array is\\n[2,4,−3,5,2], whose sum\\nis 10\\nO(n3)Time Solution A straightforward way to solve the problem is to go through\\nall possible subarrays, calculate the sum of values in each subarray and maintain the\\nmaximum sum. The following code implements this algorithm:\\nint best = 0;\\nfor (int a=0 ;a<n ;a + + ){\\nfor (int b=a ;b<n ; b++) {\\nint sum = 0;\\nfor (int k = a; k <= b; k++) {\\nsum += array[k];\\n}\\nbest = max(best,sum);\\n}\\n}\\ncout << best << \"\\\\n\";\\nThe variables aandbﬁx the ﬁrst and last index of the subarray, and the sum of\\nvalues is calculated to the variable sum . The variable best contains the maximum\\nsum found during the search. The time complexity of the algorithm is O(n3), because\\nit consists of three nested loops that go through the input.\\nO(n2)Time Solution It is easy to make the algorithm more efﬁcient by removing\\none loop from it. This is possible by calculating the sum at the same time when the\\nright end of the subarray moves. The result is the following code:\\nint best = 0;\\nfor (int a=0 ;a<n ;a + + ){\\nint sum = 0;\\nfor (int b=a ;b<n ; b++) {\\nsum += array[b];\\nbest = max(best,sum);\\n}\\n}\\ncout << best << \"\\\\n\";\\nAfter this change, the time complexity is O(n2).\\nO(n)Time Solution It turns out that it is possible to solve the problem in O(n)time,\\nwhich means that just one loop is enough. The idea is to calculate, for each array\\nposition, the maximum sum of a subarray that ends at that position. After this, the\\nanswer to the problem is the maximum of those sums.', metadata={'source': 'documents\\\\doc.pdf', 'page': 40}),\n",
       " Document(page_content='34 3E f ﬁ c i e n c y\\nConsider the subproblem of ﬁnding the maximum-sum subarray that ends at posi-\\ntion k. There are two possibilities:\\n1. The subarray only contains the element at position k.\\n2. The subarray consists of a subarray that ends at position k−1, followed by the\\nelement at position k.\\nIn the latter case, since we want to ﬁnd a subarray with maximum sum, the subarray\\nthat ends at position k−1 should also have the maximum sum. Thus, we can solve\\nthe problem efﬁciently by calculating the maximum subarray sum for each ending\\nposition from left to right.\\nThe following code implements the algorithm:\\nint best = 0, sum = 0;\\nfor (int k=0 ;k<n ;k + + ){\\nsum = max(array[k],sum+array[k]);\\nbest = max(best,sum);\\n}\\ncout << best << \"\\\\n\";\\nThe algorithm only contains one loop that goes through the input, so the time\\ncomplexity is O(n). This is also the best possible time complexity, because any\\nalgorithm for the problem has to examine all array elements at least once.\\nEfﬁciency Comparison How efﬁcient are the above algorithms in practice? Table 3.2\\nshows the running times of the above algorithms for different values of non a modern\\ncomputer. In each test, the input was generated randomly, and the time needed for\\nreading the input was not measured.\\nThe comparison shows that all algorithms work quickly when the input size is\\nsmall, but larger inputs bring out remarkable differences in the running times. The\\nO(n3)algorithm becomes slow when n=104, and the O(n2)algorithm becomes\\nslow when n=105. Only the O(n)algorithm is able to process even the largest\\ninputs instantly.\\nTable 3.2 Comparing running times of the maximum subarray sum algorithms\\nArray size n O(n3)(s) O(n2)(s) O(n)(s)\\n1020.0 0.0 0.0\\n1030.1 0.0 0.0\\n104>10.0 0.1 0.0\\n105>10.0 5.3 0.0\\n106>10.0 >10.0 0.0\\n107>10.0 >10.0 0.0', metadata={'source': 'documents\\\\doc.pdf', 'page': 41}),\n",
       " Document(page_content='3.2 Examples 35\\n3.2.2 Two Queens Problem\\nGiven an n×nchessboard, our next problem is to count the number of ways we can\\nplace two queens on the board in such a way that they do not attack each other. For\\nexample, as Fig. 3.2 shows, there are eight ways to place two queens on the 3 ×3\\nboard. Let q(n)denote the number of valid combinations for an n×nboard. For\\nexample, q(3)=8, and Table 3.3shows the values of q(n)for 1≤n≤10.\\nTo start with, a simple way to solve the problem is to go through all possible ways\\nto place two queens on the board and count the combinations where the queens do\\nnot attack each other. Such an algorithm works in O(n4)time, because there are n2\\nways to choose the position of the ﬁrst queen, and for each such position, there are\\nn2−1 ways to choose the position of the second queen.\\nSince the number of combinations grows fast, an algorithm that counts combina-\\ntions one by one will certainly be too slow for processing larger values of n. Thus, to\\ncreate an efﬁcient algorithm, we need to ﬁnd a way to count combinations in groups .\\nOne useful observation is that it is quite easy to calculate the number of squares that\\na single queen attacks (Fig. 3.3). First, it always attacks n−1 squares horizontally\\nand n−1 squares vertically. Then, for both diagonals, it attacks d−1 squares where\\ndis the number of squares on the diagonal. Using this information, we can calculate\\nFig. 3.2 All possible ways\\nto place two non-attacking\\nqueens on the 3 ×3\\nchessboard\\nTable 3.3 First values of the\\nfunction q(n): the number of\\nways to place two\\nnon-attacking queens on an\\nn×nchessboardBoard size n Number of ways q(n)\\n1 0\\n2 0\\n3 8\\n4 44\\n5 140\\n6 340\\n7 700\\n8 1288\\n9 2184\\n10 3480', metadata={'source': 'documents\\\\doc.pdf', 'page': 42}),\n",
       " Document(page_content='36 3E f ﬁ c i e n c y\\nFig. 3.3 The queen attacks\\nall squares marked with “*”\\non the board\\nFig. 3.4 Possible positions\\nfor queens on the last row\\nand column\\ninO(1)time the number of squares where the other queen can be placed, which\\nyields an O(n2)time algorithm.\\nAnother way to approach the problem is to try to formulate a recursive function\\nthat counts the number of combinations. The question is: if we know the value of\\nq(n), how can we use it to calculate the value of q(n+1)?\\nTo get a recursive solution, we may focus on the last row and last column of the n×\\nnboard (Fig. 3.4). First, if there are no queens on the last row or column, the number\\nof combinations is simply q(n−1). Then, there are 2 n−1 positions for a queen on\\nthe last row or column. It attacks 3 (n−1)squares, so there are n2−3(n−1)−1\\npositions for the other queen. Finally, there are (n−1)(n−2)combinations where\\nboth queens are on the last row or column. Since we counted those combinations\\ntwice, we have to remove this number from the result. By combining all this, we get\\na recursive formula\\nq(n)=q(n−1)+(2n−1)(n2−3(n−1)−1)−(n−1)(n−2)\\n=q(n−1)+2(n−1)2(n−2),\\nwhich provides an O(n)solution to the problem.\\nFinally, it turns out that there is also a closed-form formula\\nq(n)=n4\\n2−5n3\\n3+3n2\\n2−n\\n3,\\nwhich can be proved using induction and the recursive formula. Using this formula,\\nwe can solve the problem in O(1)time.', metadata={'source': 'documents\\\\doc.pdf', 'page': 43}),\n",
       " Document(page_content='Sorting and Searching\\nMany efﬁcient algorithms are based on sorting the input data, because sorting often\\nmakes solving the problem easier. This chapter discusses the theory and practice of\\nsorting as an algorithm design tool.\\nSection 4.1ﬁrst discusses three important sorting algorithms: bubble sort, merge\\nsort, and counting sort. After this, we will learn how to use the sorting algorithm\\navailable in the C++ standard library.\\nSection 4.2 shows how sorting can be used as a subroutine to create efﬁcient\\nalgorithms. For example, to quickly determine if all array elements are unique, we\\ncan ﬁrst sort the array and then simply check all pairs of consecutive elements.\\nSection 4.3presents the binary search algorithm, which is another important build-\\ning block of efﬁcient algorithms.\\n4.1 Sorting Algorithms\\nThe basic problem in sorting is as follows: Given an array that contains nelements,\\nsort the elements in increasing order. For example, Fig. 4.1 shows an array before\\nand after sorting.\\nIn this section we will go through some fundamental sorting algorithms and exam-\\nine their properties. It is easy to design an O(n2)time sorting algorithm, but there\\nare also more efﬁcient algorithms. After discussing the theory of sorting, we will\\nfocus on using sorting in practice in C++.', metadata={'source': 'documents\\\\doc.pdf', 'page': 44}),\n",
       " Document(page_content='38 4 Sorting and Searching\\nFig. 4.1 An array before and\\nafter sorting\\nFig. 4.2 The ﬁrst round of\\nbubble sort\\n4.1.1 Bubble Sort\\nBubble sort is a simple sorting algorithm that works in O(n2)time. The algorithm\\nconsists of nrounds, and on each round, it iterates through the elements of the array.\\nWhenever two consecutive elements are found that are in wrong order, the algorithm\\nswaps them. The algorithm can be implemented as follows:\\nfor (int i=0 ;i<n ;i + + ){\\nfor (int j = 0; j < n-1; j++) {\\nif(array[j] > array[j+1]) {\\nswap(array[j],array[j+1]);\\n}\\n}\\n}\\nAfter the ﬁrst round of bubble sort, the largest element will be in the correct\\nposition, and more generally, after krounds, the klargest elements will be in the\\ncorrect positions. Thus, after nrounds, the whole array will be sorted.\\nFor example, Fig. 4.2shows the ﬁrst round of swaps when bubble sort is used to\\nsort an array.\\nBubble sort is an example of a sorting algorithm that always swaps consecutive\\nelements in the array. It turns out that the time complexity of such an algorithm\\nisalways at least O(n2), because in the worst case, O(n2)swaps are required for\\nsorting the array.', metadata={'source': 'documents\\\\doc.pdf', 'page': 45}),\n",
       " Document(page_content='4.1 Sorting Algorithms 39\\nFig. 4.3 This array has three\\ninversions: (3,4),(3,5),a n d\\n(6,7)\\nInversions A useful concept when analyzing sorting algorithms is an inversion :a\\npair of array indices (a,b)such that a<bandarray [a]>array [b], i.e., the\\nelements are in wrong order. For example, the array in Fig. 4.3has three inversions:\\n(3,4),(3,5), and(6,7).\\nThe number of inversions indicates how much work is needed to sort the array.\\nAn array is completely sorted when there are no inversions. On the other hand, if the\\narray elements are in the reverse order, the number of inversions is\\n1+2+···+ (n−1)=n(n−1)\\n2=O(n2),\\nwhich is the largest possible.\\nSwapping a pair of consecutive elements that are in the wrong order removes\\nexactly one inversion from the array. Hence, if a sorting algorithm can only swap\\nconsecutive elements, each swap removes at most one inversion, and the time com-\\nplexity of the algorithm is at least O(n2).\\n4.1.2 Merge Sort\\nIf we want to create an efﬁcient sorting algorithm, we have to be able to reorder\\nelements that are in different parts of the array. There are several such sorting algo-\\nrithms that work in O(nlog n)time. One of them is merge sort , which is based on\\nrecursion. Merge sort sorts a subarray array [a... b]as follows:\\n1. If a=b, do not do anything, because a subarray that only contains one element\\nis already sorted.\\n2. Calculate the position of the middle element: k=⌊(a+b)/2⌋.\\n3. Recursively sort the subarray array [a... k].\\n4. Recursively sort the subarray array [k+1... b].\\n5.Merge the sorted subarrays array [a... k]andarray [k+1... b]into a sorted\\nsubarray array [a... b].\\nFor example, Fig. 4.4shows how merge sort sorts an array of eight elements. First,\\nthe algorithm divides the array into two subarrays of four elements. Then, it sorts\\nthese subarrays recursively by calling itself. Finally, it merges the sorted subarrays\\ninto a sorted array of eight elements.\\nMerge sort is an efﬁcient algorithm, because it halves the size of the subarray at\\neach step. Then, merging the sorted subarrays is possible in linear time, because they\\nare already sorted. Since there are O(log n)recursive levels, and processing each\\nlevel takes a total of O(n)time, the algorithm works in O(nlog n)time.', metadata={'source': 'documents\\\\doc.pdf', 'page': 46}),\n",
       " Document(page_content='40 4 Sorting and Searching\\nFig. 4.4 Sorting an array\\nusing merge sort\\nFig. 4.5 The progress of a\\nsorting algorithm that\\ncompares array elements\\n4.1.3 Sorting Lower Bound\\nIs it possible to sort an array faster than in O(nlog n)time? It turns out that this\\nisnot possible when we restrict ourselves to sorting algorithms that are based on\\ncomparing array elements.\\nThe lower bound for the time complexity can be proved by considering sorting as\\na process where each comparison of two elements gives more information about the\\ncontents of the array. Figure 4.5illustrates the tree created in this process.\\nHere “ x<y?” means that some elements xand yare compared. If x<y, the\\nprocess continues to the left, and otherwise to the right. The results of the process\\nare the possible ways to sort the array, a total of n!ways. For this reason, the height\\nof the tree must be at least\\nlog2(n!)=log2(1)+log2(2)+···+ log2(n).\\nWe get a lower bound for this sum by choosing the last n/2 elements and changing\\nthe value of each element to log2(n/2). This yields an estimate\\nlog2(n!)≥(n/2)·log2(n/2),\\nso the height of the tree and the worst-case number of steps in a sorting algorithm is\\nΩ(nlog n).', metadata={'source': 'documents\\\\doc.pdf', 'page': 47}),\n",
       " Document(page_content='4.1 Sorting Algorithms 41\\nFig. 4.6 Sorting an array\\nusing counting sort\\n4.1.4 Counting Sort\\nThe lower bound Ω(nlog n)does not apply to algorithms that do not compare array\\nelements but use some other information. An example of such an algorithm is count-\\ning sort that sorts an array in O(n)time assuming that every element in the array is\\nan integer between 0 ... cand c=O(n).\\nThe algorithm creates a bookkeeping array, whose indices are elements of the\\noriginal array. The algorithm iterates through the original array and calculates how\\nmany times each element appears in the array. As an example, Fig. 4.6 shows an\\narray and the corresponding bookkeeping array. For example, the value at position\\n3 is 2, because the value 3 appears 2 times in the original array.\\nThe construction of the bookkeeping array takes O(n)time. After this, the sorted\\narray can be created in O(n)time, because the number of occurrences of each element\\ncan be retrieved from the bookkeeping array. Thus, the total time complexity of\\ncounting sort is O(n).\\nCounting sort is a very efﬁcient algorithm but it can only be used when the constant\\ncis small enough, so that the array elements can be used as indices in the bookkeeping\\narray.\\n4.1.5 Sorting in Practice\\nIn practice, it is almost never a good idea to implement a home-made sorting algo-\\nrithm, because all modern programming languages have good sorting algorithms\\nin their standard libraries. There are many reasons to use a library function: it is\\ncertainly correct and efﬁcient, and also easy to use.\\nIn C++, the function sort efﬁciently1sorts the contents of a data structure. For\\nexample, the following code sorts the elements of a vector in increasing order:\\nvector< int> v = {4,2,5,3,5,8,3};\\nsort(v.begin(),v.end());\\nAfter the sorting, the contents of the vector will be [2,3,3,4,5,5,8]. The default\\nsorting order is increasing, but a reverse order is possible as follows:\\n1The C++11 standard requires that the sort function works in O(nlog n)time; the exact imple-\\nmentation depends on the compiler.', metadata={'source': 'documents\\\\doc.pdf', 'page': 48}),\n",
       " Document(page_content='42 4 Sorting and Searching\\nsort(v.rbegin(),v.rend());\\nAn ordinary array can be sorted as follows:\\nint n=7 ; // array size\\nint a[] = {4,2,5,3,5,8,3};\\nsort(a,a+n);\\nThen, the following code sorts the string s:\\nstring s = \"monkey\";\\nsort(s.begin(), s.end());\\nSorting a string means that the characters of the string are sorted. For example,\\nthe string “monkey” becomes “ekmnoy”.\\nComparison Operators Thesort function requires that a comparison operator is\\ndeﬁned for the data type of the elements to be sorted. When sorting, this operator\\nwill be used whenever it is necessary to ﬁnd out the order of two elements.\\nMost C++ data types have a built-in comparison operator, and elements of those\\ntypes can be sorted automatically. Numbers are sorted according to their values, and\\nstrings are sorted in alphabetical order. Pairs are sorted primarily according to their\\nﬁrst elements and secondarily according to their second elements:\\nvector<pair< int,int>> v;\\nv.push_back({1,5});\\nv.push_back({2,3});\\nv.push_back({1,2});\\nsort(v.begin(), v.end());\\n// result: [(1,2),(1,5),(2,3)]\\nIn a similar way, tuples are sorted primarily by the ﬁrst element, secondarily by\\nthe second element, etc.2:\\nvector<tuple< int,int,int>> v;\\nv.push_back({2,1,4});\\nv.push_back({1,5,3});\\nv.push_back({2,1,3});\\nsort(v.begin(), v.end());\\n// result: [(1,5,3),(2,1,3),(2,1,4)]\\nUser-deﬁned structs do not have a comparison operator automatically. The opera-\\ntor should be deﬁned inside the struct as a function operator <, whose parameter\\n2Note that in some older compilers, the function make_tuple has to be used to create a tuple\\ninstead of braces (for example, make_tuple(2,1,4) instead of {2,1,4} ).', metadata={'source': 'documents\\\\doc.pdf', 'page': 49}),\n",
       " Document(page_content='4.1 Sorting Algorithms 43\\nis another element of the same type. The operator should return true if the element\\nis smaller than the parameter, and false otherwise.\\nFor example, the following struct point contains the x and y coordinates of a\\npoint. The comparison operator is deﬁned so that the points are sorted primarily by\\nthe x coordinate and secondarily by the y coordinate.\\nstruct point {\\nint x, y;\\nbool operator <(const point &p) {\\nif(x == p.x) return y < p.y;\\nelse return x < p.x;\\n}\\n};\\nComparison Functions It is also possible to give an external comparison function\\nto the sort function as a callback function. For example, the following comparison\\nfunction comp sorts strings primarily by length and secondarily by alphabetical\\norder:\\nbool comp(string a, string b) {\\nif(a.size() == b.size()) return a<b ;\\nelse return a.size() < b.size();\\n}\\nNow a vector of strings can be sorted as follows:\\nsort(v.begin(), v.end(), comp);\\n4.2 Solving Problems by Sorting\\nOften, we can easily solve a problem in O(n2)time using a brute force algorithm,\\nbut such an algorithm is too slow if the input size is large. In fact, a frequent goal\\nin algorithm design is to ﬁnd O(n)orO(nlog n)time algorithms for problems that\\ncan be trivially solved in O(n2)time. Sorting is one way to achieve this goal.\\nFor example, suppose that we want to check if all elements in an array are unique.\\nA brute force algorithm goes through all pairs of elements in O(n2)time:\\nbool ok = true ;\\nfor (int i=0 ;i<n ;i + + ){\\nfor (int j = i+1; j < n; j++) {\\nif(array[i] == array[j]) ok = false ;\\n}\\n}', metadata={'source': 'documents\\\\doc.pdf', 'page': 50}),\n",
       " Document(page_content='44 4 Sorting and Searching\\nHowever, we can solve the problem in O(nlog n)time by ﬁrst sorting the array.\\nThen, if there are equal elements, they are next to each other in the sorted array, so\\nthey are easy to ﬁnd in O(n)time:\\nbool ok = true ;\\nsort(array, array+n);\\nfor (int i=0 ;i<n - 1 ;i + + ){\\nif(array[i] == array[i+1]) ok = false ;\\n}\\nSeveral other problems can be solved in a similar way in O(nlog n)time, such\\nas counting the number of distinct elements, ﬁnding the most frequent element, and\\nﬁnding two elements whose difference is minimum.\\n4.2.1 Sweep Line Algorithms\\nAsweep line algorithm models a problem as a set of events that are processed in\\na sorted order. For example, suppose that there is a restaurant and we know the\\narriving and leaving times of all customers on a certain day. Our task is to ﬁnd out\\nthe maximum number of customers who visited the restaurant at the same time.\\nFor example, Fig. 4.7 shows an instance of the problem where there are four\\ncustomers A,B,C, and D. In this case, the maximum number of simultaneous\\ncustomers is three between A’s arrival and B’s leaving.\\nTo solve the problem, we create two events for each customer: one event for\\narrival and another event for leaving. Then, we sort the events and go through them\\naccording to their times. To ﬁnd the maximum number of customers, we maintain\\na counter whose value increases when a customer arrives and decreases when a\\ncustomer leaves. The largest value of the counter is the answer to the problem.\\nFigure 4.8 shows the events in our example scenario. Each customer is assigned\\ntwo events: “ +” denotes an arriving customer and “ −” denotes a leaving customer.\\nThe resulting algorithm works in O(nlog n)time, because sorting the events takes\\nO(nlog n)time and the sweep line part takes O(n)time.\\nFig. 4.7 An instance of the\\nrestaurant problem\\nFig. 4.8 Solving the\\nrestaurant problem using a\\nsweep line algorithm', metadata={'source': 'documents\\\\doc.pdf', 'page': 51}),\n",
       " Document(page_content='4.2 Solving Problems by Sorting 45\\nFig. 4.9 An instance of the\\nscheduling problem and an\\noptimal solution with two\\nevents\\nFig. 4.10 If we select the\\nshort event, we can only\\nselect one event, but we\\ncould select both long events\\nFig. 4.11 If we select the\\nﬁrst event, we cannot select\\nany other events, but we\\ncould to select the other two\\nevents\\n4.2.2 Scheduling Events\\nMany scheduling problems can be solved by sorting the input data and then using a\\ngreedy strategy to construct a solution. A greedy algorithm always makes a choice\\nthat looks the best at the moment and never takes back its choices.\\nAs an example, consider the following problem: Given nevents with their starting\\nand ending times, ﬁnd a schedule that includes as many events as possible. For\\nexample, Fig. 4.9 shows an instance of the problem where an optimal solution is to\\nselect two events.\\nIn this problem, there are several ways how we could sort the input data. One\\nstrategy is to sort the events according to their lengths and select as short events as\\npossible. However, this strategy does not always work, as shown in Fig. 4.10 . Then,\\nanother idea is to sort the events according to their starting times and always select\\nthe next possible event that begins asearly as possible. However, we can ﬁnd a\\ncounterexample also for this strategy, shown in Fig. 4.11 .\\nA third idea is to sort the events according to their ending times and always select\\nthe next possible event that ends asearly as possible. It turns out that this algorithm\\nalways produces an optimal solution. To justify this, consider what happens if we\\nﬁrst select an event that ends later than the event that ends as early as possible. Now,\\nwe will have at most an equal number of choices left how we can select the next\\nevent. Hence, selecting an event that ends later can never yield a better solution, and\\nthe greedy algorithm is correct.\\n4.2.3 Tasks and Deadlines\\nFinally, consider a problem where we are given ntasks with durations and deadlines\\nand our task is to choose an order to perform the tasks. For each task, we earn d−x\\npoints where dis the task’s deadline and xis the moment when we ﬁnish the task.\\nWhat is the largest possible total score we can obtain?', metadata={'source': 'documents\\\\doc.pdf', 'page': 52}),\n",
       " Document(page_content='46 4 Sorting and Searching\\nFig. 4.12 An optimal\\nschedule for the tasks\\nFig. 4.13 Improving the\\nsolution by swapping tasks\\nXand Y\\nFor example, suppose that the tasks are as follows:\\ntask duration deadline\\nA 42\\nB 31 0\\nC 28\\nD 41 5\\nFigure 4.12 shows an optimal schedule for the tasks in our example scenario.\\nUsing this schedule, Cyields 6 points, Byields 5 points, Ayields −7 points, and D\\nyields 2 points, so the total score is 6.\\nIt turns out that the optimal solution to the problem does not depend on the\\ndeadlines at all, but a correct greedy strategy is to simply perform the tasks sorted\\nby their durations in increasing order. The reason for this is that if we ever perform\\ntwo tasks one after another such that the ﬁrst task takes longer than the second task,\\nwe can obtain a better solution if we swap the tasks.\\nFor example, in Fig. 4.13 , there are two tasks Xand Ywith durations aand\\nb. Initially, Xis scheduled before Y. However, since a>b, the tasks should be\\nswapped. Now Xgives bpoints less and Ygives apoints more, so the total score\\nincreases by a−b>0. Thus, in an optimal solution, a shorter task must always\\ncome before a longer task, and the tasks must be sorted by their durations.\\n4.3 Binary Search\\nBinary search is an O(log n)time algorithm that can be used, for example, to efﬁ-\\nciently check whether a sorted array contains a given element. In this section, we\\nﬁrst focus on the implementation of binary search, and after that, we will see how\\nbinary search can be used to ﬁnd optimal solutions for problems.', metadata={'source': 'documents\\\\doc.pdf', 'page': 53}),\n",
       " Document(page_content='4.3 Binary Search 47\\nFig. 4.14 The traditional\\nway to implement binary\\nsearch. At each step we\\ncheck the middle element of\\nthe active subarray and\\nproceed to the left or right\\npart\\n4.3.1 Implementing the Search\\nSuppose that we are given a sorted array of nelements and we want to check if\\nthe array contains an element with a target value x. Next we discuss two ways to\\nimplement a binary search algorithm for this problem.\\nFirst Method The most common way to implement binary search resembles looking\\nfor a word in a dictionary.3The search maintains an active subarray in the array, which\\ninitially contains all array elements. Then, a number of steps are performed, each of\\nwhich halves the search range. At each step, the search checks the middle element of\\nthe active subarray. If the middle element has the target value, the search terminates.\\nOtherwise, the search recursively continues to the left or right half of the subarray,\\ndepending on the value of the middle element. For example, Fig. 4.14 shows how an\\nelement with value 9 is found in the array.\\nThe search can be implemented as follows:\\nint a = 0, b = n-1;\\nwhile (a <= b) {\\nint k = (a+b)/2;\\nif(array[k] == x) {\\n// x found at index k\\n}\\nif(array[k] < x ) a = k+1;\\nelse b = k-1;\\n}\\nIn this implementation, the range of the active subarray is a... b, and the initial\\nrange is 0 ... n−1. The algorithm halves the size of the subarray at each step, so\\nthe time complexity is O(log n).\\n3Some people, including the author of this book, still use printed dictionaries. Another example is\\nﬁnding a phone number in a printed phone book, which is even more obsolete.', metadata={'source': 'documents\\\\doc.pdf', 'page': 54}),\n",
       " Document(page_content='48 4 Sorting and Searching\\nFig. 4.15 An alternative\\nway to implement binary\\nsearch. We scan the array\\nfrom left to right jumping\\nover elements\\nSecond Method Another way to implement binary search is to go through the array\\nfrom left to right making jumps . The initial jump length is n/2, and the jump length\\nis halved on each round: ﬁrst n/4, then n/8, then n/16, etc., until ﬁnally the length\\nis 1. On each round, we make jumps until we would end up outside the array or in\\nan element whose value exceeds the target value. After the jumps, either the desired\\nelement has been found or we know that it does not appear in the array. Figure 4.15\\nillustrates the technique in our example scenario.\\nThe following code implements the search:\\nint k=0 ;\\nfor (int b = n/2; b >= 1; b /= 2) {\\nwhile (k+b < n && array[k+b] <= x) k += b;\\n}\\nif(array[k] == x) {\\n// x found at index k\\n}\\nDuring the search, the variable bcontains the current jump length. The time\\ncomplexity of the algorithm is O(log n), because the code in the while loop is\\nperformed at most twice for each jump length.\\n4.3.2 Finding Optimal Solutions\\nSuppose that we are solving a problem and have a function valid (x)that returns\\ntrue ifxis a valid solution and false otherwise. In addition, we know that\\nvalid (x)isfalse when x<kandtrue when x≥k. In this situation, we can\\nuse binary search to efﬁciently ﬁnd the value of k.\\nThe idea is to binary search for the largest value of xfor which valid (x)is\\nfalse . Thus, the next value k=x+1 is the smallest possible value for which\\nvalid (k)istrue . The search can be implemented as follows:', metadata={'source': 'documents\\\\doc.pdf', 'page': 55}),\n",
       " Document(page_content='4.3 Binary Search 49\\nFig. 4.16 An optimal\\nprocessing schedule:\\nmachine 1 processes four\\njobs, machine 2 processes\\nthree jobs, and machine 3\\nprocesses one job\\nint x = -1;\\nfor (int b=z ;b> =1 ;b/ =2 ){\\nwhile (!valid(x+b)) x += b;\\n}\\nint k = x+1;\\nThe initial jump length zhas to be an upper bound for the answer, i.e., any value\\nfor which we surely know that valid (z)istrue . The algorithm calls the function\\nvalid O(log z)times, so the running time depends on the function valid .F o r\\nexample, if the function works in O(n)time, the running time is O(nlog z).\\nExample Consider a problem where our task is to process kjobs using nmachines.\\nEach machine iis assigned an integer pi: the time to process a single job. What is\\nthe minimum time to process all the jobs?\\nFor example, suppose that k=8,n=3 and the processing times are p1=2,\\np2=3, and p3=7. In this case, the minimum total processing time is 9, by following\\nthe schedule in Fig. 4.16 .\\nLetvalid (x)be a function that ﬁnds out whether it is possible to process all\\nthe jobs using at most xunits of time. In our example scenario, clearly valid (9)is\\ntrue , because we can follow the schedule in Fig. 4.16 . On the other hand, valid (8)\\nmust be false , because the minimum processing time is 9.\\nCalculating the value of valid (x)is easy, because each machine ican process\\nat most ⌊x/pi⌋jobs in xunits of time. Thus, if the sum of all ⌊x/pi⌋values is k\\nor more, xis a valid solution. Then, we can use binary search to ﬁnd the minimum\\nvalue of xfor which valid (x)istrue .\\nHow efﬁcient is the resulting algorithm? The function valid takes O(n)time,\\nso the algorithm works in O(nlog z)time, where zis an upper bound for the answer.\\nOne possible value for ziskp1which corresponds to a solution where only the ﬁrst\\nmachine is used to process all the jobs. This is surely a valid upper bound.', metadata={'source': 'documents\\\\doc.pdf', 'page': 56}),\n",
       " Document(page_content='Data Structures\\nThis chapter introduces the most important data structures of the C++ standard\\nlibrary. In competitive programming, it is crucial to know which data structures\\nare available in the standard library and how to use them. This often saves a large\\namount of time when implementing an algorithm.\\nSection 5.1ﬁrst describes the vector structure which is an efﬁcient dynamic array.\\nAfter this, we will focus on using iterators and ranges with data structures, and brieﬂy\\ndiscuss deques, stacks, and queues.\\nSection 5.2 discusses sets, maps and priority queues. Those data structures are\\noften used as building blocks of efﬁcient algorithms, because they allow us to main-\\ntain dynamic structures that support both efﬁcient searches and updates.\\nSection 5.3shows some results about the efﬁciency of data structures in practice.\\nAs we will see, there are important performance differences that cannot be detected\\nby only looking at time complexities.\\n5.1 Dynamic Arrays\\nIn C++, ordinary arrays are ﬁxed-size structures, and it is not possible to change the\\nsize of an array after creating it. For example, the following code creates an array\\nwhich contains ninteger values:\\nint array[n];\\nAdynamic array is an array whose size can be changed during the execution of\\nthe program. The C++ standard library provides several dynamic arrays, most useful\\nof them being the vector structure.', metadata={'source': 'documents\\\\doc.pdf', 'page': 57}),\n",
       " Document(page_content='52 5 Data Structures\\n5.1.1 Vectors\\nAvector is a dynamic array that allows us to efﬁciently add and remove elements\\nat the end of the structure. For example, the following code creates an empty vector\\nand adds three elements to it:\\nvector< int>v ;\\nv.push_back(3); // [3]\\nv.push_back(2); // [3,2]\\nv.push_back(5); // [3,2,5]\\nThen, the elements can be accessed like in an ordinary array:\\ncout << v[0] << \"\\\\n\"; // 3\\ncout << v[1] << \"\\\\n\"; // 2\\ncout << v[2] << \"\\\\n\"; // 5\\nAnother way to create a vector is to give a list of its elements:\\nvector< int> v = {2,4,2,5,1};\\nWe can also give the number of elements and their initial values:\\nvector< int> a(8); // size 8, initial value 0\\nvector< int> b(8,2); // size 8, initial value 2\\nThe function size returns the number of elements in the vector. For example,\\nthe following code iterates through the vector and prints its elements:\\nfor (int i=0 ;i<v .size(); i++) {\\ncout << v[i] << \"\\\\n\";\\n}\\nA shorter way to iterate through a vector is as follows:\\nfor (auto x:v ){\\ncout << x << \"\\\\n\";\\n}\\nThe function back returns the last element of a vector, and the function\\npop_back removes the last element:\\nvector< int> v = {2,4,2,5,1};\\ncout << v.back() << \"\\\\n\"; // 1\\nv.pop_back();\\ncout << v.back() << \"\\\\n\"; // 5', metadata={'source': 'documents\\\\doc.pdf', 'page': 58}),\n",
       " Document(page_content='5.1 Dynamic Arrays 53\\nV ectors are implemented so that the push_back andpop_back operations\\nwork in O(1)time on average. In practice, using a vector is almost as fast as using\\nan ordinary array.\\n5.1.2 Iterators and Ranges\\nAn iterator is a variable that points to an element of a data structure. The iterator\\nbegin points to the ﬁrst element of a data structure, and the iterator end points to\\nthe position after the last element. For example, the situation can look as follows in\\na vector vthat consists of eight elements:\\n[5 ,2 ,3 ,1 ,2 ,5 ,7 ,1]\\n↑↑\\nv.begin() v.end()\\nNote the asymmetry in the iterators: begin() points to an element in the data\\nstructure, while end() points outside the data structure.\\nArange is a sequence of consecutive elements in a data structure. The usual way\\nto specify a range is to give iterators to its ﬁrst element and the position after its\\nlast element. In particular, the iterators begin() andend() deﬁne a range that\\ncontains all elements in a data structure.\\nThe C++ standard library functions typically operate with ranges. For example,\\nthe following code ﬁrst sorts a vector, then reverses the order of its elements, and\\nﬁnally shufﬂes its elements.\\nsort(v.begin(),v.end());\\nreverse(v.begin(),v.end());\\nrandom_shuffle(v.begin(),v.end());\\nThe element to which an iterator points can be accessed using the *syntax. For\\nexample, the following code prints the ﬁrst element of a vector:\\ncout << *v.begin() << \"\\\\n\";\\nTo give a more useful example, lower_bound gives an iterator to the ﬁrst\\nelement in a sorted range whose value is at least x , and upper_bound gives an\\niterator to the ﬁrst element whose value is larger than x :\\nvector< int> v = {2,3,3,5,7,8,8,8};\\nauto a = lower_bound(v.begin(),v.end(),5);\\nauto b = upper_bound(v.begin(),v.end(),5);\\ncout << *a < <\"\"< <* b< <\" \\\\ n \" ; / /57\\nNote that the above functions only work correctly when the given range is sorted.\\nThe functions use binary search and ﬁnd the requested element in logarithmic time.', metadata={'source': 'documents\\\\doc.pdf', 'page': 59}),\n",
       " Document(page_content='54 5 Data Structures\\nIf there is no such element, the functions return an iterator to the element after the\\nlast element in the range.\\nThe C++ standard library contains a large number of useful functions that are\\nworth exploring. For example, the following code creates a vector that contains the\\nunique elements of the original vector in a sorted order:\\nsort(v.begin(),v.end());\\nv.erase(unique(v.begin(),v.end()),v.end());\\n5.1.3 Other Structures\\nAdeque is a dynamic array that can be efﬁciently manipulated at both ends of\\nthe structure. Like a vector, a deque provides the functions push_back and\\npop_back , but it also provides the functions push_front andpop_front\\nwhich are not available in a vector. A deque can be used as follows:\\ndeque< int>d ;\\nd.push_back(5); // [5]\\nd.push_back(2); // [5,2]\\nd.push_front(3); // [3,5,2]\\nd.pop_back(); // [3,5]\\nd.pop_front(); // [5]\\nThe operations of a deque also work in O(1)average time. However, deques have\\nlarger constant factors than vectors, so deques should be used only if there is a need\\nto manipulate both ends of the array.\\nC++ also provides two specialized data structures that are, by default, based on a\\ndeque. A stack has the functions push andpop for inserting and removing elements\\nat the end of the structure and the function top that retrieves the last element:\\nstack< int>s ;\\ns.push(2); // [2]\\ns.push(5); // [2,5]\\ncout << s.top() << \"\\\\n\"; // 5\\ns.pop(); // [2]\\ncout << s.top() << \"\\\\n\"; // 2\\nThen, in a queue , elements are inserted at the end of the structure and removed\\nfrom the front of the structure. Both the functions front andback are provided\\nfor accessing the ﬁrst and last element.', metadata={'source': 'documents\\\\doc.pdf', 'page': 60}),\n",
       " Document(page_content='5.1 Dynamic Arrays 55\\nqueue< int>q ;\\nq.push(2); // [2]\\nq.push(5); // [2,5]\\ncout << q.front() << \"\\\\n\"; // 2\\nq.pop(); // [5]\\ncout << q.back() << \"\\\\n\"; // 5\\n5.2 Set Structures\\nAsetis a data structure that maintains a collection of elements. The basic operations\\nof sets are element insertion, search, and removal. Sets are implemented so that all\\nthe above operations are efﬁcient, which often allows us to improve on running times\\nof algorithms using sets.\\n5.2.1 Sets and Multisets\\nThe C++ standard library contains two set structures:\\n•set is based on a balanced binary search tree and its operations work in O(log n)\\ntime.\\n•unordered_set is based on a hash table and its operations work, on average,1\\ninO(1)time.\\nBoth structures are efﬁcient, and often either of them can be used. Since they are\\nused in the same way, we focus on the set structure in the following examples.\\nThe following code creates a set that contains integers and shows some of its\\noperations. The function insert adds an element to the set, the function count\\nreturns the number of occurrences of an element in the set, and the function erase\\nremoves an element from the set.\\n1The worst-case time complexity of the operations is O(n), but this is very unlikely to occur.', metadata={'source': 'documents\\\\doc.pdf', 'page': 61}),\n",
       " Document(page_content='56 5 Data Structures\\nset< int>s ;\\ns.insert(3);\\ns.insert(2);\\ns.insert(5);\\ncout << s.count(3) << \"\\\\n\"; // 1\\ncout << s.count(4) << \"\\\\n\"; // 0\\ns.erase(3);\\ns.insert(4);\\ncout << s.count(3) << \"\\\\n\"; // 0\\ncout << s.count(4) << \"\\\\n\"; // 1\\nAn important property of sets is that all their elements are distinct . Thus, the\\nfunction count always returns either 0 (the element is not in the set) or 1 (the\\nelement is in the set), and the function insert never adds an element to the set if\\nit is already there. The following code illustrates this:\\nset< int>s ;\\ns.insert(3);\\ns.insert(3);\\ns.insert(3);\\ncout << s.count(3) << \"\\\\n\"; // 1\\nA set can be used mostly like a vector, but it is not possible to access the elements\\nusing the []notation. The following code prints the number of elements in a set and\\nthen iterates through the elements:\\ncout << s.size() << \"\\\\n\";\\nfor (auto x:s ){\\ncout << x << \"\\\\n\";\\n}\\nThe function find (x)returns an iterator that points to an element whose value\\nisx. However, if the set does not contain x, the iterator will be end() .\\nauto it = s.find(x);\\nif(it == s.end()) {\\n// x is not found\\n}\\nOrdered Sets The main difference between the two C++ set structures is that set\\nisordered , while unordered_set is not. Thus, if we want to maintain the order\\nof the elements, we have to use the set structure.\\nFor example, consider the problem of ﬁnding the smallest and largest value in a\\nset. To do this efﬁciently, we need to use the set structure. Since the elements are\\nsorted, we can ﬁnd the smallest and largest value as follows:', metadata={'source': 'documents\\\\doc.pdf', 'page': 62}),\n",
       " Document(page_content='5.2 Set Structures 57\\nauto first = s.begin();\\nauto last = s.end(); last--;\\ncout << *first < <\"\"< <* last << \"\\\\n\";\\nNote that since end() points to an element after the last element, we have to\\ndecrease the iterator by one.\\nThe set structure also provides the functions lower_bound (x)and\\nupper_bound (x)that return an iterator to the smallest element in a set whose\\nvalue is at least orlarger than x , respectively. In both the functions, if the requested\\nelement does not exist, the return value is end() .\\ncout << *s.lower_bound(x) << \"\\\\n\";\\ncout << *s.upper_bound(x) << \"\\\\n\";\\nMultisets Amultiset is a set that can have several copies of the same value. C++ has\\nthe structures multiset andunordered_multiset that resemble set and\\nunordered_set . For example, the following code adds three copies of the value\\n5 to a multiset.\\nmultiset< int>s ;\\ns.insert(5);\\ns.insert(5);\\ns.insert(5);\\ncout << s.count(5) << \"\\\\n\"; // 3\\nThe function erase removes all copies of a value from a multiset:\\ns.erase(5);\\ncout << s.count(5) << \"\\\\n\"; // 0\\nOften, only one value should be removed, which can be done as follows:\\ns.erase(s.find(5));\\ncout << s.count(5) << \"\\\\n\"; // 2\\nNote that the functions count anderase have an additional O(k)factor where\\nkis the number of elements counted/removed. In particular, it is not efﬁcient to count\\nthe number of copies of a value in a multiset using the count function.\\n5.2.2 Maps\\nAmap is a set that consists of key-value pairs. A map can also be seen as a gen-\\neralized array. While the keys in an ordinary array are always consecutive integers\\n0,1,..., n−1, where nis the size of the array, the keys in a map can be of any data\\ntype and they do not have to be consecutive values.', metadata={'source': 'documents\\\\doc.pdf', 'page': 63}),\n",
       " Document(page_content='58 5 Data Structures\\nThe C++ standard library contains two map structures that correspond to the set\\nstructures: map is based on a balanced binary search tree and accessing elements\\ntakes O(log n)time, while unordered_map uses hashing and accessing elements\\ntakes O(1)time on average.\\nThe following code creates a map whose keys are strings and values are integers:\\nmap<string, int>m ;\\nm[\"monkey\"] = 4;\\nm[\"banana\"] = 3;\\nm[\"harpsichord\"] = 9;\\ncout << m[\"banana\"] << \"\\\\n\"; // 3\\nIf the value of a key is requested but the map does not contain it, the key is\\nautomatically added to the map with a default value. For example, in the following\\ncode, the key “aybabtu” with value 0 is added to the map.\\nmap<string, int>m ;\\ncout << m[\"aybabtu\"] << \"\\\\n\"; // 0\\nThe function count checks if a key exists in a map:\\nif(m.count(\"aybabtu\")) {\\n// key exists\\n}\\nThen, the following code prints all keys and values in a map:\\nfor (auto x:m ){\\ncout << x.first < <\"\"< <x .second << \"\\\\n\";\\n}\\n5.2.3 Priority Queues\\nApriority queue is a multiset that supports element insertion and, depending on the\\ntype of the queue, retrieval and removal of either the minimum or maximum element.\\nInsertion and removal take O(log n)time, and retrieval takes O(1)time.\\nA priority queue is usually based on a heap structure, which is a special binary\\ntree. While a multiset provides all the operations of a priority queue and more,\\nthe beneﬁt of using a priority queue is that it has smaller constant factors. Thus, if\\nwe only need to efﬁciently ﬁnd minimum or maximum elements, it is a good idea to\\nuse a priority queue instead of a set or multiset.', metadata={'source': 'documents\\\\doc.pdf', 'page': 64}),\n",
       " Document(page_content='5.2 Set Structures 59\\nBy default, the elements in a C++ priority queue are sorted in decreasing order,\\nand it is possible to ﬁnd and remove the largest element in the queue. The following\\ncode illustrates this:\\npriority_queue< int>q ;\\nq.push(3);\\nq.push(5);\\nq.push(7);\\nq.push(2);\\ncout << q.top() << \"\\\\n\"; // 7\\nq.pop();\\ncout << q.top() << \"\\\\n\"; // 5\\nq.pop();\\nq.push(6);\\ncout << q.top() << \"\\\\n\"; // 6\\nq.pop();\\nIf we want to create a priority queue that supports ﬁnding and removing the\\nsmallest element, we can do it as follows:\\npriority_queue< int,vector< int>,greater< int>> q;\\n5.2.4 Policy-Based Sets\\nTheg++ compiler also provides some data structures that are not part of the C++\\nstandard library. Such structures are called policy-based structures. To use these\\nstructures, the following lines must be added to the code:\\n#include <ext/pb_ds/assoc_container.hpp>\\nusing namespace __gnu_pbds;\\nAfter this, we can deﬁne a data structure indexed_set that is like set but can\\nbe indexed like an array. The deﬁnition for int values is as follows:\\ntypedef tree< int,null_type,less< int>,rb_tree_tag,\\ntree_order_statistics_node_update> indexed_set;\\nThen, we can create a set as follows:\\nindexed_set s;\\ns.insert(2);\\ns.insert(3);\\ns.insert(7);\\ns.insert(9);', metadata={'source': 'documents\\\\doc.pdf', 'page': 65}),\n",
       " Document(page_content='60 5 Data Structures\\nThe speciality of this set is that we have access to the indices that the elements\\nwould have in a sorted array. The function find_by_order returns an iterator to\\nthe element at a given position:\\nauto x = s.find_by_order(2);\\ncout << *x << \"\\\\n\"; // 7\\nThen, the function order_of_key returns the position of a given element:\\ncout << s.order_of_key(7) << \"\\\\n\"; // 2\\nIf the element does not appear in the set, we get the position that the element\\nwould have in the set:\\ncout << s.order_of_key(6) << \"\\\\n\"; // 2\\ncout << s.order_of_key(8) << \"\\\\n\"; // 3\\nBoth the functions work in logarithmic time.\\n5.3 Experiments\\nIn this section, we present some results concerning the practical efﬁciency of the\\ndata structures presented in this chapter. While time complexities are a great tool,\\nthey do not always tell the whole truth about the efﬁciency, so it is worthwhile to\\nalso do experiments with real implementations and data sets.\\n5.3.1 Set Versus Sorting\\nMany problems can be solved using either sets or sorting. It is important to realize\\nthat algorithms that use sorting are usually much faster, even if this is not evident by\\njust looking at the time complexities.\\nAs an example, consider the problem of calculating the number of unique elements\\nin a vector. One way to solve the problem is to add all the elements to a set and return\\nthe size of the set. Since it is not needed to maintain the order of the elements, we\\nmay use either a set or an unordered_set . Then, another way to solve the\\nproblem is to ﬁrst sort the vector and then go through its elements. It is easy to count\\nthe number of unique elements after sorting the vector.\\nTable 5.1 shows the results of an experiment where the above algorithms were\\ntested using random vectors of int values. It turns out that the unordered_set', metadata={'source': 'documents\\\\doc.pdf', 'page': 66}),\n",
       " Document(page_content='5.3 Experiments 61\\nTable 5.1 The results of an experiment where the number of unique elements in a vector was\\ncalculated. The ﬁrst two algorithms insert the elements to a set structure, while the last algorithm\\nsorts the vector and inspects consecutive elements\\nInput size n set (s) unordered_set (s) Sorting (s)\\n1060.65 0.34 0.11\\n2·1061.50 0.76 0.18\\n4·1063.38 1.63 0.33\\n8·1067.57 3.45 0.68\\n16·10617.35 7.18 1.38\\nTable 5.2 The results of an experiment where the most frequent value in a vector was determined.\\nThe two ﬁrst algorithms use map structures, and the last algorithm uses an ordinary array\\nInput size n map (s) unordered_map (s) Array (s)\\n1060.55 0.23 0.01\\n2·1061.14 0.39 0.02\\n4·1062.34 0.73 0.03\\n8·1064.68 1.46 0.06\\n16·1069.57 2.83 0.11\\nalgorithm is about two times faster than the set algorithm, and the sorting algorithm\\nis more than ten times faster than the set algorithm. Note that both the set algorithm\\nand the sorting algorithm work in O(nlog n)time; still the latter is much faster. The\\nreason for this is that sorting is a simple operation, while the balanced binary search\\ntree used in set is a complex data structure.\\n5.3.2 Map Versus Array\\nMaps are convenient structures compared to arrays, because any indices can be used,\\nbut they also have large constant factors. In our next experiment, we created a vector\\nofnrandom integers between 1 and 106and then determined the most frequent value\\nby counting the number of each element. First we used maps, but since the upper\\nbound 106is quite small, we were also able to use arrays.\\nTable 5.2shows the results of the experiment. While unordered_map is about\\nthree times faster than map , an array is almost a hundred times faster. Thus, arrays\\nshould be used whenever possible instead of maps. Especially, note that while\\nunordered_map provides O(1)time operations, there are large constant factors\\nhidden in the data structure.', metadata={'source': 'documents\\\\doc.pdf', 'page': 67}),\n",
       " Document(page_content='62 5 Data Structures\\nTable 5.3 The results of an experiment where elements were added and removed using a multiset\\nand a priority queue\\nInput size n multiset (s) priority_queue (s)\\n1061.17 0.19\\n2·1062.77 0.41\\n4·1066.10 1.05\\n8·10613.96 2.52\\n16·10630.93 5.95\\n5.3.3 Priority Queue Versus Multiset\\nAre priority queues really faster than multisets? To ﬁnd out this, we conducted\\nanother experiment where we created two vectors of nrandom int numbers. First,\\nwe added all elements of the ﬁrst vector to a data structure. Then, we went through the\\nsecond vector and repeatedly removed the smallest element from the data structure\\nand added the new element to it.\\nTable 5.3 shows the results of the experiment. It turns out that in this problem a\\npriority queue is about ﬁve times faster than a multiset.', metadata={'source': 'documents\\\\doc.pdf', 'page': 68}),\n",
       " Document(page_content='Dynamic Programming\\nDynamic programming is an algorithm design technique that can be used to ﬁnd\\noptimal solutions to problems and to count the number of solutions. This chapter\\nis an introduction to dynamic programming, and the technique will be used many\\ntimes later in the book when designing algorithms.\\nSection 6.1discusses the basic elements of dynamic programming in the context\\nof a coin change problem. In this problem we are given a set of coin values and our\\ntask is to construct a sum of money using as few coins as possible. There is a simple\\ngreedy algorithm for the problem, but as we will see, it does not always produce an\\noptimal solution. However, using dynamic programming, we can create an efﬁcient\\nalgorithm that always ﬁnds an optimal solution.\\nSection 6.2 presents a selection of problems that show some of the possibilities\\nof dynamic programming. The problems include determining the longest increasing\\nsubsequence in an array, ﬁnding an optimal path in a two-dimensional grid, and\\ngenerating all possible weight sums in a knapsack problem.\\n6.1 Basic Concepts\\nIn this section, we go through the basic concepts of dynamic programming in the\\ncontext of a coin change problem. First we present a greedy algorithm for the problem,\\nwhich does not always produce an optimal solution. After this, we show how the\\nproblem can be efﬁciently solved using dynamic programming.\\n6.1.1 When Greedy Fails\\nSuppose that we are given a set of coin values coins ={c1,c2,..., ck}and a\\ntarget sum of money n, and we are asked to construct the sum nusing as few coins as', metadata={'source': 'documents\\\\doc.pdf', 'page': 69}),\n",
       " Document(page_content='64 6 Dynamic Programming\\npossible. There are no restrictions on how many times we can use each coin value. For\\nexample, if coins ={1,2,5}andn=12, the optimal solution is 5 +5+2=12,\\nwhich requires three coins.\\nThere is a natural greedy algorithm for solving the problem: always select the\\nlargest possible coin so that the sum of coin values does not exceed the target sum.\\nFor example, if n=12, we ﬁrst select two coins of value 5, and then one coin of\\nvalue 2, which completes the solution. This looks like a reasonable strategy, but is it\\nalways optimal?\\nIt turns out that this strategy does not always work. For example, if coins =\\n{1,3,4}and n=6, the optimal solution has only two coins (3 +3=6) but the\\ngreedy strategy produces a solution with three coins (4 +1+1=6). This simple\\ncounterexample shows that the greedy algorithm is not correct.1\\nHow could we solve the problem, then? Of course, we could try to ﬁnd another\\ngreedy algorithm, but there are no other obvious strategies that we could consider.\\nAnother possibility would be to create a brute force algorithm that goes through all\\npossible ways to select coins. Such an algorithm would surely give correct results,\\nbut it would be very slow on large inputs.\\nHowever, using dynamic programming, we can create an algorithm that is almost\\nlike a brute force algorithm but it is also efﬁcient . Thus, we can both be sure that the\\nalgorithm is correct and use it for processing large inputs. Furthermore, we can use\\nthe same technique for solving a large number of other problems.\\n6.1.2 Finding an Optimal Solution\\nTo use dynamic programming, we should formulate the problem recursively so that\\nthe solution to the problem can be calculated from solutions to smaller subproblems.\\nIn the coin problem, a natural recursive problem is to calculate values of a function\\nsolve (x): what is the minimum number of coins required to form a sum x? Clearly,\\nthe values of the function depend on the values of the coins. For example, if coins =\\n{1,3,4}, the ﬁrst values of the function are as follows:\\nsolve (0)=0\\nsolve (1)=1\\nsolve (2)=2\\nsolve (3)=1\\nsolve (4)=1\\nsolve (5)=2\\nsolve (6)=2\\nsolve (7)=2\\nsolve (8)=2\\nsolve (9)=3\\nsolve (10)=3\\n1It is an interesting question when exactly does the greedy algorithm work. Pearson [ 24] describes\\nan efﬁcient algorithm for testing this.', metadata={'source': 'documents\\\\doc.pdf', 'page': 70}),\n",
       " Document(page_content='6.1 Basic Concepts 65\\nFor example, solve (10)=3, because at least 3 coins are needed to form the\\nsum 10. The optimal solution is 3 +3+4=10.\\nThe essential property of solve is that its values can be recursively calculated\\nfrom its smaller values. The idea is to focus on the ﬁrst coin that we choose for the\\nsum. For example, in the above scenario, the ﬁrst coin can be either 1, 3 or 4. If\\nwe ﬁrst choose coin 1, the remaining task is to form the sum 9 using the minimum\\nnumber of coins, which is a subproblem of the original problem. Of course, the\\nsame applies to coins 3 and 4. Thus, we can use the following recursive formula to\\ncalculate the minimum number of coins:\\nsolve (x)=min(solve (x−1)+1,\\nsolve (x−3)+1,\\nsolve (x−4)+1).\\nThe base case of the recursion is solve (0)=0, because no coins are needed to\\nform an empty sum. For example,\\nsolve (10)=solve (7)+1=solve (4)+2=solve (0)+3=3.\\nNow we are ready to give a general recursive function that calculates the minimum\\nnumber of coins needed to form a sum x:\\nsolve (x)=⎧\\n⎪⎨\\n⎪⎩∞ x<0\\n0 x=0\\nmin c∈coins solve (x−c)+1x>0\\nFirst, if x<0, the value is inﬁnite, because it is impossible to form a negative\\nsum of money. Then, if x=0, the value is zero, because no coins are needed to form\\nan empty sum. Finally, if x>0, the variable cgoes through all possibilities how to\\nchoose the ﬁrst coin of the sum.\\nOnce a recursive function that solves the problem has been found, we can directly\\nimplement a solution in C++ (the constant INF denotes inﬁnity):\\nint solve( int x) {\\nif( x<0 ) return INF;\\nif(x == 0) return 0;\\nint best = INF;\\nfor (auto c : coins) {\\nbest = min(best, solve(x-c)+1);\\n}\\nreturn best;\\n}\\nStill, this function is not efﬁcient, because there may be a large number of ways to\\nconstruct the sum and the function checks all of them. Fortunately, it turns out that\\nthere is a simple way to make the function efﬁcient.', metadata={'source': 'documents\\\\doc.pdf', 'page': 71}),\n",
       " Document(page_content='66 6 Dynamic Programming\\nMemoization The key idea in dynamic programming is memoization , which means\\nthat we store each function value in an array directly after calculating it. Then, when\\nthe value is needed again, it can be retrieved from the array without recursive calls.\\nTo do this, we create arrays\\nbool ready[N];\\nint value[N];\\nwhere ready [x]indicates whether the value of solve (x)has been calculated, and\\nif it is, value [x]contains this value. The constant Nhas been chosen so that all\\nrequired values ﬁt in the arrays.\\nAfter this, the function can be efﬁciently implemented as follows:\\nint solve( int x) {\\nif( x<0 ) return INF;\\nif(x == 0) return 0;\\nif(ready[x]) return value[x];\\nint best = INF;\\nfor (auto c : coins) {\\nbest = min(best, solve(x-c)+1);\\n}\\nready[x] = true ;\\nvalue[x] = best;\\nreturn best;\\n}\\nThe function handles the base cases x<0 and x=0 as previously. Then it\\nchecks from ready [x]ifsolve (x)has already been stored in value [x], and if\\nit is, the function directly returns it. Otherwise the function calculates the value of\\nsolve (x)recursively and stores it in value [x].\\nThis function works efﬁciently, because the answer for each parameter xis calcu-\\nlated recursively only once. After a value of solve (x)has been stored in value [x],\\nit can be efﬁciently retrieved whenever the function will be called again with the para-\\nmeter x. The time complexity of the algorithm is O(nk), where nis the target sum\\nandkis the number of coins.\\nIterative Implementation Note that we can also iteratively construct the array\\nvalue using a loop as follows:\\nvalue[0] = 0;\\nfor (int x = 1; x <= n; x++) {\\nvalue[x] = INF;\\nfor (auto c : coins) {\\nif(x-c >= 0) {\\nvalue[x] = min(value[x], value[x-c]+1);\\n}\\n}\\n}', metadata={'source': 'documents\\\\doc.pdf', 'page': 72}),\n",
       " Document(page_content='6.1 Basic Concepts 67\\nIn fact, most competitive programmers prefer this implementation, because it\\nis shorter and has smaller constant factors. From now on, we also use iterative\\nimplementations in our examples. Still, it is often easier to think about dynamic\\nprogramming solutions in terms of recursive functions.\\nConstructing a Solution Sometimes we are asked both to ﬁnd the value of an optimal\\nsolution and to give an example how such a solution can be constructed. To construct\\nan optimal solution in our coin problem, we can declare a new array that indicates\\nfor each sum of money the ﬁrst coin in an optimal solution:\\nint first[N];\\nThen, we can modify the algorithm as follows:\\nvalue[0] = 0;\\nfor (int x = 1; x <= n; x++) {\\nvalue[x] = INF;\\nfor (auto c : coins) {\\nif(x-c >= 0 && value[x-c]+1 < value[x]) {\\nvalue[x] = value[x-c]+1;\\nfirst[x] = c;\\n}\\n}\\n}\\nAfter this, the following code prints the coins that appear in an optimal solution\\nfor the sum n:\\nwhile ( n>0 ){\\ncout << first[n] << \"\\\\n\";\\nn -= first[n];\\n}\\n6.1.3 Counting Solutions\\nLet us now consider another variant of the coin problem where our task is to calcu-\\nlate the total number of ways to produce a sum xusing the coins. For example, if\\ncoins ={1,3,4}andx=5, there are a total of 6 ways:\\n•1+1+1+1+1\\n•1+1+3\\n•1+3+1•3+1+1\\n•1+4\\n•4+1\\nAgain, we can solve the problem recursively. Let solve (x)denote the number of\\nways we can form the sum x. For example, if coins ={1,3,4}, then solve (5)=6\\nand the recursive formula is', metadata={'source': 'documents\\\\doc.pdf', 'page': 73}),\n",
       " Document(page_content='68 6 Dynamic Programming\\nsolve (x)=solve (x−1)+\\nsolve (x−3)+\\nsolve (x−4).\\nThen, the general recursive function is as follows:\\nsolve (x)=⎧\\n⎪⎨\\n⎪⎩0 x<0\\n1 x=0∑\\nc∈coins solve (x−c)x>0\\nIfx<0, the value is zero, because there are no solutions. If x=0, the value is\\none, because there is only one way to form an empty sum. Otherwise we calculate\\nthe sum of all values of the form solve (x−c)where cis incoins .\\nThe following code constructs an array count such that count [x]equals the\\nvalue of solve (x)for 0≤x≤n:\\ncount[0] = 1;\\nfor (int x = 1; x <= n; x++) {\\nfor (auto c : coins) {\\nif(x-c >= 0) {\\ncount[x] += count[x-c];\\n}\\n}\\n}\\nOften the number of solutions is so large that it is not required to calculate the\\nexact number but it is enough to give the answer modulo mwhere, for example,\\nm=109+7. This can be done by changing the code so that all calculations are\\ndone modulo m. In the above code, it sufﬁces to add the line\\ncount[x] %= m;\\nafter the line\\ncount[x] += count[x-c];\\n6.2 Further Examples\\nAfter having discussed the basic concepts of dynamic programming, we are now\\nready to go through a set of problems that can be efﬁciently solved using dynamic\\nprogramming. As we will see, dynamic programming is a versatile technique that\\nhas many applications in algorithm design.', metadata={'source': 'documents\\\\doc.pdf', 'page': 74}),\n",
       " Document(page_content='6.2 Further Examples 69\\nFig. 6.1 The longest\\nincreasing subsequence of\\nthis array is [2,5,7,8]\\n6.2.1 Longest Increasing Subsequence\\nThe longest increasing subsequence in an array of nelements is a maximum-length\\nsequence of array elements that goes from left to right, and each element in the\\nsequence is larger than the previous element. For example, Fig. 6.1shows the longest\\nincreasing subsequence in an array of eight elements.\\nWe can efﬁciently ﬁnd the longest increasing subsequence in an array using\\ndynamic programming. Let length (k)denote the length of the longest increasing\\nsubsequence that ends at position k. Then, if we calculate all values of length (k)\\nwhere 0 ≤k≤n−1, we will ﬁnd out the length of the longest increasing subse-\\nquence. The values of the function for our example array are as follows:\\nlength (0)=1\\nlength (1)=1\\nlength (2)=2\\nlength (3)=1\\nlength (4)=3\\nlength (5)=2\\nlength (6)=4\\nlength (7)=2\\nFor example, length (6)=4, because the longest increasing subsequence that\\nends at position 6 consists of 4 elements.\\nTo calculate a value of length (k), we should ﬁnd a position i<kfor which\\narray [i]<array [k]andlength (i)is as large as possible. Then we know that\\nlength (k)=length (i)+1, because this is an optimal way to append array [k]\\nto a subsequence. However, if there is no such position i, then length (k)=1,\\nwhich means that the subsequence only contains array [k].\\nSince all values of the function can be calculated from its smaller values, we can\\nuse dynamic programming to calculate the values. In the following code, the values\\nof the function will be stored in an array length .\\nfor (int k=0 ;k<n ;k + + ){\\nlength[k] = 1;\\nfor (int i=0 ;i<k ; i++) {\\nif(array[i] < array[k]) {\\nlength[k] = max(length[k],length[i]+1);\\n}\\n}\\n}', metadata={'source': 'documents\\\\doc.pdf', 'page': 75}),\n",
       " Document(page_content='70 6 Dynamic Programming\\nFig. 6.2 An optimal path\\nfrom the upper-left corner to\\nthe lower-right corner\\nFig. 6.3 Two possible ways\\nto reach a square on a path\\nThe resulting algorithm clearly works in O(n2)time.2\\n6.2.2 Paths in a Grid\\nOur next problem is to ﬁnd a path from the upper-left corner to the lower-right corner\\nof an n×ngrid, with the restriction that we may only move down and right. Each\\nsquare contains an integer, and the path should be constructed so that the sum of the\\nvalues along the path is as large as possible.\\nAs an example, Fig. 6.2 shows an optimal path in a 5 ×5 grid. The sum of the\\nvalues on the path is 67, and this is the largest possible sum on a path from the\\nupper-left corner to the lower-right corner.\\nAssume that the rows and columns of the grid are numbered from 1 to n, and\\nvalue [y][x]equals the value of square (y,x). Letsum(y,x)denote the maximum\\nsum on a path from the upper-left corner to square (y,x). Then, sum(n,n)tells us\\nthe maximum sum from the upper-left corner to the lower-right corner. For example,\\nin the above grid, sum(5,5)=67. Now we can use the formula\\nsum(y,x)=max(sum(y,x−1),sum(y−1,x))+value [y][x],\\nwhich is based on the observation that a path that ends at square (y,x)can come\\neither from square (y,x−1)or from square (y−1,x)(Fig. 6.3). Thus, we select\\nthe direction that maximizes the sum. We assume that sum(y,x)=0i f y=0o r\\nx=0, so the recursive formula also works for leftmost and topmost squares.\\nSince the function sum has two parameters, the dynamic programming array also\\nhas two dimensions. For example, we can use an array\\n2In this problem, it is also possible to calculate the dynamic programming values more efﬁciently\\ninO(nlogn)time. Can you ﬁnd a way to do this?', metadata={'source': 'documents\\\\doc.pdf', 'page': 76}),\n",
       " Document(page_content='6.2 Further Examples 71\\nint sum[N][N];\\nand calculate the sums as follows:\\nfor (int y = 1; y <= n; y++) {\\nfor (int x=1 ;x< =n ;x + + ){\\nsum[y][x] = max(sum[y][x-1],sum[y-1][x])+value[y][x];\\n}\\n}\\nThe time complexity of the algorithm is O(n2).\\n6.2.3 Knapsack Problems\\nThe term knapsack refers to problems where a set of objects is given, and subsets\\nwith some properties have to be found. Knapsack problems can often be solved using\\ndynamic programming.\\nIn this section, we focus on the following problem: Given a list of weights\\n[w1,w2,..., wn], determine all sums that can be constructed using the weights.\\nFor example, Fig. 6.4 shows the possible sums for weights [1,3,3,5]. In this case,\\nall sums between 0 ...12 are possible, except 2 and 10. For example, the sum 7 is\\npossible because we can choose the weights [1,3,3].\\nTo solve the problem, we focus on subproblems where we only use the ﬁrst k\\nweights to construct sums. Let possible (x,k)=true if we can construct a sum\\nxusing the ﬁrst kweights, and otherwise possible (x,k)=false . The values\\nof the function can be recursively calculated using the formula\\npossible (x,k)=possible (x−wk,k−1)orpossible (x,k−1),\\nwhich is based on the fact that we can either use or not use the weight wkin the\\nsum. If we use wk, the remaining task is to form the sum x−wkusing the ﬁrst k−1\\nweights, and if we do not use wk, the remaining task is to form the sum xusing the\\nﬁrst k−1 weights. The base cases are\\npossible (x,0)={\\ntrue x=0\\nfalse x̸=0,\\nbecause if no weights are used, we can only form the sum 0. Finally, possible (x,n)\\ntells us whether we can construct a sum xusing allweights.\\nFig. 6.4 Constructing sums\\nusing the weights [1,3,3,5]', metadata={'source': 'documents\\\\doc.pdf', 'page': 77}),\n",
       " Document(page_content='72 6 Dynamic Programming\\nFig. 6.5 Solving the\\nknapsack problem for the\\nweights [1,3,3,5]using\\ndynamic programming\\nFigure 6.5shows all values of the function for the weights [1,3,3,5](the symbol\\n“✓” indicates the true values). For example, the row k=2 tells us that we can\\nconstruct the sums [0,1,3,4]using the weights [1,3].\\nLetmdenote the total sum of the weights. The following O(nm)time dynamic\\nprogramming solution corresponds to the recursive function:\\npossible[0][0] = true ;\\nfor (int k = 1; k <= n; k++) {\\nfor (int x=0 ;x< =m ;x + + ){\\nif(x-w[k] >= 0) {\\npossible[x][k] |= possible[x-w[k]][k-1];\\n}\\npossible[x][k] |= possible[x][k-1];\\n}\\n}\\nIt turns out that there is also a more compact way to implement the dynamic\\nprogramming calculation, using only a one-dimensional array possible [x]that\\nindicates whether we can construct a subset with sum x. The trick is to update the\\narray from right to left for each new weight:\\npossible[0] = true ;\\nfor (int k = 1; k <= n; k++) {\\nfor (int x = m-w[k]; x >= 0; x--) {\\npossible[x+w[k]] |= possible[x];\\n}\\n}\\nNote that the general dynamic programming idea presented in this section can\\nalso be used in other knapsack problems, such as in a situation where objects have\\nweights and values and we have to ﬁnd a maximum-value subset whose weight does\\nnot exceed a given limit.\\n6.2.4 From Permutations to Subsets\\nUsing dynamic programming, it is often possible to change an iteration over per-\\nmutations into an iteration over subsets. The beneﬁt of this is that n!, the number of', metadata={'source': 'documents\\\\doc.pdf', 'page': 78}),\n",
       " Document(page_content='6.2 Further Examples 73\\npermutations, is much larger than 2n, the number of subsets. For example, if n=20,\\nn!≈ 2.4·1018and 2n≈106. Thus, for certain values of n, we can efﬁciently go\\nthrough the subsets but not through the permutations.\\nAs an example, consider the following problem: There is an elevator with maxi-\\nmum weight x, and npeople who want to get from the ground ﬂoor to the top ﬂoor.\\nThe people are numbered 0 ,1,..., n−1, and the weight of person iisweight [i].\\nWhat is the minimum number of rides needed to get everybody to the top ﬂoor?\\nFor example, suppose that x=12,n=5, and the weights are as follows:\\n•weight [0]= 2\\n•weight [1]= 3\\n•weight [2]= 4\\n•weight [3]= 5\\n•weight [4]= 9\\nIn this scenario, the minimum number of rides is two. One optimal solution is as\\nfollows: ﬁrst, people 0, 2, and 3 take the elevator (total weight 11), and then, people\\n1 and 4 take the elevator (total weight 12).\\nThe problem can be easily solved in O(n!n)time by testing all possible permu-\\ntations of npeople. However, we can use dynamic programming to create a more\\nefﬁcient O(2nn)time algorithm. The idea is to calculate for each subset of people\\ntwo values: the minimum number of rides needed and the minimum weight of people\\nwho ride in the last group.\\nLetrides (S)denote the minimum number of rides for a subset S, and let\\nlast(S)denote the minimum weight of the last ride in a solution where the number\\nof rides is minimum. For example, in the above scenario\\nrides ({3,4})=2 and last({3,4})=5,\\nbecause the optimal way for people 3 and 4 to get to the top ﬂoor is that they take\\ntwo separate rides and person 4 goes ﬁrst, which minimizes the weight of the second\\nride. Of course, our ﬁnal goal is to calculate the value of rides ({0...n−1}).\\nWe can calculate the values of the functions recursively and then apply dynamic\\nprogramming. To calculate the values for a subset S, we go through all people who\\nbelong to Sand optimally choose the last person pwho enters the elevator. Each\\nsuch choice yields a subproblem for a smaller subset of people. If last(S\\\\p)+\\nweight [p]≤x, we can add pto the last ride. Otherwise, we have to reserve a new\\nride that only contains p.\\nA convenient way to implement the dynamic programming calculation is to use\\nbit operations. First, we declare an array\\npair< int,int> best[1<<N];\\nthat contains for each subset Sa pair (rides (S),last(S)). For an empty subset,\\nno rides are needed:', metadata={'source': 'documents\\\\doc.pdf', 'page': 79}),\n",
       " Document(page_content='74 6 Dynamic Programming\\nFig. 6.6 One way to ﬁll the\\n4×7g r i du s i n g1 ×2a n d\\n2×1t i l e s\\nbest[0] = {0,0};\\nThen, we can ﬁll the array as follows:\\nfor (int s = 1; s < (1<<n); s++) {\\n// initial value: n+1 rides are needed\\nbest[s] = {n+1,0};\\nfor (int p=0 ;p<n ; p++) {\\nif(s&(1<<p)) {\\nauto option = best[s^(1<<p)];\\nif(option.second+weight[p] <= x) {\\n// add p to an existing ride\\noption.second += weight[p];\\n}else {\\n// reserve a new ride for p\\noption.first++;\\noption.second = weight[p];\\n}\\nbest[s] = min(best[s], option);\\n}\\n}\\n}\\nNote that the above loop guarantees that for any two subsets S1and S2such\\nthat S1⊂S2, we process S1before S2. Thus, the dynamic programming values are\\ncalculated in the correct order.\\n6.2.5 Counting Tilings\\nSometimes the states of a dynamic programming solution are more complex than\\nﬁxed combinations of values. As an example, consider the problem of calculating\\nthe number of distinct ways to ﬁll an n×mgrid using 1 ×2 and 2 ×1 size tiles. For\\nexample, there are a total of 781 ways to ﬁll the 4 ×7 grid, one of them being the\\nsolution shown in Fig. 6.6.\\nThe problem can be solved using dynamic programming by going through the\\ngrid row by row. Each row in a solution can be represented as a string that contains', metadata={'source': 'documents\\\\doc.pdf', 'page': 80}),\n",
       " Document(page_content='6.2 Further Examples 75\\nmcharacters from the set {⊓,⊔,⊏,⊐}. For example, the solution in Fig. 6.6consists\\nof four rows that correspond to the following strings:\\n•⊓⊏⊐⊓⊏⊐⊓\\n•⊔⊏⊐⊔⊓⊓ ⊔\\n•⊏⊐⊏⊐ ⊔⊔⊓\\n•⊏⊐⊏⊐⊏⊐ ⊔\\nSuppose that the rows of the grid are indexed from 1 to n. Let count (k,x)\\ndenote the number of ways to construct a solution for rows 1 ...ksuch that string x\\ncorresponds to row k. It is possible to use dynamic programming here, because the\\nstate of a row is constrained only by the state of the previous row.\\nA solution is valid if row 1 does not contain the character ⊔,r o w ndoes not\\ncontain the character ⊓, and all consecutive rows are compatible. For example, the\\nrows⊔⊏⊐⊔⊓⊓⊔ and⊏⊐⊏⊐ ⊔⊔⊓ are compatible, while the rows ⊓⊏⊐⊓⊏⊐⊓\\nand⊏⊐⊏⊐⊏⊐ ⊔are not compatible.\\nSince a row consists of mcharacters and there are four choices for each character,\\nthe number of distinct rows is at most 4m. We can go through the O(4m)possible\\nstates for each row, and for each state, there are O(4m)possible states for the previous\\nrow, so the time complexity of the solution is O(n42m). In practice, it is a good idea to\\nrotate the grid so that the shorter side has length m, because the factor 42mdominates\\nthe time complexity.\\nIt is possible to make the solution more efﬁcient by using a more compact rep-\\nresentation for the rows. It turns out that it sufﬁces to know which columns of the\\nprevious row contain the upper square of a vertical tile. Thus, we can represent a\\nrow using only the characters ⊓and□, where □is a combination of the characters\\n⊔,⊏, and⊐. Using this representation, there are only 2mdistinct rows, and the time\\ncomplexity is O(n22m).\\nAs a ﬁnal note, there is also a direct formula for calculating the number of tilings:\\n⌈n/2⌉∏\\na=1⌈m/2⌉∏\\nb=14·(\\ncos2πa\\nn+1+cos2πb\\nm+1)\\nThis formula is very efﬁcient, because it calculates the number of tilings in O(nm)\\ntime, but since the answer is a product of real numbers, a problem when using the\\nformula is how to store the intermediate results accurately.', metadata={'source': 'documents\\\\doc.pdf', 'page': 81}),\n",
       " Document(page_content='Graph Algorithms\\nMany programming problems can be solved by considering the situation as a graph\\nand using an appropriate graph algorithm. In this chapter, we will learn the basics of\\ngraphs and a selection of important graph algorithms.\\nSection 7.1 discusses graph terminology and data structures that can be used to\\nrepresent graphs in algorithms.\\nSection 7.2 introduces two fundamental graph traversal algorithms. Depth-ﬁrst\\nsearch is a simple way to visit all nodes that can be reached from a starting node,\\nand breadth-ﬁrst search visits the nodes in increasing order of their distance from\\nthe starting node.\\nSection 7.3 presents algorithms for ﬁnding shortest paths in weighted graphs.\\nThe Bellman–Ford algorithm is a simple algorithm that ﬁnds shortest paths from a\\nstarting node to all other nodes. Dijkstra’s algorithm is a more efﬁcient algorithm\\nwhich requires that all edge weights are nonnegative. The Floyd–Warshall algorithm\\ndetermines shortest paths between all node pairs of a graph.\\nSection 7.4 explores special properties of directed acyclic graphs. We will learn\\nhow to construct a topological sort and how to use dynamic programming to efﬁ-\\nciently process such graphs.\\nSection 7.5focuses on successor graphs where each node has a unique successor.\\nWe will discuss an efﬁcient way to ﬁnd successors of nodes and Floyd’s algorithm\\nfor cycle detection.\\nSection 7.6 presents Kruskal’s and Prim’s algorithms for constructing minimum\\nspanning trees. Kruskal’s algorithm is based on an efﬁcient union-ﬁnd structure\\nwhich has also other uses in algorithm design.', metadata={'source': 'documents\\\\doc.pdf', 'page': 82}),\n",
       " Document(page_content='78 7 Graph Algorithms\\n7.1 Basics of Graphs\\nIn this section, we ﬁrst go through terminology which is used when discussing\\ngraphs and their properties. After this, we focus on data structures that can be used\\nto represent graphs in algorithm programming.\\n7.1.1 Graph Terminology\\nAgraph consists of nodes (also called vertices ) that are connected with edges .I n\\nthis book, the variable ndenotes the number of nodes in a graph, and the variable\\nmdenotes the number of edges. The nodes are numbered using integers 1 ,2,..., n.\\nFor example, Fig. 7.1shows a graph with 5 nodes and 7 edges.\\nApath leads from a node to another node through the edges of the graph. The\\nlength of a path is the number of edges in it. For example, Fig. 7.2 shows a path\\n1→3→4→5 of length 3 from node 1 to node 5. A cycle is a path where the ﬁrst\\nand last node is the same. For example, Fig. 7.3shows a cycle 1 →3→4→1.\\nA graph is connected if there is a path between any two nodes. In Fig. 7.4, the left\\ngraph is connected, but the right graph is not connected, because it is not possible to\\nget from node 4 to any other node.\\nThe connected parts of a graph are called its components . For example, the graph\\nin Fig. 7.5has three components: {1,2,3},{4,5,6,7}, and{8}.\\nAtree is a connected graph that does not contain cycles. Figure 7.6 shows an\\nexample of a graph that is a tree.\\nIn a directed graph, the edges can be traversed in one direction only. Figure 7.7\\nshows an example of a directed graph. This graph contains a path 3 →1→2→5\\nfrom node 3 to node 5, but there is no path from node 5 to node 3.\\nIn a weighted graph, each edge is assigned a weight . The weights are often inter-\\npreted as edge lengths, and the length of a path is the sum of its edge weights. For\\nexample, the graph in Fig. 7.8is weighted, and the length of the path 1 →3→4→5\\nis 1+7+3=11. This is the shortest path from node 1 to node 5.\\nTwo nodes are neighbors oradjacent if there is an edge between them. The degree\\nof a node is the number of its neighbors. Figure 7.9 shows the degree of each node\\nFig. 7.1 Ag r a p hw i t h5\\nnodes and 7 edges1 2\\n3 45\\nFig. 7.2 A path from node 1\\nto node 51 2\\n3 45', metadata={'source': 'documents\\\\doc.pdf', 'page': 83}),\n",
       " Document(page_content='7.1 Basics of Graphs 79\\nFig. 7.3 A cycle of three\\nnodes1 2\\n3 45\\nFig. 7.4 The left graph is\\nconnected, the right graph is\\nnot1 2\\n3 41 2\\n3 4\\nFig. 7.5 A graph with three\\ncomponents1 2\\n3 6 74 5\\n8\\nFig. 7.6 At r e e1 2\\n3 45\\nof a graph. For example, the degree of node 2 is 3, because its neighbors are 1, 4,\\nand 5.\\nThe sum of degrees in a graph is always 2 m, where mis the number of edges,\\nbecause each edge increases the degree of exactly two nodes by one. For this reason,\\nthe sum of degrees is always even. A graph is regular if the degree of every node is\\na constant d. A graph is complete if the degree of every node is n−1, i.e., the graph\\ncontains all possible edges between the nodes.\\nIn a directed graph, the indegree of a node is the number of edges that end at\\nthe node, and the outdegree of a node is the number of edges that start at the node.\\nFig. 7.7 A directed graph1 2\\n3 45\\nFig. 7.8 A weighted graph\\n1 2\\n3 455\\n1\\n767\\n3', metadata={'source': 'documents\\\\doc.pdf', 'page': 84}),\n",
       " Document(page_content='80 7 Graph Algorithms\\nFig. 7.9 Degrees of nodes\\n1 2\\n3 45\\n2 333\\n1\\nFig. 7.10 Indegrees and\\noutdegrees1 2\\n3 45\\n1/13 /00/32 /1\\n0/1\\nFig. 7.11 A bipartite graph\\nand its coloring2 3\\n5 6 41\\n2 3\\n5 6 41\\nFigure 7.10 shows the indegree and outdegree of each node of a graph. For example,\\nnode 2 has indegree 2 and outdegree 1.\\nA graph is bipartite if it is possible to color its nodes using two colors in such a\\nway that no adjacent nodes have the same color. It turns out that a graph is bipartite\\nexactly when it does not have a cycle with an odd number of edges. For example,\\nFig. 7.11 shows a bipartite graph and its coloring.\\n7.1.2 Graph Representation\\nThere are several ways to represent graphs in algorithms. The choice of a data struc-\\nture depends on the size of the graph and the way the algorithm processes it. Next\\nwe will go through three popular representations.\\nAdjacency Lists In the adjacency list representation, each node xof the graph is\\nassigned an adjacency list that consists of nodes to which there is an edge from x.\\nAdjacency lists are the most popular way to represent graphs, and most algorithms\\ncan be efﬁciently implemented using them.\\nA convenient way to store the adjacency lists is to declare an array of vectors as\\nfollows:', metadata={'source': 'documents\\\\doc.pdf', 'page': 85}),\n",
       " Document(page_content='7.1 Basics of Graphs 81\\nFig. 7.12 Example graphs\\n1 2 34(a)\\n1 2 34\\n5 76 5 2(b)\\nvector< int> adj[N];\\nThe constant Nis chosen so that all adjacency lists can be stored. For example,\\nthe graph in Fig. 7.12 a can be stored as follows:\\nadj[1].push_back(2);\\nadj[2].push_back(3);\\nadj[2].push_back(4);\\nadj[3].push_back(4);\\nadj[4].push_back(1);\\nIf the graph is undirected, it can be stored in a similar way, but each edge is added\\nin both directions.\\nFor a weighted graph, the structure can be extended as follows:\\nvector<pair< int,int>> adj[N];\\nIn this case, the adjacency list of node acontains the pair (b,w)always when\\nthere is an edge from node ato node bwith weight w. For example, the graph in Fig.\\n7.12 b can be stored as follows:\\nadj[1].push_back({2,5});\\nadj[2].push_back({3,7});\\nadj[2].push_back({4,6});\\nadj[3].push_back({4,5});\\nadj[4].push_back({1,2});\\nUsing adjacency lists, we can efﬁciently ﬁnd the nodes to which we can move\\nfrom a given node through an edge. For example, the following loop goes through\\nall nodes to which we can move from node s:\\nfor (auto u : adj[s]) {\\n// process node u\\n}\\nAdjacency Matrix Anadjacency matrix indicates the edges that a graph contains.\\nWe can efﬁciently check from an adjacency matrix if there is an edge between two\\nnodes. The matrix can be stored as an array', metadata={'source': 'documents\\\\doc.pdf', 'page': 86}),\n",
       " Document(page_content='82 7 Graph Algorithms\\nint adj[N][N];\\nwhere each value adj[a][b]indicates whether the graph contains an edge from node\\nato node b. If the edge is included in the graph, then adj[a][b]= 1, and otherwise\\nadj[a][b]= 0. For example, the adjacency matrix for the graph in Fig. 7.12 ai s\\n⎡\\n⎢⎢⎣0100\\n0011\\n0001\\n1000⎤\\n⎥⎥⎦.\\nIf the graph is weighted, the adjacency matrix representation can be extended\\nso that the matrix contains the weight of the edge if the edge exists. Using this\\nrepresentation, the graph in Fig. 7.12 b corresponds to the following matrix:\\n⎡\\n⎢⎢⎣0500\\n0076\\n0005\\n2000⎤\\n⎥⎥⎦\\nThe drawback of the adjacency matrix representation is that an adjacency matrix\\ncontains n2elements, and usually most of them are zero. For this reason, the repre-\\nsentation cannot be used if the graph is large.\\nEdge List Anedge list contains all edges of a graph in some order. This is a convenient\\nway to represent a graph if the algorithm processes all its edges, and it is not needed\\nto ﬁnd edges that start at a given node.\\nThe edge list can be stored in a vector\\nvector<pair< int,int>> edges;\\nwhere each pair (a,b)denotes that there is an edge from node ato node b. Thus, the\\ngraph in Fig. 7.12 a can be represented as follows:\\nedges.push_back({1,2});\\nedges.push_back({2,3});\\nedges.push_back({2,4});\\nedges.push_back({3,4});\\nedges.push_back({4,1});\\nIf the graph is weighted, the structure can be extended as follows:\\nvector<tuple< int,int,int>> edges;', metadata={'source': 'documents\\\\doc.pdf', 'page': 87}),\n",
       " Document(page_content='7.1 Basics of Graphs 83\\nEach element in this list is of the form (a,b,w), which means that there is an edge\\nfrom node ato node bwith weight w. For example, the graph in Fig. 7.12 b can be\\nrepresented as follows1:\\nedges.push_back({1,2,5});\\nedges.push_back({2,3,7});\\nedges.push_back({2,4,6});\\nedges.push_back({3,4,5});\\nedges.push_back({4,1,2});\\n7.2 Graph Traversal\\nThis section discusses two fundamental graph algorithms: depth-ﬁrst search and\\nbreadth-ﬁrst search. Both algorithms are given a starting node in the graph, and they\\nvisit all nodes that can be reached from the starting node. The difference in the\\nalgorithms is the order in which they visit the nodes.\\n7.2.1 Depth-First Search\\nDepth-ﬁrst search (DFS) is a straightforward graph traversal technique. The algo-\\nrithm begins at a starting node and proceeds to all other nodes that are reachable\\nfrom the starting node using the edges of the graph.\\nDepth-ﬁrst search always follows a single path in the graph as long as it ﬁnds new\\nnodes. After this, it returns to previous nodes and begins to explore other parts of\\nthe graph. The algorithm keeps track of visited nodes, so that it processes each node\\nonly once.\\nFigure 7.13 shows how depth-ﬁrst search processes a graph. The search can begin\\nat any node of the graph; in this example we begin the search at node 1. First the\\nsearch explores the path 1 →2→3→5, then returns back to node 1 and visits\\nthe remaining node 4.\\nImplementation Depth-ﬁrst search can be conveniently implemented using recur-\\nsion. The following function dfs begins a depth-ﬁrst search at a given node. The\\nfunction assumes that the graph is stored as adjacency lists in an array\\nvector< int> adj[N];\\nand also maintains an array\\n1In some older compilers, the function make_tuple must be used instead of the braces (e.g.,\\nmake_tuple(1,2,5) instead of {1,2,5} ).', metadata={'source': 'documents\\\\doc.pdf', 'page': 88}),\n",
       " Document(page_content='84 7 Graph Algorithms\\nFig. 7.13 Depth-ﬁrst search1 2\\n3\\n4 5\\nstep 11 2\\n3\\n4 5\\nstep 2\\n1 2\\n3\\n4 5\\nstep 31 2\\n3\\n4 5\\nstep 4\\n1 2\\n3\\n4 5\\nstep 51 2\\n3\\n4 5\\nstep 6\\n1 2\\n3\\n4 5\\nstep 71 2\\n3\\n4 5\\nstep 8\\nbool visited[N];\\nthat keeps track of the visited nodes. Initially, each array value is false , and when\\nthe search arrives at node s, the value of visited [s] becomes true . The function\\ncan be implemented as follows:\\nvoid dfs( int s) {\\nif(visited[s]) return ;\\nvisited[s] = true ;\\n// process node s\\nfor (auto u: adj[s]) {\\ndfs(u);\\n}\\n}\\nThe time complexity of depth-ﬁrst search is O(n+m)where nis the number of\\nnodes and mis the number of edges, because the algorithm processes each node and\\nedge once.', metadata={'source': 'documents\\\\doc.pdf', 'page': 89}),\n",
       " Document(page_content='7.2 Graph Traversal 85\\nFig. 7.14 Breadth-ﬁrst\\nsearch1 2 3\\n4 5 6\\nstep 11 2 3\\n4 5 6\\nstep 2\\n1 2 3\\n4 5 6\\nstep 31 2 3\\n4 5 6\\nstep 4\\n7.2.2 Breadth-First Search\\nBreadth-ﬁrst search (BFS) visits the nodes of a graph in increasing order of their\\ndistance from the starting node. Thus, we can calculate the distance from the starting\\nnode to all other nodes using breadth-ﬁrst search. However, breadth-ﬁrst search is\\nmore difﬁcult to implement than depth-ﬁrst search.\\nBreadth-ﬁrst search goes through the nodes one level after another. First the search\\nexplores the nodes whose distance from the starting node is 1, then the nodes whose\\ndistance is 2, and so on. This process continues until all nodes have been visited.\\nFigure 7.14 shows how breadth-ﬁrst search processes a graph. Suppose that the\\nsearch begins at node 1. First the search visits nodes 2 and 4 with distance 1, then\\nnodes 3 and 5 with distance 2, and ﬁnally node 6 with distance 3.\\nImplementation Breadth-ﬁrst search is more difﬁcult to implement than depth-ﬁrst\\nsearch, because the algorithm visits nodes in different parts of the graph. A typical\\nimplementation is based on a queue that contains nodes. At each step, the next node\\nin the queue will be processed.\\nThe following code assumes that the graph is stored as adjacency lists and main-\\ntains the following data structures:\\nqueue< int>q ;\\nbool visited[N];\\nint distance[N];\\nThe queue qcontains nodes to be processed in increasing order of their distance.\\nNew nodes are always added to the end of the queue, and the node at the beginning\\nof the queue is the next node to be processed. The array visited indicates which\\nnodes the search has already visited, and the array distance will contain the\\ndistances from the starting node to all nodes of the graph.\\nThe search can be implemented as follows, starting at node x:', metadata={'source': 'documents\\\\doc.pdf', 'page': 90}),\n",
       " Document(page_content='86 7 Graph Algorithms\\nFig. 7.15 Checking the\\nconnectivity of a graph2 1\\n3\\n5 4\\nvisited[x] = true ;\\ndistance[x] = 0;\\nq.push(x);\\nwhile (!q.empty()) {\\nint s = q.front(); q.pop();\\n// process node s\\nfor (auto u : adj[s]) {\\nif(visited[u]) continue ;\\nvisited[u] = true ;\\ndistance[u] = distance[s]+1;\\nq.push(u);\\n}\\n}\\nLike in depth-ﬁrst search, the time complexity of breadth-ﬁrst search is O(n+m),\\nwhere nis the number of nodes and mis the number of edges.\\n7.2.3 Applications\\nUsing the graph traversal algorithms, we can check many properties of graphs. Usu-\\nally, both depth-ﬁrst search and breadth-ﬁrst search may be used, but in practice,\\ndepth-ﬁrst search is a better choice, because it is easier to implement. In the appli-\\ncations described below we will assume that the graph is undirected.\\nConnectivity Check A graph is connected if there is a path between any two nodes\\nof the graph. Thus, we can check if a graph is connected by starting at an arbitrary\\nnode and ﬁnding out if we can reach all other nodes.\\nFor example, in Fig. 7.15 , since a depth-ﬁrst search from node 1 does not visit all\\nthe nodes, we can conclude that the graph is not connected. In a similar way, we can\\nalso ﬁnd all connected components of a graph by iterating through the nodes and\\nalways starting a new depth-ﬁrst search if the current node does not belong to any\\ncomponent yet.\\nCycle Detection A graph contains a cycle if during a graph traversal, we ﬁnd a\\nnode whose neighbor (other than the previous node in the current path) has already\\nbeen visited. For example, in Fig. 7.16 , a depth-ﬁrst search from node 1 reveals that\\nthe graph contains a cycle. After moving from node 2 to node 5 we notice that the\\nneighbor 3 of node 5 has already been visited. Thus, the graph contains a cycle that\\ngoes through node 3, for example, 3 →2→5→3.', metadata={'source': 'documents\\\\doc.pdf', 'page': 91}),\n",
       " Document(page_content='7.2 Graph Traversal 87\\nFig. 7.16 Finding a cycle in\\nag r a p h2 1\\n3\\n5 4\\nFig. 7.17 A conﬂict when\\nchecking bipartiteness2 1\\n3\\n5 4\\nAnother way to determine if a graph contains a cycle is to simply calculate the\\nnumber of nodes and edges in every component. If a component contains cnodes\\nand no cycle, it must contain exactly c−1 edges (so it has to be a tree). If there are\\ncor more edges, the component surely contains a cycle.\\nBipartiteness Check A graph is bipartite if its nodes can be colored using two colors\\nso that there are no adjacent nodes with the same color. It is surprisingly easy to check\\nif a graph is bipartite using graph traversal algorithms.\\nThe idea is to pick two colors XandY, color the starting node X, all its neighbors\\nY, all their neighbors X, and so on. If at some point of the search we notice that\\ntwo adjacent nodes have the same color, this means that the graph is not bipartite.\\nOtherwise the graph is bipartite and one coloring has been found.\\nFor example, in Fig. 7.17 , a depth-ﬁrst search from node 1 shows that the graph is\\nnot bipartite, because we notice that both nodes 2 and 5 should have the same color,\\nwhile they are adjacent nodes in the graph.\\nThis algorithm always works, because when there are only two colors available,\\nthe color of the starting node in a component determines the colors of all other nodes\\nin the component. It does not make any difference what the colors are.\\nNote that in the general case it is difﬁcult to ﬁnd out if the nodes in a graph can be\\ncolored using kcolors so that no adjacent nodes have the same color. The problem\\nis NP-hard already for k=3.\\n7.3 Shortest Paths\\nFinding a shortest path between two nodes of a graph is an important problem that\\nhas many practical applications. For example, a natural problem related to a road\\nnetwork is to calculate the shortest possible length of a route between two cities,\\ngiven the lengths of the roads.\\nIn an unweighted graph, the length of a path equals the number of its edges, and\\nwe can simply use breadth-ﬁrst search to ﬁnd a shortest path. However, in this section', metadata={'source': 'documents\\\\doc.pdf', 'page': 92}),\n",
       " Document(page_content='88 7 Graph Algorithms\\nFig. 7.18 The\\nBellman–Ford algorithm1 2\\n3 450 ∞\\n∞∞∞2\\n3\\n−235\\n27\\nstep 11 2\\n3 450 2\\n3 7∞2\\n3\\n−235\\n27\\nstep 2\\n1 2\\n3 450 2\\n3 172\\n3\\n−235\\n27\\nstep 31 2\\n3 450 2\\n3 132\\n3\\n−235\\n27\\nstep 4\\nwe focus on weighted graphs where more sophisticated algorithms are needed for\\nﬁnding shortest paths.\\n7.3.1 Bellman–Ford Algorithm\\nThe Bellman–Ford algorithm ﬁnds shortest paths from a starting node to all nodes\\nof the graph. The algorithm can process all kinds of graphs, provided that the graph\\ndoes not contain a cycle with negative length. If the graph contains a negative cycle,\\nthe algorithm can detect this.\\nThe algorithm keeps track of distances from the starting node to all nodes of the\\ngraph. Initially, the distance to the starting node is 0 and the distance to any other node\\nis inﬁnite. The algorithm then reduces the distances by ﬁnding edges that shorten the\\npaths until it is not possible to reduce any distance.\\nFigure 7.18 shows how the Bellman–Ford algorithm processes a graph. First, the\\nalgorithm reduces distances using the edges 1 →2, 1→3 and 1 →4, then using\\nthe edges 2 →5 and 3 →4, and ﬁnally using the edge 4 →5. After this, no edge\\ncan be used to reduce distances, which means that the distances are ﬁnal.\\nImplementation The implementation of the Bellman–Ford algorithm below deter-\\nmines the shortest distances from a node xto all nodes of the graph. The code\\nassumes that the graph is stored as an edge list edges that consists of tuples of the\\nform(a,b,w), meaning that there is an edge from node ato node bwith weight w.\\nThe algorithm consists of n−1 rounds, and on each round the algorithm goes\\nthrough all edges of the graph and attempts to reduce the distances. The algorithm\\nconstructs an array distance that will contain the distances from node xto all\\nnodes. The constant INF denotes an inﬁnite distance.', metadata={'source': 'documents\\\\doc.pdf', 'page': 93}),\n",
       " Document(page_content='7.3 Shortest Paths 89\\nFig. 7.19 Ag r a p hw i t ha\\nnegative cycle\\n12\\n343 1\\n5 −72\\nfor (int i=1 ;i< =n ;i + + ){\\ndistance[i] = INF;\\n}\\ndistance[x] = 0;\\nfor (int i = 1; i <= n-1; i++) {\\nfor (auto e : edges) {\\nint a, b, w;\\nt i e ( a ,b ,w )=e ;\\ndistance[b] = min(distance[b], distance[a]+w);\\n}\\n}\\nThe time complexity of the algorithm is O(nm), because the algorithm consists of\\nn−1 rounds and iterates through all medges during a round. If there are no negative\\ncycles in the graph, all distances are ﬁnal after n−1 rounds, because each shortest\\npath can contain at most n−1 edges.\\nThere are several ways to optimize the algorithm in practice. First, the ﬁnal dis-\\ntances can usually be found earlier than after n−1 rounds, so we can simply stop the\\nalgorithm if no distance can be reduced during a round. A more advanced variant is\\ntheSPFA algorithm (“Shortest Path Faster Algorithm” [ 8]) which maintains a queue\\nof nodes that might be used for reducing the distances. Only the nodes in the queue\\nwill be processed, which often yields a more efﬁcient search.\\nNegative Cycles The Bellman–Ford algorithm can also be used to check if the graph\\ncontains a cycle with negative length. In this case, any path that contains the cycle can\\nbe shortened inﬁnitely many times, so the concept of a shortest path is not meaningful.\\nFor example, the graph in Fig. 7.19 contains a negative cycle 2 →3→4→2 with\\nlength −4.\\nA negative cycle can be detected using the Bellman–Ford algorithm by running\\nthe algorithm for nrounds. If the last round reduces any distance, the graph contains\\na negative cycle. Note that this algorithm can be used to search for a negative cycle\\nin the entire graph regardless of the starting node.\\n7.3.2 Dijkstra’s Algorithm\\nDijkstra’s algorithm ﬁnds shortest paths from the starting node to all nodes of the\\ngraph, like the Bellman–Ford algorithm. The beneﬁt of Dijkstra’s algorithm is that it', metadata={'source': 'documents\\\\doc.pdf', 'page': 94}),\n",
       " Document(page_content='90 7 Graph Algorithms\\nFig. 7.20 Dijkstra’s\\nalgorithm3 4\\n2 15∞∞\\n∞ 0∞6\\n2\\n592\\n1\\nstep 13 4\\n2 15∞ 9\\n5 016\\n2\\n592\\n1\\nstep 2\\n3 4\\n2 15∞ 3\\n5 016\\n2\\n592\\n1\\nstep 33 4\\n2 159 3\\n5 016\\n2\\n592\\n1\\nstep 4\\n3 4\\n2 157 3\\n5 016\\n2\\n592\\n1\\nstep 53 4\\n2 157 3\\n5 016\\n2\\n592\\n1\\nstep 6\\nis more efﬁcient and can be used for processing large graphs. However, the algorithm\\nrequires that there are no negative weight edges in the graph.\\nLike the Bellman–Ford algorithm, Dijkstra’s algorithm maintains distances to the\\nnodes and reduces them during the search. At each step, Dijkstra’s algorithm selects a\\nnode that has not been processed yet and whose distance is as small as possible. Then,\\nthe algorithm goes through all edges that start at the node and reduces the distances\\nusing them. Dijkstra’s algorithm is efﬁcient, because it only processes each edge in\\nthe graph once, using the fact that there are no negative edges.\\nFigure 7.20 shows how Dijkstra’s algorithm processes a graph. Like in the\\nBellman–Ford algorithm, the initial distance to all nodes, except for the starting\\nnode, is inﬁnite. The algorithm processes the nodes in the order 1, 5, 4, 2, 3, and at\\neach node reduces distances using edges that start at the node. Note that the distance\\nto a node never changes after processing the node.\\nImplementation An efﬁcient implementation of Dijkstra’s algorithm requires that\\nwe can efﬁciently ﬁnd the minimum-distance node that has not been processed. An\\nappropriate data structure for this is a priority queue that contains the remaining nodes\\nordered by their distances. Using a priority queue, the next node to be processed can\\nbe retrieved in logarithmic time.\\nA typical textbook implementation of Dijkstra’s algorithm uses a priority queue\\nthat has an operation for modifying a value in the queue. This allows us to have\\na single instance of each node in the queue and update its distance when needed.\\nHowever, standard library priority queues do not provide such an operation, and\\na somewhat different implementation is usually used in competitive programming.', metadata={'source': 'documents\\\\doc.pdf', 'page': 95}),\n",
       " Document(page_content='7.3 Shortest Paths 91\\nThe idea is to add a new instance of a node to the priority queue always when its\\ndistance changes.\\nOur implementation of Dijkstra’s algorithm calculates the minimum distances\\nfrom a node xto all other nodes of the graph. The graph is stored as adjacency lists\\nso that adj[ a]contains a pair (b,w)always when there is an edge from node ato\\nnode bwith weight w. The priority queue\\npriority_queue<pair< int,int>> q;\\ncontains pairs of the form (−d,x), meaning that the current distance to node xisd.\\nThe array distance contains the distance to each node, and the array processed\\nindicates whether a node has been processed.\\nNote that the priority queue contains negative distances to nodes. The reason for\\nthis is that the default version of the C++ priority queue ﬁnds maximum elements,\\nwhile we want to ﬁnd minimum elements. By exploiting negative distances, we can\\ndirectly use the default priority queue.2Also note that while there may be several\\ninstances of a node in the priority queue, only the instance with the minimum distance\\nwill be processed.\\nThe implementation is as follows:\\nfor (int i=1 ;i< =n ;i + + ){\\ndistance[i] = INF;\\n}\\ndistance[x] = 0;\\nq.push({0,x});\\nwhile (!q.empty()) {\\nint a = q.top().second; q.pop();\\nif(processed[a]) continue ;\\nprocessed[a] = true ;\\nfor (auto u : adj[a]) {\\nint b = u.first, w = u.second;\\nif(distance[a]+w < distance[b]) {\\ndistance[b] = distance[a]+w;\\nq.push({-distance[b],b});\\n}\\n}\\n}\\nThe time complexity of the above implementation is O(n+mlogm), because\\nthe algorithm goes through all nodes of the graph and adds for each edge at most\\none distance to the priority queue.\\nNegative Edges The efﬁciency of Dijkstra’s algorithm is based on the fact that the\\ngraph does not have negative edges. However, if the graph has a negative edge, the\\n2Of course, we could also declare the priority queue as in Sect. 5.2.3 and use positive distances, but\\nthe implementation would be longer.', metadata={'source': 'documents\\\\doc.pdf', 'page': 96}),\n",
       " Document(page_content='92 7 Graph Algorithms\\nFig. 7.21 Ag r a p hw h e r e\\nDijkstra’s algorithm fails\\n12\\n342 3\\n6 −5\\nFig. 7.22 An input for the\\nFloyd–Warshall algorithm3 4\\n2 157\\n2\\n592\\n1\\nalgorithm may give incorrect results. As an example, consider the graph in Fig. 7.21 .\\nThe shortest path from node 1 to node 4 is 1 →3→4 and its length is 1. However,\\nDijkstra’s algorithm incorrectly ﬁnds the path 1 →2→4 by greedily following\\nminimum weight edges.\\n7.3.3 Floyd–Warshall Algorithm\\nThe Floyd–Warshall algorithm provides an alternative way to approach the problem\\nof ﬁnding shortest paths. Unlike the other algorithms in this chapter, it ﬁnds shortest\\npaths between all node pairs of the graph in a single run.\\nThe algorithm maintains a matrix that contains distances between the nodes. The\\ninitial matrix is directly constructed based on the adjacency matrix of the graph.\\nThen, the algorithm consists of consecutive rounds, and on each round, it selects a\\nnew node that can act as an intermediate node in paths from now on, and reduces\\ndistances using this node.\\nLet us simulate the Floyd–Warshall algorithm for the graph in Fig. 7.22 . In this\\ncase, the initial matrix is as follows:\\n⎡\\n⎢⎢⎢⎢⎣05 ∞ 91\\n502 ∞∞\\n∞ 207 ∞\\n9∞ 702\\n1∞∞ 20⎤\\n⎥⎥⎥⎥⎦\\nOn the ﬁrst round, node 1 is the new intermediate node. There is a new path\\nbetween nodes 2 and 4 with length 14, because node 1 connects them. There is also\\na new path between nodes 2 and 5 with length 6.\\n⎡\\n⎢⎢⎢⎢⎣05 ∞ 91\\n502 14 6\\n∞ 207 ∞\\n9 14 702\\n1 6∞ 20⎤\\n⎥⎥⎥⎥⎦', metadata={'source': 'documents\\\\doc.pdf', 'page': 97}),\n",
       " Document(page_content='7.3 Shortest Paths 93\\nFig. 7.23 A shortest path\\nfrom node 2 to node 43 4\\n2 157\\n2\\n592\\n1\\nOn the second round, node 2 is the new intermediate node. This creates new paths\\nbetween nodes 1 and 3 and between nodes 3 and 5:\\n⎡\\n⎢⎢⎢⎢⎣05 7 91\\n502 1 4 6\\n7 207 8\\n91 47 0 2\\n16 8 20⎤\\n⎥⎥⎥⎥⎦\\nThe algorithm continues like this, until all nodes have been appointed intermediate\\nnodes. After the algorithm has ﬁnished, the matrix contains the minimum distances\\nbetween any two nodes:\\n⎡\\n⎢⎢⎢⎢⎣05731\\n50286\\n72078\\n38702\\n16820⎤\\n⎥⎥⎥⎥⎦\\nFor example, the matrix tells us that the shortest distance between nodes 2 and 4\\nis 8. This corresponds to the path in Fig. 7.23 .\\nImplementation The Floyd–Warshall algorithm is particularly easy to implement.\\nThe implementation below constructs a distance matrix where dist[a][b]denotes\\nthe shortest distance between nodes aand b. First, the algorithm initializes dist\\nusing the adjacency matrix adj of the graph:\\nfor (int i=1 ;i< =n ;i + + ){\\nfor (int j=1 ;j< =n ;j + + ){\\nif(i == j) dist[i][j] = 0;\\nelse if (adj[i][j]) dist[i][j] = adj[i][j];\\nelse dist[i][j] = INF;\\n}\\n}\\nAfter this, the shortest distances can be found as follows:', metadata={'source': 'documents\\\\doc.pdf', 'page': 98}),\n",
       " Document(page_content='94 7 Graph Algorithms\\nFig. 7.24 Ag r a p ha n da\\ntopological sort1 2 3\\n4 5 6\\n1 2 3 4 5 6\\nfor (int k=1 ;k< =n ;k + + ){\\nfor (int i=1 ;i< =n ;i + + ){\\nfor (int j = 1; j <= n; j++) {\\ndist[i][j] = min(dist[i][j],dist[i][k]+dist[k][j]);\\n}\\n}\\n}\\nThe time complexity of the algorithm is O(n3), because it contains three nested\\nloops that go through the nodes of the graph.\\nSince the implementation of the Floyd–Warshall algorithm is simple, the algorithm\\ncan be a good choice even if it is only needed to ﬁnd a single shortest path in the\\ngraph. However, the algorithm can only be used when the graph is so small that a\\ncubic time complexity is fast enough.\\n7.4 Directed Acyclic Graphs\\nAn important class of graphs are directed acyclic graphs , also called DAGs . Such\\ngraphs do not contain cycles, and many problems are easier to solve if we may assume\\nthat this is the case. In particular, we can always construct a topological sort for the\\ngraph and then apply dynamic programming.\\n7.4.1 Topological Sorting\\nAtopological sort is an ordering of the nodes of a directed graph such that if there\\nis a path from node ato node b, then node aappears before node bin the ordering.\\nFor example, in Fig. 7.24 , one possible topological sort is [4,1,5,2,3,6].\\nA directed graph has a topological sort exactly when it is acyclic. If the graph\\ncontains a cycle, it is not possible to form a topological sort, because no node of the\\ncycle can appear before the other nodes of the cycle in the ordering. It turns out that\\ndepth-ﬁrst search can be used to both check if a directed graph contains a cycle and,\\nif it does not, to construct a topological sort.', metadata={'source': 'documents\\\\doc.pdf', 'page': 99}),\n",
       " Document(page_content='7.4 Directed Acyclic Graphs 95\\nFig. 7.25 The ﬁrst search\\nadds nodes 6, 3, 2, and 1 to\\nthe list1 2 3\\n4 5 6\\nFig. 7.26 The second search\\nadds nodes 5 and 4 to the list1 2 3\\n4 5 6\\nFig. 7.27 The ﬁnal\\ntopological sort1 2 3 4 5 6\\nThe idea is to go through the nodes of the graph and always begin a depth-ﬁrst\\nsearch at the current node if it has not been processed yet. During the searches, the\\nnodes have three possible states:\\n•state 0: the node has not been processed (white)\\n•state 1: the node is under processing (light gray)\\n•state 2: the node has been processed (dark gray)\\nInitially, the state of each node is 0. When a search reaches a node for the ﬁrst\\ntime, its state becomes 1. Finally, after all edges from the node have been processed,\\nits state becomes 2.\\nIf the graph contains a cycle, we will discover this during the search, because\\nsooner or later we will arrive at a node whose state is 1. In this case, it is not possible\\nto construct a topological sort. If the graph does not contain a cycle, we can construct\\na topological sort by adding each node to a list when its state becomes 2. Finally, we\\nreverse the list and get a topological sort for the graph.\\nNow we are ready to construct a topological sort for our example graph. The ﬁrst\\nsearch (Fig. 7.25 ) proceeds from node 1 to node 6, and adds nodes 6, 3, 2, and 1 to\\nthe list. Then, the second search (Fig. 7.26 ) proceeds from node 4 to node 5 and adds\\nnodes 5 and 4 to the list. The ﬁnal reversed list is [4,5,1,2,3,6], which corresponds\\nto a topological sort (Fig. 7.27 ). Note that a topological sort is not unique; there can\\nbe several topological sorts for a graph.\\nFigure 7.28 shows a graph that does not have a topological sort. During the search,\\nwe reach node 2 whose state is 1, which means that the graph contains a cycle. Indeed,\\nthere is a cycle 2 →3→5→2.', metadata={'source': 'documents\\\\doc.pdf', 'page': 100}),\n",
       " Document(page_content='96 7 Graph Algorithms\\nFig. 7.28 This graph does\\nnot have a topological sort,\\nbecause it contains a cycle1 2 3\\n4 5 6\\n7.4.2 Dynamic Programming\\nUsing dynamic programming, we can efﬁciently answer many questions regarding\\npaths in directed acyclic graphs. Examples of such questions are:\\n•What is the shortest/longest path from node ato node b?\\n•How many different paths are there?\\n•What is the minimum/maximum number of edges in a path?\\n•Which nodes appear in every possible path?\\nNote that many of the above problems are difﬁcult to solve or not well-deﬁned\\nfor general graphs.\\nAs an example, consider the problem of calculating the number of paths from\\nnode ato node b. Letpaths (x)denote the number of paths from node ato node\\nx. As a base case, paths (a)=1. Then, to calculate other values of paths (x),w e\\ncan use the recursive formula\\npaths (x)=paths (s1)+paths (s2)+···+ paths (sk),\\nwhere s1,s2,..., skare the nodes from which there is an edge to x. Since the graph\\nis acyclic, the values of paths can be calculated in the order of a topological sort.\\nFigure 7.29 shows the values of paths in an example scenario where we want\\nto calculate the number of paths from node 1 to node 6. For example,\\npaths (6)=paths (2)+paths (3),\\nbecause the edges that end at node 6 are 2 →6 and 3 →6. Since paths (2)=2\\nandpaths (3)=2, we conclude that paths (6)=4. The paths are as follows:\\n•1→2→3→6\\n•1→2→6\\n•1→4→5→2→3→6\\n•1→4→5→2→6\\nProcessing Shortest Paths Dynamic programming can also be used to answer ques-\\ntions regarding shortest paths in general (not necessarily acyclic) graphs. Namely,\\nif we know minimum distances from a starting node to other nodes (e.g., after using\\nDijkstra’s algorithm), we can easily create a directed acyclic shortest paths graph', metadata={'source': 'documents\\\\doc.pdf', 'page': 101}),\n",
       " Document(page_content='7.4 Directed Acyclic Graphs 97\\nFig. 7.29 Calculating the\\nnumber of paths from node 1\\nto node 61 2 3\\n4 5 6\\n114122\\nFig. 7.30 Ag r a p ha n di t s\\nshortest paths graph 1 2\\n3 453\\n5 48\\n212\\n1 2\\n3 453\\n5 4\\n212\\nFig. 7.31 Coin problem as a\\ndirected acyclic graph\\n0 1 2 3 4 5 6\\nthat indicates for each node the possible ways to reach the node using a shortest path\\nfrom the starting node. For example, Fig. 7.30 shows a graph and the corresponding\\nshortest paths graph.\\nCoin Problem Revisited In fact, any dynamic programming problem can be rep-\\nresented as a directed acyclic graph where each node corresponds to a dynamic\\nprogramming state and the edges indicate how the states depend on each other.\\nFor example, consider the problem of forming a sum of money nusing coins\\n{c1,c2,..., ck}(Sect. 6.1.1 ). In this scenario, we can construct a graph where each\\nnode corresponds to a sum of money, and the edges show how the coins can be\\nchosen. For example, Fig. 7.31 shows the graph for the coins {1,3,4}and n=6.\\nUsing this representation, the shortest path from node 0 to node ncorresponds to\\na solution with the minimum number of coins, and the total number of paths from\\nnode 0 to node nequals the total number of solutions.\\n7.5 Successor Graphs\\nAnother special class of directed graphs are successor graphs . In those graphs, the\\noutdegree of each node is 1, i.e., each node has a unique successor . A successor', metadata={'source': 'documents\\\\doc.pdf', 'page': 102}),\n",
       " Document(page_content='98 7 Graph Algorithms\\nFig. 7.32 A successor graph\\n1 2 3\\n45\\n6 7\\n89\\nFig. 7.33 Walking in a\\nsuccessor graph4 6 2 5 2 5 2\\ngraph consists of one or more components, each of which contains one cycle and\\nsome paths that lead to it.\\nSuccessor graphs are sometimes called functional graphs , because any successor\\ngraph corresponds to a function succ(x)that deﬁnes the edges of the graph. The\\nparameter xis a node of the graph, and the function gives the successor of the node.\\nFor example, the function\\nx 123456789\\nsucc(x)357622163\\ndeﬁnes the graph in Fig. 7.32 .\\n7.5.1 Finding Successors\\nSince each node of a successor graph has a unique successor, we can also deﬁne a\\nfunction succ(x,k)that gives the node that we will reach if we begin at node xand\\nwalk ksteps forward. For example, in our example graph succ(4,6)=2, because\\nwe will reach node 2 by walking 6 steps from node 4 (Fig. 7.33 ).\\nA straightforward way to calculate a value of succ(x,k)is to start at node xand\\nwalk ksteps forward, which takes O(k)time. However, using preprocessing, any\\nvalue of succ(x,k)can be calculated in only O(logk)time.\\nLetudenote the maximum number of steps we will ever walk. The idea is to\\nprecalculate all values of succ(x,k)where kis a power of two and at most u. This\\ncan be efﬁciently done, because we can use the following recurrence:\\nsucc(x,k)={\\nsucc(x) k=1\\nsucc(succ(x,k/2),k/2)k>1\\nThe idea is that a path of length kthat begins at node xcan be divided into two\\npaths of length k/2. Precalculating all values of succ(x,k)where kis a power of\\ntwo and at most utakes O(nlogu)time, because O(logu)values are calculated for\\neach node. In our example graph, the ﬁrst values are as follows:', metadata={'source': 'documents\\\\doc.pdf', 'page': 103}),\n",
       " Document(page_content='7.5 Successor Graphs 99\\nx 123456789\\nsucc(x,1)357622163\\nsucc(x,2)721255327\\nsucc(x,4)327255123\\nsucc(x,8)721255327\\n···\\nFig. 7.34 A cycle in a\\nsuccessor graph\\n5 46\\n3 2 1\\nAfter the precalculation, any value of succ(x,k)can be calculated by presenting\\nkas a sum of powers of two. Such a representation always consists of O(logk)parts,\\nso calculating a value of succ(x,k)takes O(logk)time. For example, if we want\\nto calculate the value of succ(x,11), we use the formula\\nsucc(x,11)=succ(succ(succ(x,8),2),1).\\nIn our example graph,\\nsucc(4,11)=succ(succ(succ(4,8),2),1)=5.\\n7.5.2 Cycle Detection\\nConsider a successor graph that only contains a path that ends in a cycle. We may\\nask the following questions: if we begin our walk at the starting node, what is the\\nﬁrst node in the cycle and how many nodes does the cycle contain? For example,\\nin Fig. 7.34 , we begin our walk at node 1, the ﬁrst node that belongs to the cycle is\\nnode 4, and the cycle consists of three nodes (4, 5, and 6).\\nA simple way to detect the cycle is to walk in the graph and keep track of all nodes\\nthat have been visited. Once a node is visited for the second time, we can conclude\\nthat the node is the ﬁrst node in the cycle. This method works in O(n)time and also\\nuses O(n)memory. However, there are better algorithms for cycle detection. The\\ntime complexity of such algorithms is still O(n), but they only use O(1)memory,\\nwhich may be an important improvement if nis large.\\nOne such algorithm is Floyd’s algorithm , which walks in the graph using two\\npointers aandb. Both pointers begin at the starting node x. Then, on each turn, the\\npointer awalks one step forward and the pointer bwalks two steps forward. The\\nprocess continues until the pointers meet each other:', metadata={'source': 'documents\\\\doc.pdf', 'page': 104}),\n",
       " Document(page_content='100 7 Graph Algorithms\\na = succ(x);\\nb = succ(succ(x));\\nwhile (a != b) {\\na = succ(a);\\nb = succ(succ(b));\\n}\\nAt this point, the pointer ahas walked ksteps and the pointer bhas walked 2 k\\nsteps, so the length of the cycle divides k. Thus, the ﬁrst node that belongs to the\\ncycle can be found by moving the pointer ato node xand advancing the pointers\\nstep by step until they meet again.\\na=x ;\\nwhile (a != b) {\\na = succ(a);\\nb = succ(b);\\n}\\nfirst = a;\\nAfter this, the length of the cycle can be calculated as follows:\\nb = succ(a);\\nlength = 1;\\nwhile (a != b) {\\nb = succ(b);\\nlength++;\\n}\\n7.6 Minimum Spanning Trees\\nAspanning tree contains all nodes of a graph and some of its edges so that there is a\\npath between any two nodes. Like trees in general, spanning trees are connected and\\nacyclic. The weight of a spanning tree is the sum of its edge weights. For example,\\nFig. 7.35 shows a graph and one of its spanning tree. The weight of this spanning\\ntree is 3 +5+9+3+2=22.\\nAminimum spanning tree is a spanning tree whose weight is as small as possible.\\nFigure 7.36 shows a minimum spanning tree for our example graph with weight 20.\\nIn a similar way, a maximum spanning tree is a spanning tree whose weight is as large\\nas possible. Figure 7.37 shows a maximum spanning tree for our example graph with\\nweight 32. Note that a graph may have several minimum and maximum spanning\\ntrees, so the trees are not unique.', metadata={'source': 'documents\\\\doc.pdf', 'page': 105}),\n",
       " Document(page_content='7.6 Minimum Spanning Trees 101\\nFig. 7.35 Ag r a p ha n da\\nspanning tree\\n12 3\\n4\\n5 635\\n9\\n5\\n276 3\\n12 3\\n4\\n5 635\\n9\\n23\\nFig. 7.36 A minimum\\nspanning tree with weight 20\\n12 3\\n4\\n5 63\\n5\\n273\\nFig. 7.37 Am a x i m u m\\nspanning tree with weight 32\\n12 3\\n4\\n5 65\\n9\\n5 76\\nIt turns out that several greedy methods can be used to construct minimum and\\nmaximum spanning trees. This section discusses two algorithms that process the\\nedges of the graph ordered by their weights. We focus on ﬁnding minimum spanning\\ntrees, but the same algorithms can also ﬁnd maximum spanning trees by processing\\nthe edges in reverse order.\\n7.6.1 Kruskal’s Algorithm\\nKruskal’s algorithm builds a minimum spanning tree by greedily adding edges to\\nthe graph. The initial spanning tree only contains the nodes of the graph and does\\nnot contain any edges. Then the algorithm goes through the edges ordered by their\\nweights and always adds an edge to the graph if it does not create a cycle.\\nThe algorithm maintains the components of the graph. Initially, each node of the\\ngraph belongs to a separate component. Always when an edge is added to the graph,\\ntwo components are joined. Finally, all nodes belong to the same component, and a\\nminimum spanning tree has been found.\\nAs an example, let us construct a minimum spanning tree for our example graph\\n(Fig. 7.35 ). The ﬁrst step is to sort the edges in increasing order of their weights:', metadata={'source': 'documents\\\\doc.pdf', 'page': 106}),\n",
       " Document(page_content='102 7 Graph Algorithms\\nedge weight\\n5–6 2\\n1–2 3\\n3–6 3\\n1–5 5\\n2–3 5\\n2–5 6\\n4–6 7\\n3–4 9\\nFig. 7.38 Kruskal’s\\nalgorithm\\n12 3\\n4\\n5 6\\nstep 112 3\\n4\\n5 62\\nstep 2\\n12 3\\n4\\n5 63\\n2\\nstep 312 3\\n4\\n5 63\\n23\\nstep 4\\n12 3\\n4\\n5 63\\n5\\n23\\nstep 512 3\\n4\\n5 63\\n5\\n273\\nstep 6\\nThen, we go through the list and add each edge to the graph if it joins two separate\\ncomponents. Figure 7.38 shows the steps of the algorithm. Initially, each node belongs\\nto its own component. Then, the ﬁrst edges on the list (5–6, 1–2, 3–6, and 1–5) are\\nadded to the graph. After this, the next edge would be 2–3, but this edge is not added,\\nbecause it would create a cycle. The same applies to edge 2–5. Finally, the edge 4–6\\nis added, and the minimum spanning tree is ready.\\nWhy Does This Work? It is a good question why Kruskal’s algorithm works. Why\\ndoes the greedy strategy guarantee that we will ﬁnd a minimum spanning tree?\\nLet us see what happens if the minimum weight edge of the graph is not included\\nin the spanning tree. For example, suppose that a minimum spanning tree of our\\nexample graph would not contain the minimum weight edge 5–6. We do not know\\nthe exact structure of such a spanning tree, but in any case it has to contain some\\nedges. Assume that the tree would look like the tree in Fig. 7.39 .\\nHowever, it is not possible that the tree in Fig. 7.39 would be a minimum spanning\\ntree, because we can remove an edge from the tree and replace it with the minimum', metadata={'source': 'documents\\\\doc.pdf', 'page': 107}),\n",
       " Document(page_content='7.6 Minimum Spanning Trees 103\\nFig. 7.39 A hypothetical\\nminimum spanning tree\\n12 3\\n4\\n5 6\\nFig. 7.40 Including the edge\\n5–6 reduces the weight of the\\nspanning tree 12 3\\n4\\n5 62\\nweight edge 5–6. This produces a spanning tree whose weight is smaller , shown in\\nFig. 7.40 .\\nFor this reason, it is always optimal to include the minimum weight edge in the\\ntree to produce a minimum spanning tree. Using a similar argument, we can show\\nthat it is also optimal to add the next edge in weight order to the tree, and so on.\\nHence, Kruskal’s algorithm always produces a minimum spanning tree.\\nImplementation When implementing Kruskal’s algorithm, it is convenient to use\\nthe edge list representation of the graph. The ﬁrst phase of the algorithm sorts the\\nedges in the list in O(mlogm)time. After this, the second phase of the algorithm\\nbuilds the minimum spanning tree as follows:\\nfor (...) {\\nif(!same(a,b)) unite(a,b);\\n}\\nThe loop goes through the edges in the list and always processes an edge (a,b)\\nwhere aandbare two nodes. Two functions are needed: the function same deter-\\nmines if aand bare in the same component, and the function unite joins the\\ncomponents that contain aandb.\\nThe problem is how to efﬁciently implement the functions same andunite .\\nOne possibility is to implement the function same as a graph traversal and check if\\nwe can get from node ato node b. However, the time complexity of such a function\\nwould be O(n+m)and the resulting algorithm would be slow, because the function\\nsame will be called for each edge in the graph.\\nWe will solve the problem using a union-ﬁnd structure that implements both\\nfunctions in O(logn)time. Thus, the time complexity of Kruskal’s algorithm will\\nbeO(mlogn)after sorting the edge list.\\n7.6.2 Union-Find Structure\\nAunion-ﬁnd structure maintains a collection of sets. The sets are disjoint, so no\\nelement belongs to more than one set. Two O(logn)time operations are supported:', metadata={'source': 'documents\\\\doc.pdf', 'page': 108}),\n",
       " Document(page_content='104 7 Graph Algorithms\\nFig. 7.41 A union-ﬁnd\\nstructure with three sets\\n12\\n34 5\\n67\\n8\\nFig. 7.42 Joining two sets\\ninto a single set\\n12\\n34\\n67\\n8\\ntheunite operation joins two sets, and the find operation ﬁnds the representative\\nof the set that contains a given element.\\nIn a union-ﬁnd structure, one element in each set is the representative of the\\nset, and there is a path from any other element of the set to the representative. For\\nexample, assume that the sets are {1,4,7},{5}and{2,3,6,8}. Figure 7.41 shows\\none way to represent these sets.\\nIn this case the representatives of the sets are 4, 5, and 2. We can ﬁnd the represen-\\ntative of any element by following the path that begins at the element. For example,\\nthe element 2 is the representative for the element 6, because we follow the path\\n6→3→2. Two elements belong to the same set exactly when their representatives\\nare the same.\\nTo join two sets, the representative of one set is connected to the representative of\\nthe other set. For example, Fig. 7.42 shows a possible way to join the sets {1,4,7}\\nand{2,3,6,8}. From this on, the element 2 is the representative for the entire set\\nand the old representative 4 points to the element 2.\\nThe efﬁciency of the union-ﬁnd structure depends on how the sets are joined. It\\nturns out that we can follow a simple strategy: always connect the representative of\\nthesmaller set to the representative of the larger set (or if the sets are of equal size,\\nwe can make an arbitrary choice). Using this strategy, the length of any path will be\\nO(logn), so we can ﬁnd the representative of any element efﬁciently by following\\nthe corresponding path.\\nImplementation The union-ﬁnd structure can be conveniently implemented using\\narrays. In the following implementation, the array link indicates for each element\\nthe next element in the path, or the element itself if it is a representative, and the\\narray size indicates for each representative the size of the corresponding set.\\nInitially, each element belongs to a separate set:\\nfor (int i=1 ;i< =n ;i + + )l i n k [ i ]=i ;\\nfor (int i=1 ;i< =n ;i + + )s i z e [ i ]=1 ;\\nThe function find returns the representative for an element x. The representative\\ncan be found by following the path that begins at x.', metadata={'source': 'documents\\\\doc.pdf', 'page': 109}),\n",
       " Document(page_content='7.6 Minimum Spanning Trees 105\\nint find( int x) {\\nwhile (x != link[x]) x = link[x];\\nreturn x;\\n}\\nThe function same checks whether elements aandbbelong to the same set. This\\ncan easily be done by using the function find :\\nbool same( int a,int b) {\\nreturn find(a) == find(b);\\n}\\nThe function unite joins the sets that contain elements aand b(the elements\\nhave to be in different sets). The function ﬁrst ﬁnds the representatives of the sets\\nand then connects the smaller set to the larger set.\\nvoid unite( int a,int b) {\\na = find(a);\\nb = find(b);\\nif(size[a] < size[b]) swap(a,b);\\nsize[a] += size[b];\\nlink[b] = a;\\n}\\nThe time complexity of the function find isO(logn)assuming that the length\\nof each path is O(logn). In this case, the functions same andunite also work\\ninO(logn)time. The function unite makes sure that the length of each path is\\nO(logn)by connecting the smaller set to the larger set.\\nPath Compression Here is an alternative way to implement the find operation:\\nint find( int x) {\\nif(x == link[x]) return x;\\nreturn link[x] = find(link[x]);\\n}\\nThis function uses path compression : each element in the path will directly point\\nto its representative after the operation. It can be shown that using this function, the\\nunion-ﬁnd operations work in amortized O(α(n))time, where α(n)is the inverse\\nAckermann function which grows very slowly (it is almost a constant). However,\\npath compression cannot be used in some applications of the union-ﬁnd structure,\\nsuch as in the dynamic connectivity algorithm (Sect. 15.5.4 ).', metadata={'source': 'documents\\\\doc.pdf', 'page': 110}),\n",
       " Document(page_content='106 7 Graph Algorithms\\nFig. 7.43 Prim’s algorithm\\n12 3\\n4\\n5 6\\nstep 112 3\\n4\\n5 63\\nstep 2\\n12 3\\n4\\n5 635\\nstep 312 3\\n4\\n5 635\\n3\\nstep 4\\n12 3\\n4\\n5 635\\n23\\nstep 512 3\\n4\\n5 635\\n273\\nstep 6\\n7.6.3 Prim’s Algorithm\\nPrim’s algorithm is an alternative method for constructing minimum spanning trees.\\nThe algorithm ﬁrst adds an arbitrary node to the tree, and then always chooses a\\nminimum weight edge that adds a new node to the tree. Finally, all nodes have been\\nadded and a minimum spanning tree has been found.\\nPrim’s algorithm resembles Dijkstra’s algorithm. The difference is that Dijkstra’s\\nalgorithm always selects a node whose distance from the starting node is minimum,\\nbut Prim’s algorithm simply selects a node that can be added to the tree using a\\nminimum weight edge.\\nAs an example, Fig. 7.43 shows how Prim’s algorithm constructs a minimum\\nspanning tree for our example graph, assuming that the starting node is node 1.\\nLike Dijkstra’s algorithm, Prim’s algorithm can be efﬁciently implemented using\\na priority queue. The priority queue should contain all nodes that can be connected\\nto the current component using a single edge, in increasing order of the weights of\\nthe corresponding edges.\\nThe time complexity of Prim’s algorithm is O(n+mlogm)that equals the time\\ncomplexity of Dijkstra’s algorithm. In practice, Prim’s and Kruskal’s algorithms\\nare both efﬁcient, and the choice of the algorithm is a matter of taste. Still, most\\ncompetitive programmers use Kruskal’s algorithm.', metadata={'source': 'documents\\\\doc.pdf', 'page': 111}),\n",
       " Document(page_content='Algorithm Design T opics\\nThis chapter discusses a selection of algorithm design topics.\\nSection 8.1focuses on bit-parallel algorithms that use bit operations to efﬁciently\\nprocess data. Typically, we can replace a for loop with bit operations, which may\\nremarkably improve the running time of the algorithm.\\nSection 8.2 presents the amortized analysis technique, which can be used to\\nestimate the time needed for a sequence of operations in an algorithm. Using the\\ntechnique, we can analyze algorithms for determining nearest smaller elements and\\nsliding window minima.\\nSection 8.3discusses ternary search and other techniques for efﬁciently calculating\\nminimum values of certain functions.\\n8.1 Bit-Parallel Algorithms\\nBit-parallel algorithms are based on the fact that individual bits of numbers can\\nbe manipulated in parallel using bit operations. Thus, a way to design an efﬁcient\\nalgorithm is to represent the steps of the algorithm so that they can be efﬁciently\\nimplemented using bit operations.\\n8.1.1 Hamming Distances\\nThe Hamming distance hamming (a,b)between two strings aand bof equal length\\nis the number of positions where the strings differ. For example,\\nhamming (01101 ,11001 )=2.', metadata={'source': 'documents\\\\doc.pdf', 'page': 112}),\n",
       " Document(page_content='108 8 Algorithm Design Topics\\nConsider the following problem: Given nbit strings, each of length k, calculate\\nthe minimum Hamming distance between two strings. For example, the answer for\\n[00111 ,01101 ,11110 ]is 2, because\\n•hamming (00111 ,01101 )=2,\\n•hamming (00111 ,11110 )=3, and\\n•hamming (01101 ,11110 )=3.\\nA straightforward way to solve the problem is to go through all pairs of strings\\nand calculate their Hamming distances, which yields an O(n2k)time algorithm. The\\nfollowing function calculates the distance between strings aand b:\\nint hamming(string a, string b) {\\nint d=0 ;\\nfor (int i=0 ;i<k ; i++) {\\nif(a[i] != b[i]) d++;\\n}\\nreturn d;\\n}\\nHowever, since the strings consist of bits, we can optimize the solution by storing\\nthe strings as integers and calculating distances using bit operations. In particular, if\\nk≤32, we can just store the strings as int values and use the following function\\nto calculate distances:\\nint hamming( int a,int b) {\\nreturn __builtin_popcount(a^b);\\n}\\nIn the above function, the xor operation constructs a string that has one bits in\\npositions where aand bdiffer. Then, the number of one bits is calculated using the\\n__builtin_popcount function.\\nTable 8.1 shows a comparison of running times of the original algorithm and\\nthe bit-parallel algorithm on a modern computer. In this problem, the bit-parallel\\nalgorithm is about 20 times faster than the original algorithm.\\n8.1.2 Counting Subgrids\\nAs another example, consider the following problem: Given an n×ngrid whose\\neach square is either black (1) or white (0), calculate the number of subgrids whose\\nall corners are black. For example, Fig. 8.1shows two such subgrids in a grid.', metadata={'source': 'documents\\\\doc.pdf', 'page': 113}),\n",
       " Document(page_content='8.1 Bit-Parallel Algorithms 109\\nTable 8.1 The running times of the algorithms when calculating minimum Hamming distances of\\nnbit strings of length k=30\\nSize n Original algorithm (s) Bit-parallel algorithm (s)\\n5000 0.84 0.06\\n10000 3.24 0.18\\n15000 7.23 0.37\\n20000 12.79 0.63\\n25000 19.99 0.97\\nFig. 8.1 This grid contains\\ntwo subgrids with black\\ncorners\\nThere is an O(n3)time algorithm for solving the problem: go through all O(n2)\\npairs of rows, and for each pair (a,b)calculate, in O(n)time, the number of columns\\nthat contain a black square in both rows aand b. The following code assumes that\\ncolor [y][x]denotes the color in row yand column x:\\nint count = 0;\\nfor (int i=0 ;i<n ;i + + ){\\nif(color[a][i] == 1 && color[b][i] == 1) {\\ncount++;\\n}\\n}\\nThen, after ﬁnding out that there are count columns where both squares are\\nblack, we can use the formula count (count −1)/2 to calculate the number of\\nsubgrids whose ﬁrst row is aand last row is b.\\nTo create a bit-parallel algorithm, we represent each row kas an n-bit bitset row[k]\\nwhere one bits denote black squares. Then, we can calculate the number of columns\\nwhere rows aand bboth have black squares using an and operation and counting\\nthe number of one bits. This can be conveniently done as follows using bitset\\nstructures:\\nint count = (row[a]&row[b]).count();\\nTable 8.2 shows a comparison of the original algorithm and the bit-parallel algo-\\nrithm for different grid sizes. The comparison shows that the bit-parallel algorithm\\ncan be up to 30 times faster than the original algorithm.', metadata={'source': 'documents\\\\doc.pdf', 'page': 114}),\n",
       " Document(page_content='110 8 Algorithm Design Topics\\nTable 8.2 The running times of the algorithms for counting the subgrids\\nGrid size n Original algorithm (s) Bit-parallel algorithm (s)\\n1000 0.65 0.05\\n1500 2.17 0.14\\n2000 5.51 0.30\\n2500 12.67 0.52\\n3000 26.36 0.87\\nFig. 8.2 Ag r a p ha n di t s\\nreach values. For example,\\nreach (2)=3, because\\nnodes 2, 4, and 5 can be\\nreached from node 21 2\\n3 45reach (1)= 5\\nreach (2)= 3\\nreach (3)= 3\\nreach (4)= 2\\nreach (5)= 1\\n8.1.3 Reachability in Graphs\\nGiven a directed acyclic graph of nnodes, consider the problem of calculating for\\neach node xa value reach (x): the number of nodes that can be reached from node\\nx. For example, Fig. 8.2shows a graph and its reach values.\\nThe problem can be solved using dynamic programming in O(n2)time by con-\\nstructing for each node a list of nodes that can be reached from it. Then, to create\\na bit-parallel algorithm, we represent each list as a bitset of nbits. This permits us\\nto efﬁciently calculate the union of two such lists using an oroperation. Assuming\\nthatreach is an array of bitset structures and the graph is stored as adjacency\\nlists in adj , the calculation for node xcan be done as follows:\\nreach[x][x] = 1;\\nfor (auto u : adj[x]) {\\nreach[x] |= reach[u];\\n}\\nTable 8.3 shows some running times for the bit-parallel algorithm. In each test,\\nthe graph has nnodes and 2 nrandom edges a→ bwhere a<b. Note that the\\nTable 8.3 The running times of the algorithms when counting reachable nodes in a graph\\nGraph size n Running time (s) Memory usage (MB)\\n2·1040.06 50\\n4·1040.17 200\\n6·1040.32 450\\n8·1040.51 800\\n1050.78 1250', metadata={'source': 'documents\\\\doc.pdf', 'page': 115}),\n",
       " Document(page_content='8.1 Bit-Parallel Algorithms 111\\nalgorithm uses a great amount of memory for large values of n. In many contests,\\nthe memory limit may be 512 MB or lower.\\n8.2 Amortized Analysis\\nThe structure of an algorithm often directly tells us its time complexity, but sometimes\\na straightforward analysis does not give a true picture of the efﬁciency. Amortized\\nanalysis can be used to analyze a sequence of operations whose time complexity\\nvaries. The idea is to estimate the total time used to all such operations during the\\nalgorithm, instead of focusing on individual operations.\\n8.2.1 Two Pointers Method\\nIn the two pointers method , two pointers walk through an array. Both pointers move\\nto one direction only, which ensures that the algorithm works efﬁciently. As a ﬁrst\\nexample of how to apply the technique, consider a problem where we are given an\\narray of npositive integers and a target sum x, and we want to ﬁnd a subarray whose\\nsum is xor report that there is no such subarray.\\nThe problem can be solved in O(n)time by using the two pointers method. The\\nidea is to maintain pointers that point to the ﬁrst and last value of a subarray. On each\\nturn, the left pointer moves one step to the right, and the right pointer moves to the\\nright as long as the resulting subarray sum is at most x. If the sum becomes exactly\\nx, a solution has been found.\\nFor example, Fig. 8.3shows how the algorithm processes an array when the target\\nsum is x=8. The initial subarray contains the values 1, 3, and 2, whose sum is\\n6. Then, the left pointer moves one step right, and the right pointer does not move,\\nbecause otherwise the sum would exceed x. Finally, the left pointer moves one\\nstep right, and the right pointer moves two steps right. The sum of the subarray is\\n2+5+1=8, so the desired subarray has been found.\\nThe running time of the algorithm depends on the number of steps the right pointer\\nmoves. While there is no useful upper bound on how many steps the pointer can move\\nFig. 8.3 Finding a subarray\\nwith sum 8 using the two\\npointers method1 3 2 5 112 3\\n1 3 2 5 112 3\\n1 3 2 5 112 3', metadata={'source': 'documents\\\\doc.pdf', 'page': 116}),\n",
       " Document(page_content='112 8 Algorithm Design Topics\\nFig. 8.4 Solving the 2SUM\\nproblem using the two\\npointers method14 56 7 99 10\\n14 56 7 99 10\\n14 56 7 99 10\\non a single turn, we know that the pointer moves a total of O (n)steps during the\\nalgorithm, because it only moves to the right. Since both the left and right pointer\\nmove O(n)steps, the algorithm works in O(n)time.\\n2SUM Problem Another problem that can be solved using the two pointers\\nmethod is the 2SUM problem : given an array of nnumbers and a target sum x,\\nﬁnd two array values such that their sum is x, or report that no such values exist.\\nTo solve the problem, we ﬁrst sort the array values in increasing order. After that,\\nwe iterate through the array using two pointers. The left pointer starts at the ﬁrst\\nvalue and moves one step to the right on each turn. The right pointer starts at the last\\nvalue and always moves to the left until the sum of the left and right value is at most\\nx. If the sum is exactly x, a solution has been found.\\nFor example, Fig. 8.4shows how the algorithm processes an array when the target\\nsum is x=12. In the initial position, the sum of the values is 1 +10=11 which is\\nsmaller than x. Then the left pointer moves one step right, and the right pointer moves\\nthree steps left, and the sum becomes 4 +7=11. After this, the left pointer moves\\none step right again. The right pointer does not move, and a solution 5 +7=12 has\\nbeen found.\\nThe running time of the algorithm is O(nlog n), because it ﬁrst sorts the array in\\nO(nlog n)time, and then both pointers move O(n)steps.\\nNote that it is also possible to solve the problem in another way in O(nlog n)time\\nusing binary search. In such a solution, we ﬁrst sort the array and then iterate through\\nthe array values and for each value binary search for another value that yields the sum\\nx. In fact, many problems that can be solved using the two pointers method can also\\nbe solved using sorting or set structures, sometimes with an additional logarithmic\\nfactor.\\nThe more general kSUM problem is also interesting. In this problem we have to\\nﬁnd kelements such that their sum is x. It turns out that we can solve the 3SUM\\nproblem in O(n2)time by extending the above 2SUM algorithm. Can you see how\\nwe can do it? For a long time, it was actually thought that O(n2)would be the best\\npossible time complexity for the 3SUM problem. However, in 2014, Grønlund and\\nPettie [ 12] showed that this is not the case.', metadata={'source': 'documents\\\\doc.pdf', 'page': 117}),\n",
       " Document(page_content='8.2 Amortized Analysis 113\\n8.2.2 Nearest Smaller Elements\\nAmortized analysis is often used to estimate the number of operations performed on\\na data structure. The operations may be distributed unevenly so that most operations\\noccur during a certain phase of the algorithm, but the total number of the operations\\nis limited.\\nAs an example, suppose that we want to ﬁnd for each array element the nearest\\nsmaller element , i.e., the ﬁrst smaller element that precedes the element in the array.\\nIt is possible that no such element exists, in which case the algorithm should report\\nthis. Next we will efﬁciently solve the problem using a stack structure.\\nWe go through the array from left to right and maintain a stack of array elements.\\nAt each array position, we remove elements from the stack until the top element is\\nsmaller than the current element, or the stack is empty. Then, we report that the top\\nelement is the nearest smaller element of the current element, or if the stack is empty,\\nthere is no such element. Finally, we add the current element to the stack.\\nFigure 8.5 shows how the algorithm processes an array. First, the element 1 is\\nadded to the stack. Since it is the ﬁrst element in the array, it clearly does not have a\\nnearest smaller element. After this, the elements 3 and 4 are added to the stack. The\\nnearest smaller element of 4 is 3, and the nearest smaller element of 3 is 1. Then, the\\nnext element 2 is smaller than the two top elements in the stack, so the elements 3\\nand 4 are removed from the stack. Thus, the nearest smaller element of 2 is 1. After\\nthis, the element 2 is added to the stack. The algorithm continues like this, until the\\nentire array has been processed.\\n1 3 42 5 3 42\\n1\\nstep 11 3 42 5 3 42\\n1 3\\nstep 2\\n1 3 42 5 3 42\\n1 3 4\\nstep 31 3 42 5 3 42\\n12\\nstep 4\\n1 3 42 5 3 42\\n12 5\\nstep 51 3 42 5 3 42\\n12 3\\nstep 6\\n1 3 42 5 3 42\\n12 3 4\\nstep 71 3 42 5 3 42\\n12\\nstep 8\\nFig. 8.5 Finding the nearest smaller elements in linear time using a stack', metadata={'source': 'documents\\\\doc.pdf', 'page': 118}),\n",
       " Document(page_content='114 8 Algorithm Design Topics\\nThe efﬁciency of the algorithm depends on the total number of stack operations.\\nIf the current element is larger than the top element in the stack, it is directly added to\\nthe stack, which is efﬁcient. However, sometimes the stack can contain several larger\\nelements and it takes time to remove them. Still, each element is added exactly once\\nto the stack and removed at most once from the stack. Thus, each element causes\\nO(1)stack operations, and the algorithm works in O(n)time.\\n8.2.3 Sliding Window Minimum\\nAsliding window is a constant-size subarray that moves from left to right through\\nan array. At each window position, we want to calculate some information about the\\nelements inside the window. Next we will focus on the problem of maintaining the\\nsliding window minimum , which means that we want to report the smallest value\\ninside each window.\\nThe sliding window minima can be calculated using a similar idea that we used\\nto calculate the nearest smaller elements. This time we maintain a queue where each\\nelement is larger than the previous element, and the ﬁrst element always corresponds\\nto the minimum element inside the window. After each window move, we remove\\nelements from the end of the queue until the last queue element is smaller than the\\nnew window element, or the queue becomes empty. We also remove the ﬁrst queue\\nelement if it is not inside the window anymore. Finally, we add the new window\\nelement to the queue.\\nFigure 8.6shows how the algorithm processes an array when the sliding window\\nsize is 4. At the ﬁrst window position, the smallest value is 1. Then the window\\nmoves one step right. The new element 3 is smaller than the elements 4 and 5 in\\nthe queue, so the elements 4 and 5 are removed from the queue and the element 3\\nFig. 8.6 Finding sliding\\nwindow minima in linear\\ntime214 5 3 412\\n14 5\\n214 5 3 412\\n1 3\\n214 5 3 412\\n3 4\\n214 5 3 412\\n1\\n214 5 3 412\\n12', metadata={'source': 'documents\\\\doc.pdf', 'page': 119}),\n",
       " Document(page_content='8.2 Amortized Analysis 115\\nis added to the queue. The smallest value is still 1. After this, the window moves\\nagain, and the smallest element 1 does not belong to the window anymore. Thus, it\\nis removed from the queue, and the smallest value is now 3. Also the new element\\n4 is added to the queue. The next new element 1 is smaller than all elements in the\\nqueue, so all elements are removed from the queue, and it only contains the element\\n1. Finally, the window reaches its last position. The element 2 is added to the queue,\\nbut the smallest value inside the window is still 1.\\nSince each array element is added to the queue exactly once and removed from\\nthe queue at most once, the algorithm works in O(n)time.\\n8.3 Finding Minimum Values\\nSuppose that there is a function f(x)that ﬁrst only decreases, then attains its min-\\nimum value, and then only increases. For example, Fig. 8.7 shows such a function\\nwhose minimum value is marked with an arrow. If we know that our function has\\nthis property, we can efﬁciently ﬁnd its minimum value.\\n8.3.1 T ernary Search\\nT ernary search provides an efﬁcient way to ﬁnd the minimum value of a function\\nthat ﬁrst decreases and then increases. Assume that we know that the value of xthat\\nminimizes f(x)is in a range [xL,xR]. The idea is to divide the range into three\\nequal-size parts [xL,a],[a,b], and[b,xR]by choosing\\na=2xL+xR\\n3and b=xL+2xR\\n3.\\nThen, if f(a)< f(b), we conclude that the minimum must be in range [xL,b], and\\notherwise it must be in range [a,xR]. After this, we recursively continue the search,\\nuntil the size of the active range is small enough.\\nAs an example, Fig. 8.8 shows the ﬁrst step of ternary search in our example\\nscenario. Since f(a)> f(b), the new range becomes [a,xR].\\nFig. 8.7 A function and its\\nminimum value', metadata={'source': 'documents\\\\doc.pdf', 'page': 120}),\n",
       " Document(page_content='116 8 Algorithm Design Topics\\nFig. 8.8 Searching for the\\nminimum using ternary\\nsearchxL\\na\\nbxR\\nxL\\na\\nbxR\\nFig. 8.9 Example of a\\nconvex function: f(x)=x2\\nab\\nIn practice, we often consider functions whose parameters are integers, and the\\nsearch is terminated when the range only contains one element. Since the size of\\nthe new range is always 2 /3 of the previous range, the algorithm works in O(log n)\\ntime, where nis the number of elements in the original range.\\nNote that when working with integer parameters, we can also use binary search\\ninstead of ternary search, because it sufﬁces to ﬁnd the ﬁrst position xfor which\\nf(x)≤f(x+1).\\n8.3.2 Convex Functions\\nA function is convex if a line segment between any two points on the graph of the\\nfunction always lies above or on the graph. For example, Fig. 8.9shows the graph of\\nf(x)=x2, which is a convex function. Indeed, the line segment between points a\\nand blies above the graph.\\nIf we know that the minimum value of a convex function is in range [xL,xR],\\nwe can use ternary search to ﬁnd it. However, note that several points of a convex\\nfunction may have the minimum value. For example, f(x)=0 is convex and its\\nminimum value is 0.\\nConvex functions have some useful properties: if f(x)and g(x)are convex func-\\ntions, then also f(x)+g(x)and max (f(x),g(x))are convex functions. For example,', metadata={'source': 'documents\\\\doc.pdf', 'page': 121}),\n",
       " Document(page_content='8.3 Finding Minimum Values 117\\nif we have nconvex functions f1,f2,..., fn, we immediately know that also the\\nfunction f1+f2+...+fnhas to be convex and we can use ternary search to ﬁnd\\nits minimum value.\\n8.3.3 Minimizing Sums\\nGiven nnumbers a1,a2,..., an, consider the problem of ﬁnding a value of xthat\\nminimizes the sum\\n|a1−x|+| a2−x|+···+| an−x|.\\nFor example, if the numbers are [1,2,9,2,6], the optimal solution is to choose\\nx=2, which produces the sum\\n|1−2|+| 2−2|+| 9−2|+| 2−2|+| 6−2|= 12.\\nSince each function |ak−x|is convex, the sum is also convex, so we could\\nuse ternary search to ﬁnd the optimal value of x. However, there is also an easier\\nsolution. It turns out that the optimal choice for xis always the median of the\\nnumbers, i.e., the middle element after sorting. For example, the list [1,2,9,2,6]\\nbecomes [1,2,2,6,9]after sorting, so the median is 2.\\nThe median is always optimal, because if xis smaller than the median, the sum\\nbecomes smaller by increasing x, and if xis larger then the median, the sum becomes\\nsmaller by decreasing x.I f nis even and there are two medians, both medians and\\nall values between them are optimal choices.\\nThen, consider the problem of minimizing the function\\n(a1−x)2+(a2−x)2+···+ (an−x)2.\\nFor example, if the numbers are [1,2,9,2,6], the best solution is to choose x=4,\\nwhich produces the sum\\n(1−4)2+(2−4)2+(9−4)2+(2−4)2+(6−4)2=46.\\nAgain, this function is convex and we could use ternary search to solve the prob-\\nlem, but there is also a simple solution: the optimal choice for xis the average of\\nthe numbers. In the example the average is (1+2+9+2+6)/5=4. This can be\\nproved by presenting the sum as follows:\\nnx2−2x(a1+a2+···+ an)+(a2\\n1+a2\\n2+···+ a2\\nn)\\nThe last part does not depend on x, so we can ignore it. The remaining parts form\\na function nx2−2xswhere s=a1+a2+···+ an. This is a parabola opening\\nupwards with roots x=0 and x=2s/n, and the minimum value is the average of\\nthe roots x=s/n, i.e., the average of the numbers a1,a2,..., an.', metadata={'source': 'documents\\\\doc.pdf', 'page': 122}),\n",
       " Document(page_content='Range Queries\\nIn this chapter, we discuss data structures for efﬁciently processing range queries on\\narrays. Typical queries are range sum queries (calculating the sum of values) and\\nrange minimum queries (ﬁnding the minimum value).\\nSection 9.1focuses on a simple situation where the array values are not modiﬁed\\nbetween the queries. In this case it sufﬁces to preprocess the array so that we can\\nefﬁciently determine the answer for any possible query. We will ﬁrst learn to process\\nsum queries using a preﬁx sum array, and then we will discuss the sparse table\\nalgorithm for processing minimum queries.\\nSection 9.2presents two tree structures that allow us to both process queries and\\nupdate array values efﬁciently. A binary indexed tree supports sum queries and can\\nbe seen as a dynamic version of a preﬁx sum array. A segment tree is a more versatile\\nstructure that supports sum queries, minimum queries, and several other queries. The\\noperations of both the structures work in logarithmic time.\\n9.1 Queries on Static Arrays\\nIn this section, we focus on a situation where the array is static , i.e., the array values\\nare never updated between the queries. In this case, it sufﬁces to preprocess the array\\nso that we can efﬁciently answer range queries.\\nFirst we will discuss a simple way to process sum queries using a preﬁx sum\\narray, which can also be generalized to higher dimensions. After this, we will learn\\nthe sparse table algorithm for processing minimum queries, which is somewhat more\\ndifﬁcult. Note that while we focus on processing minimum queries, we can always\\nalso process maximum queries using similar methods.', metadata={'source': 'documents\\\\doc.pdf', 'page': 123}),\n",
       " Document(page_content='120 9 Range Queries\\n9.1.1 Sum Queries\\nLetsum q(a,b)(“range sum query”) denote the sum of array values in a range [a,b].\\nWe can efﬁciently process any sum query by ﬁrst constructing a preﬁx sum array .\\nEach value in the preﬁx sum array equals the sum of values in the original array up to\\nthe corresponding position, i.e., the value at position kissum q(0,k). For example,\\nFig.9.1shows an array and its preﬁx sum array.\\nThe preﬁx sum array can be constructed in O(n)time. Then, since the preﬁx sum\\narray contains all values of sum q(0,k), we can calculate any value of sum q(a,b)in\\nO(1)time using the formula\\nsum q(a,b)=sum q(0,b)−sum q(0,a−1).\\nBy deﬁning sum q(0,−1)=0, the above formula also holds when a=0.\\nAs an example, Fig. 9.2shows how to calculate the sum of values in the range\\n[3,6]using the preﬁx sum array. We can see in the original array that sum q(3,6)=\\n8+6+1+4=19. Using the preﬁx sum array, we need to examine only two values:\\nsum q(3,6)=sum q(0,6)−sum q(0,2)=27−8=19.\\nHigher Dimensions It is also possible to generalize this idea to higher dimensions.\\nFor example, Fig. 9.3shows a two-dimensional preﬁx sum array that can be used to\\ncalculate the sum of any rectangular subarray in O(1)time. Each sum in this array\\nFig. 9.1 An array and its\\npreﬁx sum array13486142012 3456 7\\noriginal array\\n14 81622232729012 3456 7\\npreﬁx s um array\\nFig. 9.2 Calculating a range\\nsum using the preﬁx sum\\narray13486142012 3456 7\\noriginal array\\n14 81622232729012 3456 7\\npreﬁx s um array\\nFig. 9.3 Calculating a\\ntwo-dimensional range sum\\nA BC D', metadata={'source': 'documents\\\\doc.pdf', 'page': 124}),\n",
       " Document(page_content='9.1 Queries on Static Arrays 121\\nFig. 9.4 Preprocessing for\\nminimum queries13486142012 3456 7\\noriginal array\\n1346112 –012 3456 7\\nrange size 2\\n13111 –––012 3456 7\\nrange size 4\\n1–––––––012 3456 7\\nrange size 8\\ncorresponds to a subarray that begins at the upper-left corner of the array. The sum\\nof the gray subarray can be calculated using the formula\\nS(A)−S(B)−S(C)+S(D),\\nwhere S(X)denotes the sum of values in a rectangular subarray from the upper-left\\ncorner to the position of X.\\n9.1.2 Minimum Queries\\nLet min q(a,b)(“range minimum query”) denote the minimum array value in a range\\n[a,b]. We will next discuss a technique using which we can process any minimum\\nquery in O(1)time after an O(nlogn)time preprocessing. The method is due to\\nBender and Farach-Colton [ 3] and often called the sparse table algorithm .\\nThe idea is to precalculate all values of min q(a,b)where b−a+1 (the length\\nof the range) is a power of two. For example, Fig. 9.4shows the precalculated values\\nfor an array of eight elements.\\nThe number of precalculated values is O(nlogn), because there are O(logn)\\nrange lengths that are powers of two. The values can be calculated efﬁciently using\\nthe recursive formula\\nmin q(a,b)=min (min q(a,a+w−1),min q(a+w,b)),\\nwhere b−a+1 is a power of two and w=(b−a+1)/2. Calculating all those\\nvalues takes O(nlogn)time.\\nAfter this, any value of min q(a,b)can be calculated in O(1)time as a minimum\\nof two precalculated values. Let kbe the largest power of two that does not exceed\\nb−a+1. We can calculate the value of min q(a,b)using the formula\\nmin q(a,b)=min (min q(a,a+k−1),min q(b−k+1,b)).', metadata={'source': 'documents\\\\doc.pdf', 'page': 125}),\n",
       " Document(page_content='122 9 Range Queries\\nFig. 9.5 Calculating a range\\nminimum using two\\noverlapping ranges13486142012 3456 7\\nrange size 6\\n13486142012 3456 7\\nrange size 4\\n13486142012 3456 7\\nrange size 4\\nIn the above formula, the range [a,b]is represented as the union of the ranges\\n[a,a+k−1]and[b−k+1,b], both of length k.\\nAs an example, consider the range [1,6]in Fig. 9.5. The length of the range is 6,\\nand the largest power of two that does not exceed 6 is 4. Thus the range [1,6]is the\\nunion of the ranges [1,4]and[3,6]. Since min q(1,4)=3 and min q(3,6)=1, we\\nconclude that min q(1,6)=1.\\nNote that there are also sophisticated techniques using which we can process\\nrange minimum queries in O(1)time after an only O(n)time preprocessing (see,\\ne.g., Fischer and Heun [ 10]), but they are beyond the scope of this book.\\n9.2 Tree Structures\\nThis section presents two tree structures, using which we can both process range\\nqueries and update array values in logarithmic time. First, we discuss binary indexed\\ntrees that support sum queries, and after that, we focus on segment trees that also\\nsupport several other queries.\\n9.2.1 Binary Indexed Trees\\nAbinary indexed tree (or a Fenwick tree )[9] can be seen as a dynamic variant of a\\npreﬁx sum array. It provides two O(logn)time operations: processing a range sum\\nquery and updating a value. Even if the name of the structure is a binary indexed tree,\\nthe structure is usually represented as an array. When discussing binary indexed trees,\\nwe assume that all arrays are one-indexed, because it makes the implementation of\\nthe structure easier.\\nLetp(k)denote the largest power of two that divides k. We store a binary indexed\\ntree as an array tree such that\\ntree[k]=sum q(k−p(k)+1,k),\\ni.e., each position kcontains the sum of values in a range of the original array whose\\nlength is p(k)and that ends at position k. For example, since p(6)=2,tree[6]', metadata={'source': 'documents\\\\doc.pdf', 'page': 126}),\n",
       " Document(page_content='9.2 Tree Structures 123\\nFig. 9.6 An array and its\\nbinary indexed tree1348614212 3456 78\\noriginal array\\n144 16 6 742912 3456 78\\nbinary indexed tree\\nFig. 9.7 Ranges in a binary\\nindexed tree144 16 6 742912 3456 78\\nFig. 9.8 Processing a range\\nsum query using a binary\\nindexed tree144 16 6 742912 3456 78\\ncontains the value of sum q(5,6). Figure 9.6shows an array and the corresponding\\nbinary indexed tree. Figure 9.7shows more clearly how each value in the binary\\nindexed tree corresponds to a range in the original array.\\nUsing a binary indexed tree, any value of sum q(1,k)can be calculated in O(logn)\\ntime, because a range [1,k]can always be divided into O(logn)subranges whose\\nsums have been stored in the tree. For example, to calculate the value of sum q(1,7),\\nwe divide the range [1,7]into three subranges [1,4],[5,6], and[7,7](Fig. 9.8).\\nSince the sums of those subranges are available in the tree, we can calculate the sum\\nof the entire range using the formula\\nsum q(1,7)=sum q(1,4)+sum q(5,6)+sum q(7,7)=16+7+4=27.\\nThen, to calculate the value of sum q(a,b)where a>1, we can use the same trick\\nthat we used with preﬁx sum arrays:\\nsum q(a,b)=sum q(1,b)−sum q(1,a−1)\\nWe can calculate both sum q(1,b)andsum q(1,a−1)inO(logn)time, so the total\\ntime complexity is O(logn).\\nAfter updating an array value, several values in the binary indexed tree should\\nbe updated. For example, when the value at position 3 changes, we should update', metadata={'source': 'documents\\\\doc.pdf', 'page': 127}),\n",
       " Document(page_content='124 9 Range Queries\\nFig. 9.9 Updating a value in\\nab i n a r yi n d e x e dt r e e144 16 6 742912 3456 78\\nthe subranges [3,3],[1,4], and[1,8](Fig. 9.9). Since each array element belongs to\\nO(logn)subranges, it sufﬁces to update O(logn)tree values.\\nImplementation The operations of a binary indexed tree can be efﬁciently imple-\\nmented using bit operations. The key fact needed is that we can easily calculate any\\nvalue of p(k)using the bit formula\\np(k)=k&−k,\\nwhich isolates the least signiﬁcant one bit of k.\\nFirst, the following function calculates the value of sum q(1,k):\\nint sum( int k) {\\nint s=0 ;\\nwhile (k >= 1) {\\ns += tree[k];\\nk -= k&-k;\\n}\\nreturn s;\\n}\\nThen, the following function increases the array value at position kbyx(xcan\\nbe positive or negative):\\nvoid add( int k,int x) {\\nwhile (k <= n) {\\ntree[k] += x;\\nk += k&-k;\\n}\\n}\\nThe time complexity of both the functions is O(logn), because the functions\\naccess O(logn)values in the binary indexed tree, and each move to the next position\\ntakes O(1)time.', metadata={'source': 'documents\\\\doc.pdf', 'page': 128}),\n",
       " Document(page_content='9.2 Tree Structures 125\\nFig. 9.10 An array and the\\ncorresponding segment tree\\nfor sum queries58632726012 3456 7\\n5863 272613 9 9 822 1739\\n9.2.2 Segment Trees\\nAsegment tree is a data structure that provides two O(logn)time operations: process-\\ning a range query and updating an array value. Segment trees support sum queries,\\nminimum queries, and many other queries. Segment trees have their origins in geo-\\nmetric algorithms (see, e.g., Bentley and Wood [ 4]), and the elegant bottom-up imple-\\nmentation presented in this section follows the textbook by Sta´ nczyk [ 30].\\nA segment tree is a binary tree whose bottom level nodes correspond to the array\\nelements, and the other nodes contain information needed for processing range\\nqueries. When discussing segment trees, we assume that the size of the array is\\na power of two, and zero-based indexing is used, because it is convenient to build a\\nsegment tree for such an array. If the size of the array is not a power of two, we can\\nalways append extra elements to it.\\nWe will ﬁrst discuss segment trees that support sum queries. As an example,\\nFig.9.10 shows an array and the corresponding segment tree for sum queries. Each\\ninternal tree node corresponds to an array range whose size is a power of two. When\\na segment tree supports sum queries, the value of each internal node is the sum of\\nthe corresponding array values, and it can be calculated as the sum of the values of\\nits left and right child node.\\nIt turns out that any range [a,b]can be divided into O(logn)subranges whose\\nvalues are stored in tree nodes. For example, Fig. 9.11 shows the range [2,7]in the\\noriginal array and in the segment tree. In this case, two tree nodes correspond to\\nthe range, and sum q(2,7)=9+17=26. When the sum is calculated using nodes\\nlocated as high as possible in the tree, at most two nodes on each level of the tree are\\nneeded. Hence, the total number of nodes is O(logn).\\nAfter an array update, we should update all nodes whose value depends on the\\nupdated value. This can be done by traversing the path from the updated array element\\nto the top node and updating the nodes along the path. For example, Fig. 9.12 shows\\nthe nodes that change when the value at position 5 changes. The path from bottom\\nto top always consists of O(logn)nodes, so each update changes O(logn)nodes in\\nthe tree.', metadata={'source': 'documents\\\\doc.pdf', 'page': 129}),\n",
       " Document(page_content='126 9 Range Queries\\nFig. 9.11 Processing a range\\nsum query using a segment\\ntree5863 2726012 3456 7\\n5863 272613 9 9 822 1739\\nFig. 9.12 Updating an array\\nvalue in a segment tree5863 2726012 3456 7\\n5863 272613 9 9 822 1739\\nFig. 9.13 Contents of a\\nsegment tree in an array39221713 99 85863272612 3456 7891011 12 131415\\nImplementation A convenient way to store the contents of a segment tree is to use\\nan array of 2 nelements where nis the size of the original array. The tree nodes are\\nstored from top to bottom: tree[1]is the top node, tree[2]andtree[3]are its\\nchildren, and so on. Finally, the values from tree[n]totree[2n−1]correspond\\nto the bottom level of the tree, which contains the values of the original array. Note\\nthat the element tree[0]is not used.\\nFor example, Fig. 9.13 shows how our example tree is stored. Note that the par-\\nent of tree[k]istree[⌊k/2⌋], its left child is tree[2k], and its right child is\\ntree[2k+1]. In addition, the position of a node (other than the top node) is even\\nif it is a left child and odd if it is a right child.', metadata={'source': 'documents\\\\doc.pdf', 'page': 130}),\n",
       " Document(page_content='9.2 Tree Structures 127\\nThe following function calculates the value of sum q(a,b):\\nint sum( int a,int b) {\\na+ =n ;b+ =n ;\\nint s=0 ;\\nwhile (a <= b) {\\nif(a%2 == 1) s += tree[a++];\\nif(b%2 == 0) s += tree[b--];\\na/ =2 ;b/ =2 ;\\n}\\nreturn s;\\n}\\nThe function maintains a range in the segment tree array. Initially, the range is\\n[a+n,b+n]. At each step, the range is moved one level higher in the tree, and the\\nvalues of the nodes that do not belong to the higher range are added to the sum.\\nThe following function increases the array value at position kbyx:\\nvoid add( int k,int x) {\\nk+ =n ;\\ntree[k] += x;\\nfor ( k/ =2 ;k> =1 ;k/ =2 ){\\ntree[k] = tree[2*k]+tree[2*k+1];\\n}\\n}\\nFirst the value at the bottom level of the tree is updated. After this, the values of\\nall internal tree nodes are updated, until the top node of the tree is reached.\\nBoth the above functions work in O(logn)time, because a segment tree of n\\nelements consists of O(logn)levels and the functions move one level higher in the\\ntree at each step.\\nOther Queries Segment trees can support any range queries where we can divide\\na range into two parts, calculate the answer separately for both parts, and then efﬁ-\\nciently combine the answers. Examples of such queries are minimum and maximum,\\ngreatest common divisor, and bit operations and, or, and xor.\\nFor example, the segment tree in Fig. 9.14 supports minimum queries. In this tree,\\nevery node contains the smallest value in the corresponding array range. The top\\nnode of the tree contains the smallest value in the whole array. The operations can\\nbe implemented like previously, but instead of sums, minima are calculated.\\nThe structure of a segment tree also allows us to use a binary search style method\\nfor locating array elements. For example, if the tree supports minimum queries, we\\ncan ﬁnd the position of an element with the smallest value in O(logn)time. For\\nexample, Fig. 9.15 shows how the element with the smallest value 1 can be found by\\ntraversing a path downwards from the top node.', metadata={'source': 'documents\\\\doc.pdf', 'page': 131}),\n",
       " Document(page_content='128 9 Range Queries\\nFig. 9.14 A segment tree for\\nprocessing range minimum\\nqueries\\n5863 17265 3 1 23 11\\nFig. 9.15 Using binary\\nsearch to ﬁnd the minimum\\nelement\\n5863 17265 3 1 23 11\\nFig. 9.16 Compressing an\\narray using index\\ncompression00 50030 4012 3456 7\\noriginal array\\n534012\\ncompressed array\\n9.2.3 Additional T echniques\\nIndex Compression A limitation in data structures that are built upon arrays is that\\nthe elements are indexed using consecutive integers. Difﬁculties arise when large\\nindices are needed. For example, if we want to use the index 109, the array should\\ncontain 109elements which would require too much memory.\\nHowever, if we know all the indices needed during the algorithm beforehand, we\\ncan bypass this limitation by using index compression . The idea is to replace the\\noriginal indices with consecutive integers 0 ,1,2, and so on. To do this, we deﬁne\\na function cthat compresses the indices. The function gives each original index ia\\ncompressed index c(i)in such a way that if aandbare two indices and a<b, then\\nc(a)<c(b). After compressing the indices, we can conveniently perform queries\\nusing them.\\nFigure 9.16 shows a simple example of index compression. Here only indices 2, 5,\\nand 7 are actually used, and all other array values are zeros. The compressed indices\\narec(2)=0,c(5)=1, and c(7)=2, which allows us to create a compressed array\\nthat only contains three elements.', metadata={'source': 'documents\\\\doc.pdf', 'page': 132}),\n",
       " Document(page_content='9.2 Tree Structures 129\\nFig. 9.17 An array and its\\ndifference array33 111 522012 3456 7\\noriginal array\\n30 −200 4−30012 3456 7\\ndifference array\\nFig. 9.18 Updating an array\\nrange using the difference\\narray36444 522012 3456 7\\noriginal array\\n33 −200 1−30012 3456 7\\ndifference array\\nAfter index compression, we can, for example, build a segment tree for the com-\\npressed array and perform queries. The only modiﬁcation needed is that we have to\\ncompress the indices before queries: a range [a,b]in the original array corresponds\\nto the range [c(a),c(b)]in the compressed array.\\nRange Updates So far, we have implemented data structures that support range\\nqueries and updates of single values. Let us now consider an opposite situation,\\nwhere we should update ranges and retrieve single values. We focus on an operation\\nthat increases all elements in a range [a,b]byx.\\nIt turns out that we can use the data structures presented in this chapter also in this\\nsituation. To do this, we build a difference array whose values indicate the differences\\nbetween consecutive values in the original array. The original array is the preﬁx sum\\narray of the difference array. Figure 9.17 shows an array and its difference array.\\nFor example, the value 2 at position 6 in the original array corresponds to the sum\\n3−2+4−3=2 in the difference array.\\nThe advantage of the difference array is that we can update a range in the origi-\\nnal array by changing just two elements in the difference array. More precisely, to\\nincrease the values in range [a,b]byx, we increase the value at position abyx\\nand decrease the value at position b+1b y x. For example, to increase the original\\narray values between positions 1 and 4 by 3, we increase the difference array value\\nat position 1 by 3 and decrease the value at position 5 by 3 (Fig. 9.18 ).\\nThus, we only update single values and process sum queries in the difference\\narray, so we can use a binary indexed tree or a segment tree. A more difﬁcult task\\nis to create a data structure that supports both range queries and range updates. In\\nSect. 15.2.1 , we will see that also this is possible using a lazy segment tree.', metadata={'source': 'documents\\\\doc.pdf', 'page': 133}),\n",
       " Document(page_content='Tree Algorithms\\nThe special properties of trees allow us to create algorithms that are specialized for\\ntrees and work more efﬁciently than general graph algorithms. This chapter presents\\na selection of such algorithms.\\nSection 10.1 introduces basic concepts and algorithms related to trees. A central\\nproblem is ﬁnding the diameter of a tree, i.e., the maximum distance between two\\nnodes. We will learn two linear time algorithms for solving the problem.\\nSection 10.2 focuses on processing queries on trees. We will learn to use a tree\\ntraversal array to process various queries related to subtrees and paths. After this,\\nwe will discuss methods for determining lowest common ancestors, and an ofﬂine\\nalgorithm which is based on merging data structures.\\nSection 10.3 presents two advanced tree processing techniques: centroid decom-\\nposition and heavy-light decomposition.\\n10.1 Basic T echniques\\nAtree is a connected acyclic graph that consists of nnodes and n−1 edges. Remov-\\ning any edge from a tree divides it into two components, and adding any edge creates\\na cycle. There is always a unique path between any two nodes of a tree. The leaves\\nof a tree are the nodes with only one neighbor.\\nAs an example, consider the tree in Fig. 10.1 . This tree consists of 8 nodes and 7\\nedges, and its leaves are nodes 3, 5, 7, and 8.\\nIn a rooted tree, one of the nodes is appointed the root of the tree, and all other\\nnodes are placed underneath the root. The lower neighbors of a node are called its\\nchildren , and the upper neighbor of a node is called its parent . Each node has exactly\\none parent, except for the root that does not have a parent. The structure of a rooted', metadata={'source': 'documents\\\\doc.pdf', 'page': 134}),\n",
       " Document(page_content='132 10 Tree Algorithms\\nFig. 10.1 At r e et h a t\\nconsists of 8 nodes and 7\\nedges1 4\\n2 3 75\\n6 8\\nFig. 10.2 A rooted tree\\nwhere node 1 is the root\\nnode1\\n4 2 3\\n7 5 6\\n8\\ntree is recursive: each node of the tree acts as the root of a subtree that contains the\\nnode itself and all nodes that are in the subtrees of its children.\\nFor example, Fig. 10.2 shows a rooted tree where node 1 is the root of the tree.\\nThe children of node 2 are nodes 5 and 6, and the parent of node 2 is node 1. The\\nsubtree of node 2 consists of nodes 2, 5, 6, and 8.\\n10.1.1 Tree Traversal\\nGeneral graph traversal algorithms can be used to traverse the nodes of a tree. How-\\never, the traversal of a tree is easier to implement than that of a general graph, because\\nthere are no cycles in the tree, and it is not possible to reach a node from more than\\none direction.\\nA typical way to traverse a tree is to start a depth-ﬁrst search at an arbitrary node.\\nThe following recursive function can be used:\\nvoid dfs( int s,int e) {\\n// process node s\\nfor (auto u : adj[s]) {\\nif(u != e) dfs(u, s);\\n}\\n}\\nThe function is given two parameters: the current node sand the previous node e.\\nThe purpose of the parameter eis to make sure that the search only moves to nodes\\nthat have not been visited yet.', metadata={'source': 'documents\\\\doc.pdf', 'page': 135}),\n",
       " Document(page_content='10.1 Basic Techniques 133\\nThe following function call starts the search at node x:\\ndfs(x, 0);\\nIn the ﬁrst call e=0, because there is no previous node, and it is allowed to\\nproceed to any direction in the tree.\\nDynamic Programming Dynamic programming can be used to calculate some infor-\\nmation during a tree traversal. For example, the following code calculates for each\\nnode sa value count [s]: the number of nodes in its subtree. The subtree contains\\nthe node itself and all nodes in the subtrees of its children, so we can calculate the\\nnumber of nodes recursively as follows:\\nvoid dfs( int s,int e) {\\ncount[s] = 1;\\nfor (auto u : adj[s]) {\\nif(u == e) continue ;\\ndfs(u, s);\\ncount[s] += count[u];\\n}\\n}\\nBinary Tree Traversals In a binary tree, each node has a left and right subtree (which\\nmay be empty), and there are three popular tree traversal orderings:\\n•pre-order : ﬁrst process the root node, then traverse the left subtree, then traverse\\nthe right subtree\\n•in-order : ﬁrst traverse the left subtree, then process the root node, then traverse\\nthe right subtree\\n•post-order : ﬁrst traverse the left subtree, then traverse the right subtree, then\\nprocess the root node\\nFor example, in Fig. 10.3 , the pre-order is [1,2,4,5,6,3,7], the in-order is\\n[4,2,6,5,1,3,7], and the post-order is [4,6,5,2,7,3,1].\\nIf we know the pre-order and in-order of a tree, we can reconstruct its exact\\nstructure. For example, the only possible tree with pre-order [1,2,4,5,6,3,7]and\\nFig. 10.3 Ab i n a r yt r e e1\\n2 3\\n4 5\\n67', metadata={'source': 'documents\\\\doc.pdf', 'page': 136}),\n",
       " Document(page_content='134 10 Tree Algorithms\\nFig. 10.4 A tree whose\\ndiameter is 41 4\\n2 3 75\\n6\\nFig. 10.5 Node 1 is the\\nhighest point on the diameter\\npath1\\n4 2 3\\n7 5 6\\nin-order [4,2,6,5,1,3,7]is shown in Fig. 10.3 . The post-order and in-order also\\nuniquely determine the structure of a tree. However, if we only know the pre-order\\nand post-order, there may be more than one tree that matches the orderings.\\n10.1.2 Calculating Diameters\\nThe diameter of a tree is the maximum length of a path between two nodes. For\\nexample, Fig. 10.4 shows a tree whose diameter is 4 that corresponds to a path of\\nlength 4 between nodes 6 and 7. Note that the tree also has another path of length 4\\nbetween nodes 5 and 7.\\nNext we will discuss two O(n)time algorithms for calculating the diameter of a\\ntree. The ﬁrst algorithm is based on dynamic programming, and the second algorithm\\nuses depth-ﬁrst searches.\\nFirst Algorithm A general way to approach tree problems is to ﬁrst root the tree\\narbitrarily and then solve the problem separately for each subtree. Our ﬁrst algorithm\\nfor calculating diameters is based on this idea.\\nAn important observation is that every path in a rooted tree has a highest point :\\nthe highest node that belongs to the path. Thus, we can calculate for each node xthe\\nlength of the longest path whose highest point is x. One of those paths corresponds\\nto the diameter of the tree. For example, in Fig. 10.5 , node 1 is the highest point on\\nthe path that corresponds to the diameter.\\nWe calculate for each node xtwo values:\\n•toLeaf (x): the maximum length of a path from xto any leaf\\n•maxLength (x): the maximum length of a path whose highest point is x\\nFor example, in Fig. 10.5 ,toLeaf (1)=2, because there is a path 1 →2→6, and\\nmaxLength (1)=4, because there is a path 6 →2→1→4→7. In this case,\\nmaxLength (1)equals the diameter.', metadata={'source': 'documents\\\\doc.pdf', 'page': 137}),\n",
       " Document(page_content='10.1 Basic Techniques 135\\nFig. 10.6 Nodes a,b,a n d c\\nwhen calculating the\\ndiameter1 4\\n2 3 75\\n6a b c\\nFig. 10.7 Why does the\\nalgorithm work? 1 4 2\\n37\\n56\\nab c x\\nDynamic programming can be used to calculate the above values for all nodes\\ninO(n)time. First, to calculate toLeaf (x), we go through the children of x,\\nchoose a child cwith the maximum toLeaf (c), and add one to this value. Then,\\nto calculate maxLength (x), we choose two distinct children aandbsuch that the\\nsumtoLeaf (a)+toLeaf (b)is maximum and add two to this sum. (The cases\\nwhere xhas less than two children are easy special cases.)\\nSecond Algorithm Another efﬁcient way to calculate the diameter of a tree is based\\non two depth-ﬁrst searches. First, we choose an arbitrary node ain the tree and ﬁnd\\nthe farthest node bfrom a. Then, we ﬁnd the farthest node cfrom b. The diameter\\nof the tree is the distance between bandc.\\nFor example, Fig. 10.6 shows a possible way to select nodes a,b, and cwhen\\ncalculating the diameter for our example tree.\\nThis is an elegant method, but why does it work? It helps to draw the tree so that\\nthe path that corresponds to the diameter is horizontal and all other nodes hang from\\nit (Fig. 10.7 ). Node xindicates the place where the path from node ajoins the path\\nthat corresponds to the diameter. The farthest node from ais node b, node c, or some\\nother node that is at least as far from node x. Thus, this node is always a valid choice\\nfor an endpoint of a path that corresponds to the diameter.\\n10.1.3 All Longest Paths\\nOur next problem is to calculate for every tree node xa value maxLength (x): the\\nmaximum length of a path that begins at node x. For example, Fig. 10.8 shows a tree\\nand its maxLength values. This can be seen as a generalization of the tree diameter\\nproblem, because the largest of those lengths equals the diameter of the tree. Also,\\nthis problem can be solved in O(n)time.\\nOnce again, a good starting point is to root the tree arbitrarily. The ﬁrst part of\\nthe problem is to calculate for every node xthe maximum length of a path that goes\\ndownwards through a child of x. For example, the longest path from node 1 goes', metadata={'source': 'documents\\\\doc.pdf', 'page': 138}),\n",
       " Document(page_content='136 10 Tree Algorithms\\nFig. 10.8 Calculating\\nmaximum path lengths\\n1\\n423\\n65maxLength (1)= 2\\nmaxLength (2)= 2\\nmaxLength (3)= 3\\nmaxLength (4)= 3\\nmaxLength (5)= 3\\nmaxLength (6)=3\\nFig. 10.9 The longest path\\nthat starts at node 11\\n4 2 3\\n5 6\\nFig. 10.10 The longest path\\nfrom node 3 goes through its\\nparent1\\n4 2 3\\n5 6\\nFig. 10.11 In this case, the\\nsecond longest path from the\\nparent should be chosen1\\n4 2 3\\n5 6\\nthrough its child 2 (Fig. 10.9 ). This part is easy to solve in O(n)time, because we\\ncan use dynamic programming as we have done previously.\\nThen, the second part of the problem is to calculate for every node xthe maximum\\nlength of a path upwards through its parent p. For example, the longest path from\\nnode 3 goes through its parent 1 (Fig. 10.10 ). At ﬁrst glance, it seems that we should\\nﬁrst move to pand then choose the longest path (upwards or downwards) from\\np. However, this does not always work, because such a path may go through x\\n(Fig. 10.11 ). Still, we can solve the second part in O(n)time by storing the maximum\\nlengths of two paths for each node x:\\n•maxLength 1(x): the maximum length of a path from xto a leaf\\n•maxLength 2(x)the maximum length of a path from xto a leaf, in another\\ndirection than the ﬁrst path', metadata={'source': 'documents\\\\doc.pdf', 'page': 139}),\n",
       " Document(page_content='10.1 Basic Techniques 137\\nFor example, in Fig. 10.11 ,maxLength 1(1)=2 using the path 1 →2→5, and\\nmaxLength 2(1)=1 using the path 1 →3.\\nFinally, to determine the maximum-length path from node xupwards through its\\nparent p, we consider two cases: if the path that corresponds to maxLength 1(p)\\ngoes through x, the maximum length is maxLength 2(p)+1 and otherwise the\\nmaximum length is maxLength 1(p)+1.\\n10.2 Tree Queries\\nIn this section we focus on processing queries on rooted trees. Such queries are\\ntypically related to subtrees and paths of the tree, and they can be processed in\\nconstant or logarithmic time.\\n10.2.1 Finding Ancestors\\nThe kthancestor of a node xin a rooted tree is the node that we will reach if we move\\nklevels up from x. Letancestor (x,k)denote the kth ancestor of a node x(or 0 if\\nthere is no such an ancestor). For example, in Fig. 10.12 ,ancestor (2,1)=1 and\\nancestor (8,2)=4.\\nAn easy way to calculate any value of ancestor (x,k)is to perform a sequence\\nofkmoves in the tree. However, the time complexity of this method is O(k), which\\nmay be slow, because a tree of nnodes may have a path of nnodes.\\nFortunately, we can efﬁciently calculate any value of ancestor (x,k)inO(logk)\\ntime after preprocessing. As in Sect. 7.5.1 , the idea is to ﬁrst precalculate all values\\nofancestor (x,k)where kis a power of two. For example, the values for the tree\\nin Fig. 10.12 are as follows:\\nFig. 10.12 Finding\\nancestors of nodes1\\n2 4 5\\n6 3 7\\n8', metadata={'source': 'documents\\\\doc.pdf', 'page': 140}),\n",
       " Document(page_content='138 10 Tree Algorithms\\nx12345678\\nancestor (x,1)01411247\\nancestor (x,2)00100114\\nancestor (x,4)00000000\\n···\\nSince we know that a node always has less than nancestors, it sufﬁces to calculate\\nO(logn)values for each node and the preprocessing takes O(nlogn)time. After this,\\nany value of ancestor (x,k)can be calculated in O(logk)time by representing k\\nas a sum where each term is a power of two.\\n10.2.2 Subtrees and Paths\\nAtree traversal array contains the nodes of a rooted tree in the order in which a\\ndepth-ﬁrst search from the root node visits them. For example, Fig. 10.13 shows a\\ntree and the corresponding tree traversal array.\\nAn important property of tree traversal arrays is that each subtree of a tree cor-\\nresponds to a subarray in the tree traversal array such that the ﬁrst element of the\\nsubarray is the root node. For example, Fig. 10.14 shows the subarray that corre-\\nsponds to the subtree of node 4.\\nSubtree Queries Suppose that each node in the tree is assigned a value and our task\\nis to process two types of queries: updating the value of a node and calculating the\\nsum of values in the subtree of a node. To solve the problem, we construct a tree\\ntraversal array that contains three values for each node: the identiﬁer of the node, the\\nsize of the subtree, and the value of the node. For example, Fig. 10.15 shows a tree\\nand the corresponding array.\\n1\\n2 3 4 5\\n6 7 8 9\\n12 6 3 4 7 8 9 5\\nFig. 10.13 A tree and its tree traversal array\\n12 6 3 4 7 8 9 5\\nFig. 10.14 The subtree of node 4 in the tree traversal array', metadata={'source': 'documents\\\\doc.pdf', 'page': 141}),\n",
       " Document(page_content='10.2 Tree Queries 139\\nFig. 10.15 A tree traversal\\narray for calculating subtree\\nsums1\\n2 3 4 5\\n6 7 8 92\\n35 3 1\\n44 3 1\\nnode id\\nsubtree size\\nnode val ue12 6 3 4 7 8 9 5\\n9 21141111\\n2 3 4 5 3 4 3 11\\nFig. 10.16 Calculating the\\nsum of values in the subtree\\nof node 4node id\\nsubtree size\\nnode val ue12 6 3 4 7 8 9 5\\n9 21141111\\n2 3 4 5 3 4 3 11\\nUsing this array, we can calculate the sum of values in any subtree by ﬁrst deter-\\nmining the size of the subtree and then summing up the values of the corresponding\\nnodes. For example, Fig. 10.16 shows the values that we access when calculating the\\nsum of values in the subtree of node 4. The last row of the array tells us that the sum\\nof values is 3 +4+3+1=11.\\nTo answer queries efﬁciently, it sufﬁces to store the last row of the array in a\\nbinary indexed or segment tree. After this, we can both update a value and calculate\\nthe sum of values in O(logn)time.\\nPath Queries Using a tree traversal array, we can also efﬁciently calculate sums of\\nvalues on paths from the root node to any node of the tree. As an example, consider\\na problem where our task is to process two types of queries: updating the value of a\\nnode and calculating the sum of values on a path from the root to a node.\\nTo solve the problem, we construct a tree traversal array that contains for each\\nnode its identiﬁer, the size of its subtree, and the sum of values on a path from the\\nroot to the node (Fig. 10.17 ). When the value of a node increases by x, the sums of\\nall nodes in its subtree increase by x. For example, Fig. 10.18 shows the array after\\nincreasing the value of node 4 by 1.\\nTo support both the operations, we need to be able to increase all values in a range\\nand retrieve a single value. This can be done in O(logn)time using a binary indexed\\nor segment tree and a difference array (see Sect. 9.2.3 ).', metadata={'source': 'documents\\\\doc.pdf', 'page': 142}),\n",
       " Document(page_content='140 10 Tree Algorithms\\nFig. 10.17 A tree traversal\\narray for calculating path\\nsums1\\n2 3 4 5\\n6 7 8 94\\n53 5 2\\n35 3 1\\nnode id\\nsubtree size\\npath s um12 6 3 4 7 8 9 5\\n9 21141111\\n4 912 7 914 12 10 6\\nFig. 10.18 Increasing the\\nvalue of node 4 by 1node id\\nsubtree size\\npath s um12 6 3 4 7 8 9 5\\n9 21141111\\n4 912 710 15 13 11 6\\nFig. 10.19 The lowest\\ncommon ancestor of nodes 5\\nand 8 is node 21\\n4 2 3\\n7 5 6\\n8\\n10.2.3 Lowest Common Ancestors\\nThe lowest common ancestor of two nodes of a rooted tree is the lowest node whose\\nsubtree contains both the nodes. For example, in Fig. 10.19 the lowest common\\nancestor of nodes 5 and 8 is node 2.\\nA typical problem is to efﬁciently process queries that require us to ﬁnd the lowest\\ncommon ancestor of two nodes. Next we will discuss two efﬁcient techniques for\\nprocessing such queries.\\nFirst Method Since we can efﬁciently ﬁnd the kth ancestor of any node in the tree,\\nwe can use this fact to divide the problem into two parts. We use two pointers that\\ninitially point to the two nodes whose lowest common ancestor we should ﬁnd.\\nFirst, we make sure that the pointers point to nodes at the same level in the tree.\\nIf this is not the case initially, we move one of the pointers upwards. After this, we', metadata={'source': 'documents\\\\doc.pdf', 'page': 143}),\n",
       " Document(page_content='10.2 Tree Queries 141\\n1\\n4 2 3\\n7 5 6\\n81\\n4 2 3\\n7 5 6\\n8\\nFig. 10.20 Two steps to ﬁnd the lowest common ancestor of nodes 5 and 8\\nnode id\\ndepth12 5 2 6 8 6 21 3 14 7 41\\n12 3 2 3 4 3 21212 3 210 12 3 4 56 7 8 910 11 12 13 14\\nFig. 10.21 An extended tree traversal array for processing lowest common ancestor queries\\ndetermine the minimum number of steps needed to move both pointers upwards so\\nthat they will point to the same node. The node to which the pointers point after this\\nis the lowest common ancestor. Since both parts of the algorithm can be performed\\ninO(logn)time using precomputed information, we can ﬁnd the lowest common\\nancestor of any two nodes in O(logn)time.\\nFigure 10.20 shows how we can ﬁnd the lowest common ancestor of nodes 5 and\\n8 in our example scenario. First, we move the second pointer one level up so that it\\npoints to node 6 which is at the same level with node 5. Then, we move both pointers\\none step upwards to node 2, which is the lowest common ancestor.\\nSecond Method Another way to solve the problem, proposed by Bender and Farach-\\nColton [ 3], is based on an extended tree traversal array, sometimes called an Euler\\ntour tree . To construct the array, we go through the tree nodes using depth-ﬁrst search\\nand add each node to the array always when the depth-ﬁrst search walks through\\nthe node (not only at the ﬁrst visit). Hence, a node that has kchildren appears k+1\\ntimes in the array, and there are a total of 2 n−1 nodes in the array. We store two\\nvalues in the array: the identiﬁer of the node and the depth of the node in the tree.\\nFigure 10.21 shows the resulting array in our example scenario.\\nNow we can ﬁnd the lowest common ancestor of nodes aandbby ﬁnding the node\\nwith the minimum depth between nodes aandbin the array. For example, Fig. 10.22\\nshows how to ﬁnd the lowest common ancestor of nodes 5 and 8. The minimum-\\ndepth node between them is node 2 whose depth is 2, so the lowest common ancestor\\nof nodes 5 and 8 is node 2.\\nNote that since a node may appear several times in the array, there may be mul-\\ntiple ways to choose the positions of nodes aandb. However, any choice correctly\\ndetermines the lowest common ancestor of the nodes.', metadata={'source': 'documents\\\\doc.pdf', 'page': 144}),\n",
       " Document(page_content='142 10 Tree Algorithms\\nnode id\\ndepth\\n↑12 5 2 6 8 6 21 3 14 7 41\\n12 3 2 3 4 3 21212 3 210 12 3 4 56 7 8 910 11 12 13 14\\nFig. 10.22 Finding the lowest common ancestor of nodes 5 and 8\\nFig. 10.23 Calculating the\\ndistance between nodes 5\\nand 81\\n4 2 3\\n7 5 6\\n8\\nUsing this technique, to ﬁnd the lowest common ancestor of two nodes, it sufﬁces\\nto process a range minimum query. A usual way is to use a segment tree to process\\nsuch queries in O(logn)time. However, since the array is static, we can also process\\nqueries in O(1)time after an O(nlogn)time preprocessing.\\nCalculating Distances Finally, consider the problem of processing queries where\\nwe need to calculate the distance between nodes aandb(i.e., the length of the path\\nbetween aandb). It turns out that this problem reduces to ﬁnding the lowest common\\nancestor of the nodes. First, we root the tree arbitrarily. After this, the distance of\\nnodes aandbcan be calculated using the formula\\ndepth (a)+depth (b)−2·depth (c),\\nwhere cis the lowest common ancestor of aandb.\\nFor example, to calculate the distance between nodes 5 and 8 in Fig. 10.23 ,w e\\nﬁrst determine that the lowest common ancestor of the nodes is node 2. Then, since\\nthe depths of the nodes are depth (5)=3,depth (8)=4, and depth (2)=2, we\\nconclude that the distance between nodes 5 and 8 is 3 +4−2·2=3.\\n10.2.4 Merging Data Structures\\nSo far, we have discussed online algorithms for tree queries. Those algorithms are\\nable to process queries one after another in such a way that each query is answered\\nbefore receiving the next query. However, in many problems, the online property\\nis not necessary, and we may use ofﬂine algorithms to solve them. Such algorithms', metadata={'source': 'documents\\\\doc.pdf', 'page': 145}),\n",
       " Document(page_content='10.2 Tree Queries 143\\nFig. 10.24 The subtree of\\nnode 4 contains two nodes\\nwhose value is 31\\n2 3 4 5\\n6 7 8 92\\n35 3 1\\n44 3 1\\nFig. 10.25 Processing\\nqueries using map structures\\n4\\n13\\n11\\n1134\\n121\\nFig. 10.26 Merging map\\nstructures at a node4\\n13\\n11\\n13\\n1\\nare given a complete set of queries which can be answered in any order. Ofﬂine\\nalgorithms are often easier to design than online algorithms.\\nOne method to construct an ofﬂine algorithm is to perform a depth-ﬁrst tree\\ntraversal and maintain data structures in nodes. At each node s, we create a data\\nstructure d[s]that is based on the data structures of the children of s. Then, using\\nthis data structure, all queries related to sare processed.\\nAs an example, consider the following problem: We are given a rooted tree where\\neach node has some value. Our task is to process queries that ask to calculate the\\nnumber of nodes with value xin the subtree of node s. For example, in Fig. 10.24 ,\\nthe subtree of node 4 contains two nodes whose value is 3.\\nIn this problem, we can use map structures to answer the queries. For example,\\nFig. 10.25 shows the maps for node 4 and its children. If we create such a data\\nstructure for each node, we can easily process all given queries, because we can\\nhandle all queries related to a node immediately after creating its data structure.\\nHowever, it would be too slow to create all data structures from scratch. Instead,\\nat each node s, we create an initial data structure d[s]that only contains the value of\\ns. After this, we go through the children of sandmerge d[s]and all data structures\\nd[u]where uis a child of s. For example, in the above tree, the map for node 4\\nis created by merging the maps in Fig. 10.26 . Here the ﬁrst map is the initial data\\nstructure for node 4, and the other three maps correspond to nodes 7, 8, and 9.\\nThe merging at node scan be done as follows: We go through the children of s\\nand at each child umerge d[s]andd[u]. We always copy the contents from d[u]to\\nd[s]. However, before this, we swap the contents of d[s]andd[u]ifd[s]is smaller', metadata={'source': 'documents\\\\doc.pdf', 'page': 146}),\n",
       " Document(page_content='144 10 Tree Algorithms\\nthand[u]. By doing this, each value is copied only O(logn)times during the tree\\ntraversal, which ensures that the algorithm is efﬁcient.\\nTo swap the contents of two data structures aand befﬁciently, we can just use\\nthe following code:\\nswap(a,b);\\nIt is guaranteed that the above code works in constant time when aandbare C++\\nstandard library data structures.\\n10.3 Advanced T echniques\\nIn this section, we discuss two advanced tree processing techniques. Centroid decom-\\nposition divides a tree into smaller subtrees and processes them recursively. Heavy-\\nlight decomposition represents a tree as a set of special paths, which allows us to\\nefﬁciently process path queries.\\n10.3.1 Centroid Decomposition\\nAcentroid of a tree of nnodes is a node whose removal divides the tree into subtrees\\neach of which contains at most ⌊n/2⌋nodes. Every tree has a centroid, and it can be\\nfound by rooting the tree arbitrarily and always moving to the subtree that has the\\nmaximum number of nodes, until the current node is a centroid.\\nIn the centroid decomposition technique, we ﬁrst locate a centroid of the tree and\\nprocess all paths that go through the centroid. After this, we remove the centroid\\nfrom the tree and process the remaining subtrees recursively. Since removing the\\ncentroid always creates subtrees whose size is at most half of the size of the original\\ntree, the time complexity of such an algorithm is O(nlogn), provided that we can\\nprocess each subtree in linear time.\\nFor example, Fig. 10.27 shows the ﬁrst step of a centroid decomposition algorithm.\\nIn this tree, node 5 is the only centroid, so we ﬁrst process all paths that go through\\nFig. 10.27 Centroid\\ndecomposition1 2 3 4\\n5\\n6\\n7 8', metadata={'source': 'documents\\\\doc.pdf', 'page': 147}),\n",
       " Document(page_content='10.3 Advanced Techniques 145\\nFig. 10.28 Heavy-light\\ndecomposition1\\n4 2 3\\n7 5 6\\n8\\nnode 5. After this, node 5 is removed from the tree, and we process the three subtrees\\n{1,2},{3,4}, and{6,7,8}recursively.\\nUsing centroid decomposition, we can, for example, efﬁciently calculate the num-\\nber of paths of length xin a tree. When processing a tree, we ﬁrst ﬁnd a centroid\\nand calculate the number of paths that go through it, which can be done in linear\\ntime. After this, we remove the centroid and recursively process the smaller trees.\\nThe resulting algorithm works in O(nlogn)time.\\n10.3.2 Heavy-Light Decomposition\\nHeavy-light decomposition1divides the nodes of a tree into a set of paths that are\\ncalled heavy paths. The heavy paths are created so that a path between any two tree\\nnodes can be represented as O(logn)subpaths of heavy paths. Using the technique,\\nwe can manipulate nodes on paths between tree nodes almost like elements in an\\narray, with only an additional O(logn)factor.\\nTo construct the heavy paths, we ﬁrst root the tree arbitrarily. Then, we start the\\nﬁrst heavy path at the root of the tree and always move to a node that has a maximum-\\nsize subtree. After this, we recursively process the remaining subtrees. For example,\\nin Fig. 10.28 , there are four heavy paths: 1–2–6–8, 3, 4–7, and 5 (note that two of\\nthe paths only have one node).\\nNow, consider any path between two nodes in the tree. Since we always chose the\\nmaximum-size subtree when creating heavy paths, this guarantees that we can divide\\nthe path into O(logn)subpaths so that each of them is a subpath of a single heavy\\npath. For example, in Fig. 10.28 , the path between nodes 7 and 8 can be divided into\\ntwo heavy subpaths: ﬁrst 7–4, then 1–2–6–8.\\nThe beneﬁt of heavy-light decomposition is that each heavy path can be treated\\nlike an array of nodes. For example, we can assign a segment tree for each heavy\\npath and support sophisticated path queries, such as calculating the minimum node\\nvalue in a path or increasing the value of every node in a path. Such queries can be\\n1Sleator and Tarjan [ 29] introduced the idea in the context of their link/cut tree data structure.', metadata={'source': 'documents\\\\doc.pdf', 'page': 148}),\n",
       " Document(page_content='146 10 Tree Algorithms\\nprocessed in O(log2n)time,2because each path consists of O(logn)heavy paths\\nand each heavy path can be processed in O(logn)time.\\nWhile many problems can be solved using heavy-light decomposition, it is good\\nto keep in mind that there is often another solution that is easier to implement.\\nIn particular, the techniques presented in Sect. 10.2.2 can often be used instead of\\nheavy-light decomposition.\\n2The notation logkncorresponds to (logn)k.', metadata={'source': 'documents\\\\doc.pdf', 'page': 149}),\n",
       " Document(page_content='Mathematics\\nThis chapter deals with mathematical topics that are recurrent in competitive pro-\\ngramming. We will both discuss theoretical results and learn how to use them in\\npractice in algorithms.\\nSection 11.1 discusses number-theoretical topics. We will learn algorithms for\\nﬁnding prime factors of numbers, techniques related to modular arithmetic, and\\nefﬁcient methods for solving integer equations.\\nSection 11.2 explores ways to approach combinatorial problems: how to efﬁciently\\ncount all valid combinations of objects. The topics of this section include binomial\\ncoefﬁcients, Catalan numbers, and inclusion-exclusion.\\nSection 11.3 shows how to use matrices in algorithm programming. For example,\\nwe will learn how to make a dynamic programming algorithm more efﬁcient by\\nexploiting an efﬁcient way to calculate matrix powers.\\nSection 11.4 ﬁrst discusses basic techniques for calculating probabilities of events\\nand the concept of Markov chains. After this, we will see examples of algorithms\\nthat are based on randomness.\\nSection 11.5 focuses on game theory. First, we will learn to optimally play a simple\\nstick game using nim theory, and after this, we will generalize the strategy to a wide\\nrange of other games.\\n11.1 Number Theory\\nNumber theory is a branch of mathematics that studies integers. In this section, we\\nwill discuss a selection of number-theoretical topics and algorithms, such as ﬁnding\\nprime numbers and factors, and solving integer equations.', metadata={'source': 'documents\\\\doc.pdf', 'page': 150}),\n",
       " Document(page_content='148 11 Mathematics\\n11.1.1 Primes and Factors\\nAn integer ais called a factor or a divisor of an integer bifadivides b.I fais a\\nfactor of b, we write a|b, and otherwise we write a∤b. For example, the factors of\\n24 are 1, 2, 3, 4, 6, 8, 12, and 24.\\nAn integer n>1i sa prime if its only positive factors are 1 and n. For example,\\n7, 19, and 41 are primes, but 35 is not a prime, because 5 ·7=35. For every integer\\nn>1, there is a unique prime factorization\\nn=pα1\\n1pα2\\n2···pαk\\nk,\\nwhere p1,p2,..., pkare distinct primes and α1,α2,...,α kare positive integers.\\nFor example, the prime factorization for 84 is\\n84=22·31·71.\\nLetτ(n)denote the number of factors of an integer n. For example, τ(12)=6,\\nbecause the factors of 12 are 1, 2, 3, 4, 6, and 12. To calculate the value of τ(n),w e\\ncan use the formula\\nτ(n)=k∏\\ni=1(αi+1),\\nbecause for each prime pi, there are αi+1 ways to choose how many times it appears\\nin the factor. For example, since 12 =22·3,τ(12)=3·2=6.\\nThen, let σ(n)denote the sum of factors of an integer n. For example, σ(12)=28,\\nbecause 1 +2+3+4+6+12=28. To calculate the value of σ(n), we can use\\nthe formula\\nσ(n)=k∏\\ni=1(1+pi+···+ pαi\\ni)=k∏\\ni=1pαi+1\\ni−1\\npi−1,\\nwhere the latter form is based on the geometric progression formula. For example,\\nσ(12)=(23−1)/(2−1)·(32−1)/(3−1)=28.\\nBasic Algorithms If an integer nis not prime, it can be represented as a product\\na·b, where a≤√norb≤√n, so it certainly has a factor between 2 and ⌊√n⌋.\\nUsing this observation, we can both test if an integer is prime and ﬁnd its prime\\nfactorization in O(√n)time.\\nThe following function prime checks if a given integer nis prime. The function\\nattempts to divide nby all integers between 2 and ⌊√n⌋, and if none of them divides\\nn, then nis prime.', metadata={'source': 'documents\\\\doc.pdf', 'page': 151}),\n",
       " Document(page_content='11.1 Number Theory 149\\nbool prime( int n) {\\nif( n<2 ) return false ;\\nfor (int x = 2; x*x <= n; x++) {\\nif(n%x == 0) return false ;\\n}\\nreturn true ;\\n}\\nThen, the following function factors constructs a vector that contains the prime\\nfactorization of n. The function divides nby its prime factors and adds them to the\\nvector. The process ends when the remaining number nhas no factors between 2 and\\n⌊√n⌋.I fn>1, it is prime and the last factor.\\nvector< int> factors( int n) {\\nvector< int>f ;\\nfor (int x = 2; x*x <= n; x++) {\\nwhile (n%x == 0) {\\nf.push_back(x);\\nn/ =x ;\\n}\\n}\\nif(n > 1) f.push_back(n);\\nreturn f;\\n}\\nNote that each prime factor appears in the vector as many times as it divides the\\nnumber. For example, 12 =22·3, so the result of the function is [2,2,3].\\nProperties of Primes It is easy to show that there is an inﬁnite number of primes. If\\nthe number of primes would be ﬁnite, we could construct a set P={p1,p2,..., pn}\\nthat would contain all the primes. For example, p1=2,p2=3,p3=5, and so on.\\nHowever, using such a set P, we could form a new prime\\np1p2···pn+1\\nthat would be larger than all elements in P. This is a contradiction, and the number\\nof primes has to be inﬁnite.\\nTheprime-counting function π(n)gives the number of primes up to n. For exam-\\nple,π(10)=4, because the primes up to 10 are 2, 3, 5, and 7. It is possible to show\\nthat\\nπ(n)≈n\\nlnn,\\nwhich means that primes are quite frequent. For example, an approximation for\\nπ(106)is 106/ln 106≈72382, and the exact value is 78498.', metadata={'source': 'documents\\\\doc.pdf', 'page': 152}),\n",
       " Document(page_content='150 11 Mathematics\\n00 1 0 1 0 111 0 1 0 111 0 1 0 12 3 4 56 7 8 910 11 12 13 14 15 16 17 18 19 20\\nFig. 11.1 Outcome of the sieve of Eratosthenes for n=20\\n2 3 2 5 2 7 2 3 21 12 13 2 3 217 219 22 3 4 56 7 8 910 11 12 13 14 15 16 17 18 19 20\\nFig.11.2 An extended sieve of Eratosthenes that contains the smallest prime factor of each number\\n11.1.2 Sieve of Eratosthenes\\nThe sieve of Eratosthenes is a preprocessing algorithm that constructs an array\\nsieve from which we can efﬁciently check if any integer xbetween 2 ...nis\\nprime. If xis prime, then sieve [x]= 0, and otherwise sieve [x]= 1. For exam-\\nple, Fig. 11.1 shows the contents of sieve forn=20.\\nTo construct the array, the algorithm iterates through the integers 2 ...none by\\none. Always when a new prime xis found, the algorithm records that the num-\\nbers 2 x,3x,4x, etc., are not primes. The algorithm can be implemented as follows,\\nassuming that every element of sieve is initially zero:\\nfor (int x = 2; x <= n; x++) {\\nif(sieve[x]) continue ;\\nfor (int u=2 * x ;u< =n ;u+ =x ){\\nsieve[u] = 1;\\n}\\n}\\nThe inner loop of the algorithm is executed ⌊n/x⌋times for each value of x. Thus,\\nan upper bound for the running time of the algorithm is the harmonic sum\\nn∑\\nx=2⌊n/x⌋=⌊ n/2⌋+⌊ n/3⌋+⌊ n/4⌋+···= O(nlogn).\\nIn fact, the algorithm is more efﬁcient, because the inner loop will be executed\\nonly if the number xis prime. It can be shown that the running time of the algorithm\\nis only O(nlog log n), a complexity very near to O(n). In practice, the sieve of\\nEratosthenes is very efﬁcient; Table 11.1 shows some real running times.\\nThere are several ways to extend the sieve of Eratosthenes. For example, we can\\ncalculate for each number kits smallest prime factor (Fig. 11.2 ). After this, we can\\nefﬁciently factorize any number between 2 ...nusing the sieve. (Note that a number\\nnhasO(logn)prime factors.)', metadata={'source': 'documents\\\\doc.pdf', 'page': 153}),\n",
       " Document(page_content='11.1 Number Theory 151\\nTable 11.1 Running times of\\nthe sieve of EratosthenesUpper bound n Running time (s)\\n1060.01\\n2·1060.03\\n4·1060.07\\n8·1060.14\\n16·1060.28\\n32·1060.57\\n64·1061.16\\n128·1062.35\\n11.1.3 Euclid’s Algorithm\\nThegreatest common divisor of integers aandb, denoted gcd (a,b), is the largest\\ninteger that divides both aandb. For example, gcd (30,12)=6. A related concept\\nis the lowest common multiple , denoted lcm (a,b), which is the smallest integer that\\nis divisible by both aandb. The formula\\nlcm(a,b)=ab\\ngcd(a,b)\\ncan be used to calculate lowest common multiples. For example, lcm (30,12)=\\n360/gcd(30,12)=60.\\nOne way to ﬁnd gcd (a,b)is to divide aandbinto prime factors, and then choose\\nfor each prime the largest power that appears in both factorizations. For example,\\nto calculate gcd (30,12), we can construct the factorizations 30 =2·3·5 and\\n12=22·3, and conclude that gcd (30,12)=2·3=6. However, this technique is\\nnot efﬁcient if aandbare large numbers.\\nEuclid’s algorithm provides an efﬁcient way to calculate the value of gcd (a,b).\\nThe algorithm is based on the formula\\ngcd(a,b)={\\nab =0\\ngcd(b,amod b)b̸=0.\\nFor example,\\ngcd(30,12)=gcd(12,6)=gcd(6,0)=6.\\nThe algorithm can be implemented as follows:\\nint gcd( int a,int b) {\\nif(b == 0) return a;\\nreturn gcd(b, a%b);\\n}', metadata={'source': 'documents\\\\doc.pdf', 'page': 154}),\n",
       " Document(page_content='152 11 Mathematics\\nFig. 11.3 Why does\\nEuclid’s algorithm work?a\\nbb amod b\\nxxxxxxxx\\nWhy does the algorithm work? To understand this, consider Fig. 11.3 . where\\nx=gcd(a,b). Since xdivides both aandb, it must also divide amod b, which\\nshows why the recursive formula holds.\\nIt can be proved that Euclid’s algorithm works in O(logn)time, where n=\\nmin(a,b).\\nExtended Euclid’s Algorithm Euclid’s algorithm can also be extended so that it\\ngives integers xandyfor which\\nax+by=gcd(a,b).\\nFor example, when a=30 and b=12,\\n30·1+12·(−2)=6.\\nWe can solve also this problem using the formula gcd (a,b)=gcd(b,amod b).\\nSuppose that we have already solved the problem for gcd (b,amod b), and we know\\nvalues x′andy′for which\\nbx′+(amod b)y′=gcd(a,b).\\nThen, since amod b=a−⌊a/b⌋·b,\\nbx′+(a−⌊a/b⌋·b)y′=gcd(a,b),\\nwhich equals\\nay′+b(x′−⌊a/b⌋·y′)=gcd(a,b).\\nThus, we can choose x=y′andy=x′−⌊a/b⌋·y′. Using this idea, the following\\nfunction returns a tuple (x,y,gcd(a,b))that satisﬁes the equation.\\ntuple< int,int,int> gcd( int a,int b) {\\nif(b == 0) {\\nreturn {1,0,a};\\n}else {\\nint x,y,g;\\ntie(x,y,g) = gcd(b,a%b);\\nreturn {y,x-(a/b)*y,g};\\n}\\n}', metadata={'source': 'documents\\\\doc.pdf', 'page': 155}),\n",
       " Document(page_content='11.1 Number Theory 153\\nWe can use the function as follows:\\nint x,y,g;\\ntie(x,y,g) = gcd(30,12);\\ncout << x < <\"\"< <y< <\"\"< <g< <\" \\\\ n \" ; / /1- 26\\n11.1.4 Modular Exponentiation\\nThere is often a need to efﬁciently calculate the value of xnmod m. This can be done\\ninO(logn)time using the following recursive formula:\\nxn=⎧\\n⎪⎨\\n⎪⎩1 n=0\\nxn/2·xn/2nis even\\nxn−1·xn is odd\\nFor example, to calculate the value of x100, we ﬁrst calculate the value of x50and\\nthen use the formula x100=x50·x50. Then, to calculate the value of x50, we ﬁrst\\ncalculate the value of x25and so on. Since nalways halves when it is even, the\\ncalculation takes only O(logn)time.\\nThe algorithm can be implemented as follows:\\nint modpow( int x,int n,int m) {\\nif(n == 0) return 1%m;\\nlong long u = modpow(x,n/2,m);\\nu = (u*u)%m;\\nif( n % 2= =1 )u=( u * x ) % m ;\\nreturn u;\\n}\\n11.1.5 Euler’s Theorem\\nTwo integers aandbare called coprime if gcd(a,b)=1.Euler’s totient function\\nϕ(n)gives the number of integers between 1 ...nthat are coprime to n. For example,\\nϕ(10)=4, because 1, 3, 7, and 9 are coprime to 10.\\nAny value of ϕ(n)can be calculated from the prime factorization of nusing the\\nformula\\nϕ(n)=k∏\\ni=1pαi−1\\ni(pi−1).\\nFor example, since 10 =2·5,ϕ(10)=20·(2−1)·50·(5−1)=4.', metadata={'source': 'documents\\\\doc.pdf', 'page': 156}),\n",
       " Document(page_content='154 11 Mathematics\\nEuler’s theorem states that\\nxϕ(m)mod m=1\\nfor all positive coprime integers xandm. For example, Euler’s theorem tells us that\\n74mod 10 =1, because 7 and 10 are coprime and ϕ(10)=4.\\nIfmis prime, ϕ(m)=m−1, so the formula becomes\\nxm−1mod m=1,\\nwhich is known as Fermat’s little theorem . This also implies that\\nxnmod m=xnmod(m−1)mod m,\\nwhich can be used to calculate values of xnifnis very large.\\nModular Multiplicative Inverses The modular multiplicative inverse ofxwith\\nrespect to mis a value inv m(x)such that\\nx·invm(x)mod m=1.\\nFor example, inv 17(6)=3, because 6 ·3 mod 17 =1.\\nUsing modular multiplicative inverses, we can divide numbers modulo m, because\\ndivision by xcorresponds to multiplication by inv m(x). For example, since we know\\nthat inv 17(6)=3, we can calculate the value of 36 /6 mod 17 in another way using\\nthe formula 36 ·3 mod 17.\\nA modular multiplicative inverse exists exactly when xandmare coprime. In this\\ncase, it can be calculated using the formula\\ninvm(x)=xϕ(m)−1,\\nwhich is based on Euler’s theorem. In particular, if mis prime, ϕ(m)=m−1 and\\nthe formula becomes\\ninvm(x)=xm−2.\\nFor example,\\ninv 17(6)mod 17 =617−2mod 17 =3.\\nThe above formula allows us to efﬁciently calculate modular multiplicative\\ninverses using the modular exponentiation algorithm (Sect. 11.1.4 ).', metadata={'source': 'documents\\\\doc.pdf', 'page': 157}),\n",
       " Document(page_content='11.1 Number Theory 155\\n11.1.6 Solving Equations\\nDiophantine Equations ADiophantine equation is an equation of the form\\nax+by=c,\\nwhere a,b, and care constants and the values of xandyshould be found. Each\\nnumber in the equation has to be an integer. For example, one solution to the equation\\n5x+2y=11\\nisx=3 and y=− 2.\\nWe can efﬁciently solve a Diophantine equation by using the extended Euclid’s\\nalgorithm (Sect. 11.1.3 ) which gives integers xandythat satisfy the equation\\nax+by=gcd(a,b).\\nA Diophantine equation can be solved exactly when cis divisible by gcd (a,b).\\nAs an example, let us ﬁnd integers xandythat satisfy the equation\\n39x+15y=12.\\nThe equation can be solved, because gcd (39,15)=3 and 3 |12. The extended\\nEuclid’s algorithm gives us\\n39·2+15·(−5)=3,\\nand by multiplying this by 4, the equation becomes\\n39·8+15·(−20)=12,\\nso a solution to the equation is x=8 and y=− 20.\\nA solution to a Diophantine equation is not unique, because we can form an inﬁnite\\nnumber of solutions if we know one solution. If a pair (x,y)is a solution, then also\\nall pairs\\n(\\nx+kb\\ngcd(a,b),y−ka\\ngcd(a,b))\\nare solutions, where kis any integer.\\nChinese Remainder Theorem TheChinese remainder theorem solves a group of\\nequations of the form\\nx=a1mod m1\\nx=a2mod m2\\n···\\nx=anmod mn', metadata={'source': 'documents\\\\doc.pdf', 'page': 158}),\n",
       " Document(page_content='156 11 Mathematics\\nwhere all pairs of m1,m2,..., mnare coprime.\\nIt turns out that a solution to the equations is\\nx=a1X1invm1(X1)+a2X2invm2(X2)+···+ anXninvmn(Xn),\\nwhere\\nXk=m1m2···mn\\nmk.\\nIn this solution, for each k=1,2,..., n,\\nakXkinvmk(Xk)mod mk=ak,\\nbecause\\nXkinvmk(Xk)mod mk=1.\\nSince all other terms in the sum are divisible by mk, they have no effect on the\\nremainder and xmod mk=ak.\\nFor example, a solution for\\nx=3 mod 5\\nx=4 mod 7\\nx=2 mod 3\\nis\\n3·21·1+4·15·1+2·35·2=263.\\nOnce we have found a solution x, we can create an inﬁnite number of other\\nsolutions, because all numbers of the form\\nx+m1m2···mn\\nare solutions.\\n11.2 Combinatorics\\nCombinatorics studies methods for counting combinations of objects. Usually, the\\ngoal is to ﬁnd a way to count the combinations efﬁciently without generating each\\ncombination separately. In this section, we discuss a selection of combinatorial tech-\\nniques that can be applied to a large number of problems.', metadata={'source': 'documents\\\\doc.pdf', 'page': 159}),\n",
       " Document(page_content='11.2 Combinatorics 157\\n11.2.1 Binomial Coefﬁcients\\nThe binomial coefﬁcient(n\\nk)\\ngives the number of ways we can choose a subset\\nofkelements from a set of nelements. For example,(5\\n3)\\n=10, because the set\\n{1,2,3,4,5}has 10 subsets of 3 elements:\\n{1,2,3},{1,2,4},{1,2,5},{1,3,4},{1,3,5},\\n{1,4,5},{2,3,4},{2,3,5},{2,4,5},{3,4,5}\\nBinomial coefﬁcients can be recursively calculated using the formula\\n(n\\nk)\\n=(n−1\\nk−1)\\n+(n−1\\nk)\\n.\\nwith the base cases(n\\n0)\\n=(n\\nn)\\n=1.\\nTo see why this formula works, consider an arbitrary element xin the set. If we\\ndecide to include xin our subset, the remaining task is to choose k−1 elements\\nfrom n−1 elements. Then, if we do not include xin our subset, we have to choose\\nkelements from n−1 elements.\\nAnother way to calculate binomial coefﬁcients is to use the formula\\n(n\\nk)\\n=n!\\nk!(n−k)!\\nwhich is based on the following reasoning: There are n!permutations of nelements.\\nWe go through all permutations and always include the ﬁrst kelements of the per-\\nmutation in the subset. Since the order of the elements in the subset and outside the\\nsubset does not matter, the result is divided by k!and(n−k)!\\nFor binomial coefﬁcients,\\n(n\\nk)\\n=(n\\nn−k)\\n,\\nbecause we actually divide a set of nelements into two subsets: the ﬁrst contains k\\nelements and the second contains n−kelements.\\nThe sum of binomial coefﬁcients is\\n(n\\n0)\\n+(n\\n1)\\n+(n\\n2)\\n+···+(n\\nn)\\n=2n.\\nThe reason for the name “binomial coefﬁcient” can be seen when the binomial\\n(a+b)is raised to the nth power:\\n(a+b)n=(n\\n0)\\nanb0+(n\\n1)\\nan−1b1+···+(n\\nn−1)\\na1bn−1+(n\\nn)\\na0bn.', metadata={'source': 'documents\\\\doc.pdf', 'page': 160}),\n",
       " Document(page_content='158 11 Mathematics\\nFig. 11.4 First 5 rows of\\nPascal’s triangle1\\n11\\n121\\n1 33 1\\n14 6 41\\n... ... ... ... ...\\nFig. 11.5 Scenario 1: Each\\nbox contains at most one ball\\nBinomial coefﬁcients also appear in Pascal’s triangle (Fig. 11.4 ) where each value\\nequals the sum of two above values.\\nMultinomial Coefﬁcients Themultinomial coefﬁcient\\n(n\\nk1,k2,..., km)\\n=n!\\nk1!k2!···km!,\\ngives the number of ways a set of nelements can be divided into subsets of sizes\\nk1,k2,..., km, where k1+k2+···+ km=n. Multinomial coefﬁcients can be seen\\nas a generalization of binomial coefﬁcients; if m=2, the above formula corresponds\\nto the binomial coefﬁcient formula.\\nBoxes and Balls “Boxes and balls” is a useful model, where we count the ways to\\nplace kballs in nboxes. Let us consider three scenarios:\\nScenario 1 : Each box can contain at most one ball. For example, when n=5\\nandk=2, there are 10 combinations (Fig. 11.5 ). In this scenario, the number of\\ncombinations is directly the binomial coefﬁcient(n\\nk)\\n.\\nScenario 2 : A box can contain multiple balls. For example, when n=5 and\\nk=2, there are 15 combinations (Fig. 11.6 ). In this scenario, the process of placing\\nthe balls in the boxes can be represented as a string that consists of symbols “o”\\nand “→.” Initially, assume that we are standing at the leftmost box. The symbol “o”\\nmeans that we place a ball in the current box, and the symbol “ →” means that we\\nmove to the next box to the right. Now each solution is a string of length k+n−1\\nthat contains ksymbols “o” and n−1 symbols “ →.” For example, the upper-right\\nsolution in Fig. 11.6 corresponds to the string “ →→ o→o→.” Thus, we can\\nconclude that the number of combinations is(k+n−1\\nk)\\n.\\nScenario 3 : Each box may contain at most one ball, and in addition, no two adjacent\\nboxes may both contain a ball. For example, when n=5 and k=2, there are 6\\ncombinations (Fig. 11.7 ). In this scenario, we can assume that kballs are initially\\nplaced in the boxes and there is an empty box between each two adjacent boxes. The', metadata={'source': 'documents\\\\doc.pdf', 'page': 161}),\n",
       " Document(page_content='11.2 Combinatorics 159\\nFig. 11.6 Scenario 2: A box\\nmay contain multiple balls\\nFig. 11.7 Scenario 3: Each\\nbox contains at most one ball\\nand no two adjacent boxes\\ncontain a ball\\nremaining task is to choose the positions for the remaining empty boxes. There are\\nn−2k+1 such boxes and k+1 positions for them. Thus, using the formula of\\nScenario 2, the number of solutions is(n−k+1\\nn−2k+1)\\n.\\n11.2.2 Catalan Numbers\\nThe Catalan number C ngives the number of valid parenthesis expressions that\\nconsist of nleft parentheses and nright parentheses. For example, C3=5, because\\nwe can construct a total of ﬁve parenthesis expressions using three left parentheses\\nand three right parentheses:\\n•()()()\\n•(())()\\n•()(())\\n•((()))\\n•(()())\\nWhat is exactly a valid parenthesis expression ? The following rules precisely\\ndeﬁne all valid parenthesis expressions:\\n•An empty parenthesis expression is valid.\\n•If an expression Ais valid, then also the expression (A)is valid.\\n•If expressions AandBare valid, then also the expression AB is valid.\\nAnother way to characterize valid parenthesis expressions is that if we choose\\nany preﬁx of such an expression, it has to contain at least as many left parentheses\\nas right parentheses, and the complete expression has to contain an equal number of\\nleft and right parentheses.', metadata={'source': 'documents\\\\doc.pdf', 'page': 162}),\n",
       " Document(page_content='160 11 Mathematics\\nCatalan numbers can be calculated using the formula\\nCn=n−1∑\\ni=0CiCn−i−1\\nwhere we consider the ways to divide the parenthesis expression into two parts that\\nare both valid parenthesis expressions, and the ﬁrst part is as short as possible but not\\nempty. For each i, the ﬁrst part contains i+1 pairs of parentheses and the number\\nof valid expressions is the product of the following values:\\n•Ci: the number of ways to construct a parenthesis expression using the parentheses\\nof the ﬁrst part, not counting the outermost parentheses\\n•Cn−i−1: the number of ways to construct a parenthesis expression using the paren-\\ntheses of the second part\\nThe base case is C0=1, because we can construct an empty parenthesis expres-\\nsion using zero pairs of parentheses.\\nCatalan numbers can also be calculated using the formula\\nCn=1\\nn+1(2n\\nn)\\n,\\nwhich can be explained as follows:\\nThere are a total of(2n\\nn)\\nways to construct a (not necessarily valid) parenthesis\\nexpression that contains nleft parentheses and nright parentheses. Let us calculate\\nthe number of such expressions that are not valid.\\nIf a parenthesis expression is not valid, it has to contain a preﬁx where the number\\nof right parentheses exceeds the number of left parentheses. The idea is to pick the\\nshortest such preﬁx and reverse each parenthesis in the preﬁx. For example, the\\nexpression ())()( has the preﬁx ()) , and after reversing the parentheses, the\\nexpression becomes )((()( . The resulting expression consists of n+1 left and\\nn−1 right parentheses. In fact, there is a unique way to produce any expression\\nofn+1 left and n−1 right parentheses in the above manner. The number of such\\nexpressions is(2n\\nn+1)\\n, which equals the number of nonvalid parenthesis expressions.\\nThus, the number of valid parenthesis expressions can be calculated using the formula\\n(2n\\nn)\\n−(2n\\nn+1)\\n=(2n\\nn)\\n−n\\nn+1(2n\\nn)\\n=1\\nn+1(2n\\nn)\\n.\\nCounting Trees We can also count certain tree structures using Catalan numbers.\\nFirst, Cnequals the number of binary trees of nnodes, assuming that left and right\\nchildren are distinguished. For example, since C3=5, there are 5 binary trees of 3\\nnodes (Fig. 11.8 ). Then, Cnalso equals the number of general rooted trees of n+1\\nnodes. For example, there are 5 rooted trees of 4 nodes (Fig. 11.9 ).', metadata={'source': 'documents\\\\doc.pdf', 'page': 163}),\n",
       " Document(page_content='11.2 Combinatorics 161\\nFig. 11.8 There are 5 binary\\ntrees of 3 nodes\\nFig. 11.9 There are 5 rooted\\ntrees of 4 nodes\\nFig. 11.10\\nInclusion-exclusion principle\\nfor two setsA B A∩B\\nFig. 11.11\\nInclusion-exclusion principle\\nfor three sets\\nA BC\\nA∩BA∩CB ∩C\\nA∩B∩C\\n11.2.3 Inclusion-Exclusion\\nInclusion-exclusion is a technique that can be used for counting the size of a union of\\nsets when the sizes of the intersections are known, and vice versa. A simple example\\nof the technique is the formula\\n|A∪B|=| A|+|B|−| A∩B|,\\nwhere AandBare sets and |X|denotes the size of X. Figure 11.10 illustrates the\\nformula. In this case, we want to calculate the size of the union A∪Bthat corresponds\\nto the area of the region that belongs to at least one circle in Fig. 11.10 . We can\\ncalculate the area of A∪Bby ﬁrst summing up the areas of Aand Band then\\nsubtracting the area of A∩Bfrom the result.\\nThe same idea can be applied when the number of sets is larger. When there are\\nthree sets, the inclusion-exclusion formula is\\n|A∪B∪C|=| A|+|B|+|C|−| A∩B|−| A∩C|−|B∩C|+| A∩B∩C|,\\nwhich corresponds to Fig. 11.11 .\\nIn the general case, the size of the union X1∪X2∪···∪ Xncan be calculated by\\ngoing through all possible intersections that contain some of the sets X1,X2,..., Xn.', metadata={'source': 'documents\\\\doc.pdf', 'page': 164}),\n",
       " Document(page_content='162 11 Mathematics\\nIf an intersection contains an odd number of sets, its size is added to the answer, and\\notherwise its size is subtracted from the answer.\\nNote that there are similar formulas for calculating the size of an intersection from\\nthe sizes of unions. For example,\\n|A∩B|=| A|+|B|−| A∪B|\\nand\\n|A∩B∩C|=| A|+|B|+|C|−| A∪B|−| A∪C|−|B∪C|+| A∪B∪C|.\\nCounting Derangements As an example, let us count the number of derangements\\nof{1,2,..., n}, i.e., permutations where no element remains in its original place.\\nFor example, when n=3, there are two derangements: (2,3,1)and(3,1,2).\\nOne approach for solving the problem is to use inclusion-exclusion. Let Xkbe\\nthe set of permutations that contain the element kat position k. For example, when\\nn=3, the sets are as follows:\\nX1={(1,2,3), (1,3,2)}\\nX2={(1,2,3), (3,2,1)}\\nX3={(1,2,3), (2,1,3)}\\nThe number of derangements equals\\nn!−|X1∪X2∪···∪ Xn|,\\nso it sufﬁces to calculate |X1∪X2∪···∪ Xn|. Using inclusion-exclusion, this reduces\\nto calculating sizes of intersections. Moreover, an intersection of cdistinct sets Xk\\nhas(n−c)!elements, because such an intersection consists of all permutations that\\ncontain celements in their original places. Thus, we can efﬁciently calculate the\\nsizes of the intersections. For example, when n=3,\\n|X1∪X2∪X3|=| X1|+|X2|+|X3|\\n−|X1∩X2|−|X1∩X3|−|X2∩X3|\\n+|X1∩X2∩X3|\\n=2+2+2−1−1−1+1\\n=4,\\nso the number of derangements is 3 !−4=2.\\nIt turns out that the problem can also be solved without using inclusion-exclusion.\\nLet f(n)denote the number of derangements for {1,2,..., n}. We can use the\\nfollowing recursive formula:\\nf(n)=⎧\\n⎪⎨\\n⎪⎩0 n=1\\n1 n=2\\n(n−1)(f(n−2)+f(n−1))n>2', metadata={'source': 'documents\\\\doc.pdf', 'page': 165}),\n",
       " Document(page_content='11.2 Combinatorics 163\\nFig. 11.12 Four symmetric\\nnecklaces\\nThe formula can be proved by considering the possibilities how the element 1\\nchanges in the derangement. There are n−1 ways to choose an element xthat\\nreplaces the element 1. In each such choice, there are two options:\\nOption 1: We also replace the element xwith the element 1. After this, the remain-\\ning task is to construct a derangement of n−2 elements.\\nOption 2: We replace the element xwith some other element than 1. Now we\\nhave to construct a derangement of n−1 element, because we cannot replace the\\nelement xwith the element 1, and all other elements must be changed.\\n11.2.4 Burnside’s Lemma\\nBurnside’s lemma can be used to count the number of distinct combinations so that\\nsymmetric combinations are counted only once. Burnside’s lemma states that the\\nnumber of combinations is\\n1\\nnn∑\\nk=1c(k),\\nwhere there are nways to change the position of a combination, and there are c(k)\\ncombinations that remain unchanged when the kth way is applied.\\nAs an example, let us calculate the number of necklaces of npearls, where\\neach pearl has mpossible colors. Two necklaces are symmetric if they are simi-\\nlar after rotating them. For example, Fig. 11.12 shows four symmetric necklaces,\\nwhich should be counted as a single combination.\\nThere are nways to change the position of a necklace, because it can be rotated\\nk=0,1,..., n−1 steps clockwise. For example, if k=0, all mnnecklaces remain\\nthe same, and if k=1, only the mnecklaces where each pearl has the same color\\nremain the same. In the general case, a total of mgcd(k,n)necklaces remain the same,\\nbecause blocks of pearls of size gcd (k,n)will replace each other. Thus, according\\nto Burnside’s lemma, the number of distinct necklaces is\\n1\\nnn−1∑\\nk=0mgcd(k,n).\\nFor example, the number of distinct necklaces of 4 pearls and 3 colors is\\n34+3+32+3\\n4=24.', metadata={'source': 'documents\\\\doc.pdf', 'page': 166}),\n",
       " Document(page_content='164 11 Mathematics\\nFig. 11.13 There are 16\\ndistinct labeled trees of 4\\nnodes1\\n2 3 42\\n1 3 43\\n1 2 44\\n1 2 3\\n1 2 3 4 1 2 4 3 1 3 2 4\\n1 3 4 2 1 4 2 3 1 4 3 2\\n2 1 3 4 2 1 4 3 2 3 1 4\\n2 4 1 3 3 1 2 4 3 2 1 4\\nFig. 11.14 Prüfer code of\\nthis tree is [4,4,2]1 2\\n3 45\\n11.2.5 Cayley’s Formula\\nCayley’s formula states that there are a total of nn−2distinct labeled trees of nnodes.\\nThe nodes are labeled 1 ,2,..., n, and two trees are considered distinct if either their\\nstructure or labeling is different. For example, when n=4, there are 44−2=16\\nlabeled trees, shown in Fig. 11.13 .\\nCayley’s formula can be proved using Prüfer codes. A Prüfer code is a sequence\\nofn−2 numbers that describes a labeled tree. The code is constructed by following\\na process that removes n−2 leaves from the tree. At each step, the leaf with the\\nsmallest label is removed, and the label of its only neighbor is added to the code.\\nFor example, the Prüfer code of the tree in Fig. 11.14 is[4,4,2], because we remove\\nleaves 1, 3, and 4.\\nWe can construct a Prüfer code for any tree, and more importantly, the original\\ntree can be reconstructed from a Prüfer code. Hence, the number of labeled trees of\\nnnodes equals nn−2, the number of Prüfer codes of length n.\\n11.3 Matrices\\nAmatrix is a mathematical concept that corresponds to a two-dimensional array in\\nprogramming. For example,\\nA=⎡\\n⎣61 37 4\\n7082\\n954 1 8⎤\\n⎦', metadata={'source': 'documents\\\\doc.pdf', 'page': 167}),\n",
       " Document(page_content='11.3 Matrices 165\\nis a matrix of size 3 ×4, i.e., it has 3 rows and 4 columns. The notation [i,j]refers\\nto the element in row iand column jin a matrix. For example, in the above matrix,\\nA[2,3]= 8 and A[3,1]= 9.\\nA special case of a matrix is a vector that is a one-dimensional matrix of size\\nn×1. For example,\\nV=⎡\\n⎣4\\n7\\n5⎤\\n⎦\\nis a vector that contains three elements.\\nThetranspose ATof a matrix Ais obtained when the rows and columns of Aare\\nswapped, i.e., AT[i,j]=A[j,i]:\\nAT=⎡\\n⎢⎢⎣679\\n13 0 5\\n784\\n42 1 8⎤\\n⎥⎥⎦\\nA matrix is a square matrix if it has the same number of rows and columns. For\\nexample, the following matrix is a square matrix:\\nS=⎡\\n⎣31 2 4\\n591 5\\n02 4⎤\\n⎦\\n11.3.1 Matrix Operations\\nThe sum A+Bof matrices AandBis deﬁned if the matrices are of the same size.\\nThe result is a matrix where each element has the sum of the corresponding elements\\ninAandB. For example,\\n[614\\n392]\\n+[493\\n813]\\n=[6+41+94+3\\n3+89+12+3]\\n=[10 10 7\\n11 10 5]\\n.\\nMultiplying a matrix Aby a value xmeans that each element of Ais multiplied\\nbyx. For example,\\n2·[614\\n392]\\n=[2·62·12·4\\n2·32·92·2]\\n=[12 2 8\\n61 8 4]\\n.\\nThe product AB of matrices AandBis deﬁned if Ais of size a×nandBis of\\nsizen×b, i.e., the width of Aequals the height of B. The result is a matrix of size\\na×bwhose elements are calculated using the formula\\nAB[i,j]=n∑\\nk=1(A[i,k]·B[k,j]).', metadata={'source': 'documents\\\\doc.pdf', 'page': 168}),\n",
       " Document(page_content='166 11 Mathematics\\nFig. 11.15 Intuition behind\\nthe matrix multiplication\\nformula\\nAA BB\\nThe idea is that each element of AB is a sum of products of elements of AandB\\naccording to Fig. 11.15 . For example,\\n⎡\\n⎣14\\n39\\n86⎤\\n⎦·[16\\n29]\\n=⎡\\n⎣1·1+4·21·6+4·9\\n3·1+9·23·6+9·9\\n8·1+6·28·6+6·9⎤\\n⎦=⎡\\n⎣94 2\\n21 99\\n20 102⎤\\n⎦.\\nWe can directly use the above formula to calculate the product Cof two n×n\\nmatrices AandBinO(n3)time1:\\nfor (int i = 1; i <= n; i++) {\\nfor (int j=1 ;j< =n ;j + + ){\\nfor (int k = 1; k <= n; k++) {\\nC[i][j] += A[i][k]*B[k][j];\\n}\\n}\\n}\\nMatrix multiplication is associative, so A(BC)=(AB)Cholds, but it is not\\ncommutative, so usually AB̸=BA.\\nAnidentity matrix is a square matrix where each element on the diagonal is 1\\nand all other elements are 0. For example, the following matrix is the 3 ×3 identity\\nmatrix:\\nI=⎡\\n⎣100\\n010\\n001⎤\\n⎦\\nMultiplying a matrix by an identity matrix does not change it. For example,\\n⎡\\n⎣100\\n010\\n001⎤\\n⎦·⎡\\n⎣14\\n39\\n86⎤\\n⎦=⎡\\n⎣14\\n39\\n86⎤\\n⎦ and⎡\\n⎣14\\n39\\n86⎤\\n⎦·[10\\n01]\\n=⎡\\n⎣14\\n39\\n86⎤\\n⎦.\\n1While the straightforward O(n3)time algorithm is sufﬁcient in competitive programming, there are\\ntheoretically more efﬁcient algorithms. In 1969, Strassen [ 31] discovered the ﬁrst such algorithm,\\nnow called Strassen’s algorithm , whose time complexity is O(n2.81). The best current algorithm,\\nproposed by Le Gall [ 11] in 2014, works in O(n2.37)time.', metadata={'source': 'documents\\\\doc.pdf', 'page': 169}),\n",
       " Document(page_content='11.3 Matrices 167\\nThe power Akof a matrix Ais deﬁned if Ais a square matrix. The deﬁnition is\\nbased on matrix multiplication:\\nAk=A·A·A···A\\ued19\\ued18\\ued17 \\ued1a\\nktimes\\nFor example,\\n[25\\n14]3\\n=[25\\n14]\\n·[25\\n14]\\n·[25\\n14]\\n=[48 165\\n33 114]\\n.\\nIn addition, A0is an identity matrix. For example,\\n[25\\n14]0\\n=[10\\n01]\\n.\\nThe matrix Akcan be efﬁciently calculated in O(n3logk)time using the algorithm\\nin Sect. 11.1.4 . For example,\\n[25\\n14]8\\n=[25\\n14]4\\n·[25\\n14]4\\n.\\n11.3.2 Linear Recurrences\\nAlinear recurrence is a function f(n)whose initial values are f(0),f(1) ,..., f(k−\\n1)and larger values are calculated recursively using the formula\\nf(n)=c1f(n−1)+c2f(n−2)+···+ ckf(n−k),\\nwhere c1,c2,..., ckare constant coefﬁcients.\\nDynamic programming can be used to calculate any value of f(n)inO(kn)time\\nby calculating all values of f(0),f(1) ,..., f(n)one after another. However, as\\nwe will see next, we can also calculate the value of f(n)inO(k3logn)time using\\nmatrix operations. This is an important improvement if kis small and nis large.\\nFibonacci Numbers A simple example of a linear recurrence is the following func-\\ntion that deﬁnes the Fibonacci numbers:\\nf(0)=0\\nf(1)=1\\nf(n)=f(n−1)+f(n−2)\\nIn this case, k=2 and c1=c2=1.', metadata={'source': 'documents\\\\doc.pdf', 'page': 170}),\n",
       " Document(page_content='168 11 Mathematics\\nTo efﬁciently calculate Fibonacci numbers, we represent the Fibonacci formula\\nas a square matrix Xof size 2 ×2, for which the following holds:\\nX·[f(i)\\nf(i+1)]\\n=[f(i+1)\\nf(i+2)]\\nThus, values f(i)and f(i+1)are given as “input” for X, and Xcalculates values\\nf(i+1)and f(i+2)from them. It turns out that such a matrix is\\nX=[01\\n11]\\n.\\nFor example,\\n[01\\n11]\\n·[f(5)\\nf(6)]\\n=[01\\n11]\\n·[5\\n8]\\n=[8\\n13]\\n=[f(6)\\nf(7)]\\n.\\nThus, we can calculate f(n)using the formula\\n[f(n)\\nf(n+1)]\\n=Xn·[f(0)\\nf(1)]\\n=[01\\n11]n\\n·[0\\n1]\\n.\\nThe value of Xncan be calculated in O(logn)time, so the value of f(n)can also\\nbe calculated in O(logn)time.\\nGeneral Case Let us now consider the general case where f(n)is any linear recur-\\nrence. Again, our goal is to construct a matrix Xfor which\\nX·⎡\\n⎢⎢⎢⎣f(i)\\nf(i+1)\\n...\\nf(i+k−1)⎤\\n⎥⎥⎥⎦=⎡\\n⎢⎢⎢⎣f(i+1)\\nf(i+2)\\n...\\nf(i+k)⎤\\n⎥⎥⎥⎦.\\nSuch a matrix is\\nX=⎡\\n⎢⎢⎢⎢⎢⎣01 0 ··· 0\\n00 1 ··· 0\\n...............\\n00 0 ··· 1\\nckck−1ck−2···c1⎤\\n⎥⎥⎥⎥⎥⎦.\\nIn the ﬁrst k−1 rows, each element is 0 except that one element is 1. These rows\\nreplace f(i)with f(i+1),f(i+1)with f(i+2), and so on. Then, the last row\\ncontains the coefﬁcients of the recurrence to calculate the new value f(i+k).', metadata={'source': 'documents\\\\doc.pdf', 'page': 171}),\n",
       " Document(page_content='11.3 Matrices 169\\nFig. 11.16 Example graphs\\nfor matrix operations\\n1\\n42 3\\n5 6(a)\\n1\\n42 3\\n5 64 12 4\\n1 2 3\\n2(b)\\nNow, f(n)can be calculated in O(k3logn)time using the formula\\n⎡\\n⎢⎢⎢⎣f(n)\\nf(n+1)\\n...\\nf(n+k−1)⎤\\n⎥⎥⎥⎦=Xn·⎡\\n⎢⎢⎢⎣f(0)\\nf(1)\\n...\\nf(k−1)⎤\\n⎥⎥⎥⎦.\\n11.3.3 Graphs and Matrices\\nThe powers of adjacency matrices of graphs have interesting properties. When Mis\\nan adjacency matrix of an unweighted graph, the matrix Mngives for each node pair\\n(a,b)the number of paths that begin at node a, end at node b, and contain exactly\\nnedges. It is allowed that a node appears on a path several times.\\nAs an example, consider the graph in Fig. 11.16 a. The adjacency matrix of this\\ngraph is\\nM=⎡\\n⎢⎢⎢⎢⎢⎢⎣000100\\n100011\\n010000\\n010000\\n000000\\n001010⎤\\n⎥⎥⎥⎥⎥⎥⎦.\\nThen, the matrix\\nM4=⎡\\n⎢⎢⎢⎢⎢⎢⎣001110\\n200022\\n020000\\n020000\\n000000\\n001110⎤\\n⎥⎥⎥⎥⎥⎥⎦\\ngives the number of paths that contain exactly 4 edges. For example, M4[2,5]= 2,\\nbecause there are two paths of 4 edges from node 2 to node 5: 2 →1→4→2→5\\nand 2→6→3→2→5.\\nUsing a similar idea in a weighted graph, we can calculate for each node pair\\n(a,b)the shortest length of a path that goes from atoband contains exactly n', metadata={'source': 'documents\\\\doc.pdf', 'page': 172}),\n",
       " Document(page_content='170 11 Mathematics\\nedges. To calculate this, we deﬁne matrix multiplication in a new way, so that we do\\nnot calculate numbers of paths but minimize lengths of paths.\\nAs an example, consider the graph in Fig. 11.16 b. Let us construct an adjacency\\nmatrix where ∞means that an edge does not exist, and other values correspond to\\nedge weights. The matrix is\\nM=⎡\\n⎢⎢⎢⎢⎢⎢⎣∞∞∞ 4∞∞\\n2∞∞∞ 12\\n∞ 4∞∞∞∞\\n∞ 1∞∞∞∞\\n∞∞∞∞∞∞\\n∞∞ 3∞ 2∞⎤\\n⎥⎥⎥⎥⎥⎥⎦.\\nInstead of the formula\\nAB[i,j]=n∑\\nk=1(A[i,k]·B[k,j])\\nwe now use the formula\\nAB[i,j]=n\\nmin\\nk=1(A[i,k]+B[k,j])\\nfor matrix multiplication, so we calculate minima instead of sums, and sums of\\nelements instead of products. After this modiﬁcation, matrix powers minimize path\\nlengths in the graph. For example, as\\nM4=⎡\\n⎢⎢⎢⎢⎢⎢⎣∞∞ 10 11 9 ∞\\n9∞∞∞ 89\\n∞ 11∞∞∞∞\\n∞ 8∞∞∞∞\\n∞∞∞∞∞∞\\n∞∞ 12 13 11 ∞⎤\\n⎥⎥⎥⎥⎥⎥⎦,\\nwe can conclude that the minimum length of a path of 4 edges from node 2 to node\\n5 is 8. Such a path is 2 →1→4→2→5.\\n11.3.4 Gaussian Elimination\\nGaussian elimination is a systematic way to solve a group of linear equations. The\\nidea is to represent the equations as a matrix and then apply a sequence of sim-\\nple matrix row operations that both preserve the information of the equations and\\ndetermine a value for each variable.', metadata={'source': 'documents\\\\doc.pdf', 'page': 173}),\n",
       " Document(page_content='11.3 Matrices 171\\nSuppose that we are given a group of nlinear equations, each of which contains\\nnvariables:\\na1,1x1+a1,2x2+···+ a1,nxn=b1\\na2,1x1+a2,2x2+···+ a2,nxn=b2\\n···\\nan,1x1+an,2x2+···+ an,nxn=bn\\nWe represent the equations as a matrix as follows:\\n⎡\\n⎢⎢⎢⎣a1,1a1,2···a1,nb1\\na2,1a2,2···a2,nb2\\n...............\\nan,1an,2···an,nbn⎤\\n⎥⎥⎥⎦\\nTo solve the equations, we want to transform the matrix to\\n⎡\\n⎢⎢⎢⎣10 ··· 0c1\\n01 ··· 0c2\\n...............\\n00 ··· 1cn⎤\\n⎥⎥⎥⎦,\\nwhich tells us that the solution is x1=c1,x2=c2,..., xn=cn. To do this, we use\\nthree types of matrix row operations:\\n1. Swap the values of two rows.\\n2. Multiply each value in a row by a nonnegative constant.\\n3. Add a row, multiplied by a constant, to another row.\\nEach above operation preserves the information of the equations, which guarantees\\nthat the ﬁnal solution agrees with the original equations. We can systematically\\nprocess each matrix column so that the resulting algorithm works in O(n3)time.\\nAs an example, consider the following group of equations:\\n2x1+4x2+x3=16\\nx1+2x2+5x3=17\\n3x1+x2+x3= 8\\nIn this case the matrix is as follows:\\n⎡\\n⎣2411 6\\n1251 7\\n311 8⎤\\n⎦', metadata={'source': 'documents\\\\doc.pdf', 'page': 174}),\n",
       " Document(page_content='172 11 Mathematics\\nWe process the matrix column by column. At each step, we make sure that the\\ncurrent column has a one in the correct position and all other values are zeros. To\\nprocess the ﬁrst column, we ﬁrst multiply the ﬁrst row by1\\n2:\\n⎡\\n⎣121\\n28\\n1251 7\\n311 8⎤\\n⎦\\nThen we add the ﬁrst row to the second row (multiplied by −1) and the ﬁrst row to\\nthe third row (multiplied by −3):\\n⎡\\n⎢⎣121\\n28\\n009\\n29\\n0−5−1\\n2−16⎤\\n⎥⎦\\nAfter this, we process the second column. Since the second value in the second\\nrow is zero, we ﬁrst swap the second and third row:\\n⎡\\n⎢⎣121\\n28\\n0−5−1\\n2−16\\n009\\n29⎤\\n⎥⎦\\nThen we multiply the second row by −1\\n5and add it to the ﬁrst row (multiplied by\\n−2):\\n⎡\\n⎢⎢⎣103\\n108\\n5\\n011\\n1016\\n5\\n009\\n29⎤\\n⎥⎥⎦\\nFinally, we process the third column by ﬁrst multiplying it by2\\n9and then adding\\nit to the ﬁrst row (multiplied by −3\\n10) and to the second row (multiplied by −1\\n10):\\n⎡\\n⎣1001\\n0103\\n0012⎤\\n⎦\\nNow the last column of the matrix tells us that the solution to the original group\\nof equations is x1=1,x2=3,x3=2.\\nNote that Gaussian elimination only works if the group of equations has a unique\\nsolution. For example, the group\\nx1+x2=2\\n2x1+2x2=4', metadata={'source': 'documents\\\\doc.pdf', 'page': 175}),\n",
       " Document(page_content='11.3 Matrices 173\\nhas an inﬁnite number of solutions, because both the equations contain the same\\ninformation. On the other hand, the group\\nx1+x2=5\\nx1+x2=7\\ncannot be solved, because the equations are contradictory. If there is no unique\\nsolution, we will notice this during the algorithm, because at some point we will not\\nbe able to successfully process a column.\\n11.4 Probability\\nAprobability is a real number between 0 and 1 that indicates how probable an event\\nis. If an event is certain to happen, its probability is 1, and if an event is impossible,\\nits probability is 0. The probability of an event is denoted P(···)where the three\\ndots describe the event. For example, when throwing a dice, there are six possible\\noutcomes 1 ,2,..., 6, and P(“the outcome is even”) =1/2.\\nTo calculate the probability of an event, we can either use combinatorics or sim-\\nulate the process that generates the event. As an example, consider an experiment\\nwhere we draw the three top cards from a shufﬂed deck of cards.2What is the\\nprobability that each card has the same value (e.g., ♠8,♣8, and ♦8)?\\nOne way to calculate the probability is to use the formula\\nnumber of desired outcomes\\ntotal number of outcomes.\\nIn our example, the desired outcomes are those in which the value of each card is\\nthe same. There are 13(4\\n3)\\nsuch outcomes, because there are 13 possibilities for the\\nvalue of the cards and(4\\n3)\\nways to choose 3 suits from 4 possible suits. Then, there\\nare a total of(52\\n3)\\noutcomes, because we choose 3 cards from 52 cards. Thus, the\\nprobability of the event is\\n13(4\\n3)\\n(52\\n3)=1\\n425.\\nAnother way to calculate the probability is to simulate the process that generates\\nthe event. In our example, we draw three cards, so the process consists of three steps.\\nWe require that each step of the process is successful.\\nDrawing the ﬁrst card certainly succeeds, because any card is ﬁne. The second\\nstep succeeds with probability 3 /51, because there are 51 cards left and 3 of them\\n2A deck of cards consists of 52 cards. Each card has a suit (spade ♠, diamond ♦,c l u b♣, or heart\\n♥) and a value (an integer between 1 and 13).', metadata={'source': 'documents\\\\doc.pdf', 'page': 176}),\n",
       " Document(page_content='174 11 Mathematics\\nhave the same value as the ﬁrst card. In a similar way, the third step succeeds with\\nprobability 2 /50. Thus, the probability that the entire process succeeds is\\n1·3\\n51·2\\n50=1\\n425.\\n11.4.1 Working with Events\\nA convenient way to represent events is to use sets. For example, the possible out-\\ncomes when throwing a dice are {1,2,3,4,5,6}, and any subset of this set is an\\nevent. The event “the outcome is even” corresponds to the set {2,4,6}.\\nEach outcome xis assigned a probability p(x), and the probability P(X)of an\\nevent Xcan be calculated using the formula\\nP(X)=∑\\nx∈Xp(x).\\nFor example, when throwing a dice, p(x)=1/6 for each outcome x, so the proba-\\nbility of the event “the outcome is even” is\\np(2)+p(4)+p(6)=1/2.\\nSince the events are represented as sets, we can manipulate them using standard\\nset operations:\\n•Thecomplement ¯Ameans “ Adoes not happen.” For example, when throwing a\\ndice, the complement of A={2,4,6}is¯A={1,3,5}.\\n•Theunion A ∪Bmeans “ AorBhappen.” For example, the union of A={2,5}\\nandB={4,5,6}isA∪B={2,4,5,6}.\\n•Theintersection A ∩Bmeans “ AandBhappen.” For example, the intersection\\nofA={2,5}andB={4,5,6}isA∩B={5}.\\nComplement The probability of ¯Ais calculated using the formula\\nP(¯A)=1−P(A).\\nSometimes, we can solve a problem easily using complements by solving the\\nopposite problem. For example, the probability of getting at least one six when\\nthrowing a dice ten times is\\n1−(5/6)10.\\nHere 5 /6 is the probability that the outcome of a single throw is not six, and (5/6)10\\nis the probability that none of the ten throws is a six. The complement of this is the\\nanswer to the problem.', metadata={'source': 'documents\\\\doc.pdf', 'page': 177}),\n",
       " Document(page_content='11.4 Probability 175\\nUnion The probability of A∪Bis calculated using the formula\\nP(A∪B)=P(A)+P(B)−P(A∩B).\\nFor example, consider the events A=“the outcome is even” and B=“the outcome\\nis less than 4” when throwing a dice. In this case, the event A∪Bmeans “the outcome\\nis even or less than 4,” and its probability is\\nP(A∪B)=P(A)+P(B)−P(A∩B)=1/2+1/2−1/6=5/6.\\nIf the events AandBaredisjoint , i.e., A∩Bis empty, the probability of the event\\nA∪Bis simply\\nP(A∪B)=P(A)+P(B).\\nIntersection The probability of A∩Bcan be calculated using the formula\\nP(A∩B)=P(A)P(B|A),\\nwhere P(B|A)is the conditional probability thatBhappens assuming that we know\\nthatAhappens. For example, using the events of our previous example, P(B|A)=\\n1/3, because we know that the outcome belongs to the set {2,4,6}, and one of the\\noutcomes is less than 4. Thus,\\nP(A∩B)=P(A)P(B|A)=1/2·1/3=1/6.\\nEvents AandBareindependent if\\nP(A|B)=P(A)and P(B|A)=P(B),\\nwhich means that the fact that Bhappens does not change the probability of A, and\\nvice versa. In this case, the probability of the intersection is\\nP(A∩B)=P(A)P(B).\\n11.4.2 Random Variables\\nArandom variable is a value that is generated by a random process. For example,\\nwhen throwing two dice, a possible random variable is\\nX=“the sum of the outcomes” .\\nFor example, if the outcomes are [4,6](meaning that we ﬁrst throw a four and then\\na six), then the value of Xis 10.\\nWe denote by P(X=x)the probability that the value of a random variable X\\nisx. For example, when throwing two dice, P(X=10)=3/36, because the total', metadata={'source': 'documents\\\\doc.pdf', 'page': 178}),\n",
       " Document(page_content='176 11 Mathematics\\nFig. 11.17 Possible ways to\\nplace two balls in four boxes\\nnumber of outcomes is 36 and there are three possible ways to obtain the sum 10:\\n[4,6],[5,5], and[6,4].\\nExpected Values Theexpected value E [X]indicates the average value of a random\\nvariable X. The expected value can be calculated as a sum\\n∑\\nxP(X=x)x,\\nwhere xgoes through all possible values of X.\\nFor example, when throwing a dice, the expected outcome is\\n1/6·1+1/6·2+1/6·3+1/6·4+1/6·5+1/6·6=7/2.\\nA useful property of expected values is linearity . It means that the sum E[X1+\\nX2+···+ Xn]always equals the sum E[X1]+E[X2]+···+ E[Xn]. This holds\\neven if random variables depend on each other. For example, when throwing two\\ndice, the expected sum of their values is\\nE[X1+X2]=E[X1]+E[X2]= 7/2+7/2=7.\\nLet us now consider a problem where nballs are randomly placed in nboxes, and\\nour task is to calculate the expected number of empty boxes. Each ball has an equal\\nprobability to be placed in any of the boxes.\\nFor example, Fig. 11.17 shows the possibilities when n=2. In this case, the\\nexpected number of empty boxes is\\n0+0+1+1\\n4=1\\n2.\\nThen, in the general case, the probability that a single box is empty is\\n(n−1\\nn)n\\n,\\nbecause no ball should be placed in it. Hence, using linearity, the expected number\\nof empty boxes is\\nn·(n−1\\nn)n\\n.', metadata={'source': 'documents\\\\doc.pdf', 'page': 179}),\n",
       " Document(page_content='11.4 Probability 177\\nDistributions Thedistribution of a random variable Xshows the probability of each\\nvalue that Xmay have. The distribution consists of values P(X=x). For example,\\nwhen throwing two dice, the distribution for their sum is:\\nx 23456789 1 0 1 1 1 2\\nP(X=x)1/36 2/36 3/36 4/36 5/36 6/36 5/36 4/36 3/36 2/36 1/36\\nIn a uniform distribution , the random variable Xhasnpossible values a,a+\\n1,..., band the probability of each value is 1 /n. For example, when throwing a\\ndice, a=1,b=6, and P(X=x)=1/6 for each value x.\\nThe expected value of Xin a uniform distribution is\\nE[X]=a+b\\n2.\\nIn abinomial distribution ,nattempts are made and the probability that a single\\nattempt succeeds is p. The random variable Xcounts the number of successful\\nattempts, and the probability of a value xis\\nP(X=x)=px(1−p)n−x(n\\nx)\\n,\\nwhere pxand(1−p)n−xcorrespond to successful and unsuccessful attempts, and(n\\nx)\\nis the number of ways we can choose the order of the attempts.\\nFor example, when throwing a dice ten times, the probability of throwing a six\\nexactly three times is (1/6)3(5/6)7(10\\n3)\\n.\\nThe expected value of Xin a binomial distribution is\\nE[X]=pn.\\nIn ageometric distribution , the probability that an attempt succeeds is p, and we\\ncontinue until the ﬁrst success happens. The random variable Xcounts the number\\nof attempts needed, and the probability of a value xis\\nP(X=x)=(1−p)x−1p,\\nwhere (1−p)x−1corresponds to the unsuccessful attempts and pcorresponds to\\nthe ﬁrst successful attempt.\\nFor example, if we throw a dice until we get a six, the probability that the number\\nof throws is exactly 4 is (5/6)31/6.\\nThe expected value of Xin a geometric distribution is\\nE[X]=1\\np.', metadata={'source': 'documents\\\\doc.pdf', 'page': 180}),\n",
       " Document(page_content='178 11 Mathematics\\nFig. 11.18 AM a r k o vc h a i n\\nfor a building that consists of\\nﬁve ﬂoors 1 2 3 4 51 1/2 1/2 1/2\\n1 1/2 1/2 1/2\\n11.4.3 Markov Chains\\nAMarkov chain is a random process that consists of states and transitions between\\nthem. For each state, we know the probabilities of moving to other states. A Markov\\nchain can be represented as a graph whose nodes correspond to the states and edges\\ndescribe the transitions.\\nAs an example, consider a problem where we are in ﬂoor 1 in an nﬂoor building.\\nAt each step, we randomly walk either one ﬂoor up or one ﬂoor down, except that\\nwe always walk one ﬂoor up from ﬂoor 1 and one ﬂoor down from ﬂoor n. What is\\nthe probability of being in ﬂoor mafter ksteps?\\nIn this problem, each ﬂoor of the building corresponds to a state in a Markov\\nchain. For example, Fig. 11.18 shows the chain when n=5.\\nThe probability distribution of a Markov chain is a vector [p1,p2,..., pn], where\\npkis the probability that the current state is k. The formula p1+p2+···+ pn=1\\nalways holds.\\nIn the above scenario, the initial distribution is [1,0,0,0,0], because we always\\nbegin in ﬂoor 1. The next distribution is [0,1,0,0,0], because we can only move\\nfrom ﬂoor 1 to ﬂoor 2. After this, we can either move one ﬂoor up or one ﬂoor down,\\nso the next distribution is [1/2,0,1/2,0,0], and so on.\\nAn efﬁcient way to simulate the walk in a Markov chain is to use dynamic pro-\\ngramming. The idea is to maintain the probability distribution, and at each step go\\nthrough all possibilities how we can move. Using this method, we can simulate a\\nwalk of msteps in O(n2m)time.\\nThe transitions of a Markov chain can also be represented as a matrix that updates\\nthe probability distribution. In the above scenario, the matrix is\\n⎡\\n⎢⎢⎢⎢⎣01/20 00\\n101 /200\\n01/201 /20\\n001 /201\\n00 01 /20⎤\\n⎥⎥⎥⎥⎦.\\nWhen we multiply a probability distribution by this matrix, we get the new dis-\\ntribution after moving one step. For example, we can move from the distribution', metadata={'source': 'documents\\\\doc.pdf', 'page': 181}),\n",
       " Document(page_content='11.4 Probability 179\\n[1,0,0,0,0]to the distribution [0,1,0,0,0]as follows:\\n⎡\\n⎢⎢⎢⎢⎣01/20 00\\n101 /200\\n01/201 /20\\n001 /201\\n00 01 /20⎤\\n⎥⎥⎥⎥⎦⎡\\n⎢⎢⎢⎢⎣1\\n0\\n0\\n0\\n0⎤\\n⎥⎥⎥⎥⎦=⎡\\n⎢⎢⎢⎢⎣0\\n1\\n0\\n0\\n0⎤\\n⎥⎥⎥⎥⎦.\\nBy calculating matrix powers efﬁciently, we can calculate the distribution after m\\nsteps in O(n3logm)time.\\n11.4.4 Randomized Algorithms\\nSometimes we can use randomness for solving a problem, even if the problem is\\nnot related to probabilities. A randomized algorithm is an algorithm that is based on\\nrandomness. There are two popular types of randomized algorithms:\\n•AMonte Carlo algorithm is an algorithm that may sometimes give a wrong\\nanswer. For such an algorithm to be useful, the probability of a wrong answer\\nshould be small.\\n•ALas Vegas algorithm is an algorithm that always gives the correct answer, but its\\nrunning time varies randomly. The goal is to design an algorithm that is efﬁcient\\nwith high probability.\\nNext we will go through three example problems that can be solved using such\\nalgorithms.\\nOrder Statistics The kth order statistic of an array is the element at position k\\nafter sorting the array in increasing order. It is easy to calculate any order statistic\\ninO(nlogn)time by ﬁrst sorting the array, but is it really needed to sort the entire\\narray just to ﬁnd one element?\\nIt turns out that we can ﬁnd order statistics using a Las V egas algorithm, whose\\nexpected running time is O(n). The algorithm chooses a random element xfrom\\nthe array and moves elements smaller than xto the left part of the array, and all\\nother elements to the right part of the array. This takes O(n)time when there are n\\nelements.\\nAssume that the left part contains aelements and the right part contains belements.\\nIfa=k, element xis the kth order statistic. Otherwise, if a>k, we recursively ﬁnd\\nthekth order statistic for the left part, and if a<k, we recursively ﬁnd the rth order\\nstatistic for the right part where r=k−a−1. The search continues in a similar\\nway, until the desired element has been found.\\nWhen each element xis randomly chosen, the size of the array about halves at\\neach step, so the time complexity for ﬁnding the kth order statistic is about\\nn+n/2+n/4+n/8+···= O(n).', metadata={'source': 'documents\\\\doc.pdf', 'page': 182}),\n",
       " Document(page_content='180 11 Mathematics\\nFig. 11.19 A valid coloring\\nof a graph1 2\\n3 45\\nNote that the worst case of the algorithm requires O(n2)time, because it is possible\\nthatxis always chosen in such a way that it is one of the smallest or largest elements\\nin the array and O(n)steps are needed. However, the probability of this is so small\\nthat we may assume that this never happens in practice.\\nVerifying Matrix Multiplication Given matrices A,B, and C, each of size n×n,\\nour next problem is to verify ifAB=Cholds. Of course, we can solve the problem\\nby just calculating the product AB inO(n3)time, but one could hope that verifying\\nthe answer would be easier than to calculate it from scratch.\\nIt turns out that we can solve the problem using a Monte Carlo algorithm whose\\ntime complexity is only O(n2). The idea is simple: we choose a random vector Xof\\nnelements and calculate the matrices ABX andCX .I fABX=CX , we report that\\nAB=C, and otherwise we report that AB̸=C.\\nThe time complexity of the algorithm is O(n2), because we can calculate the\\nmatrices ABX andCX inO(n2)time. We can calculate the matrix ABX efﬁciently\\nby using the representation A(BX), so only two multiplications of n×nandn×1\\nsize matrices are needed.\\nThe drawback of the algorithm is that there is a small chance that the algorithm\\nmakes a mistake when it reports that AB=C. For example,\\n[68\\n13]\\n̸=[87\\n32]\\n,\\nbut[68\\n13][3\\n6]\\n=[87\\n32][3\\n6]\\n.\\nHowever, in practice, the probability that the algorithm makes a mistake is small,\\nand we can decrease the probability by verifying the result using multiple random\\nvectors Xbefore reporting that AB=C.\\nGraph Coloring Given a graph that contains nnodes and medges, our ﬁnal problem\\nis to ﬁnd a way to color the nodes using two colors so that for at least m/2 edges,\\nthe endpoints have different colors. For example, Fig. 11.19 shows a valid coloring\\nof a graph. In this case the graph contains seven edges, and the endpoints of ﬁve of\\nthem have different colors in the coloring.\\nThe problem can be solved using a Las V egas algorithm that generates random\\ncolorings until a valid coloring has been found. In a random coloring, the color of\\neach node is independently chosen so that the probability of both colors is 1 /2.\\nHence, the expected number of edges whose endpoints have different colors is m/2.', metadata={'source': 'documents\\\\doc.pdf', 'page': 183}),\n",
       " Document(page_content='11.4 Probability 181\\nSince it is expected that a random coloring is valid, we will quickly ﬁnd a valid\\ncoloring in practice.\\n11.5 Game Theory\\nIn this section, we focus on two-player games where the players move alternately\\nand have the same set of moves available, and there are no random elements. Our\\ngoal is to ﬁnd a strategy that we can follow to win the game no matter what the\\nopponent does, if such a strategy exists.\\nIt turns out that there is a general strategy for such games, and we can analyze the\\ngames using nim theory . First, we will analyze simple games where players remove\\nsticks from heaps, and after this, we will generalize the strategy used in those games\\nto other games.\\n11.5.1 Game States\\nLet us consider a game that starts with a heap of nsticks. Two players move alter-\\nnately, and on each move, the player has to remove 1, 2, or 3 sticks from the heap.\\nFinally, the player who removes the last stick wins the game.\\nFor example, if n=10, the game may proceed as follows:\\n•Player Aremoves 2 sticks (8 sticks left).\\n•Player Bremoves 3 sticks (5 sticks left).\\n•Player Aremoves 1 stick (4 sticks left).\\n•Player Bremoves 2 sticks (2 sticks left).\\n•Player Aremoves 2 sticks and wins.\\nThis game consists of states 0 ,1,2,..., n, where the number of the state corre-\\nsponds to the number of sticks left.\\nAwinning state is a state where the player will win the game if they play optimally,\\nand a losing state is a state where the player will lose the game if the opponent plays\\noptimally. It turns out that we can classify all states of a game so that each state is\\neither a winning state or a losing state.\\nIn the above game, state 0 is clearly a losing state, because the player cannot make\\nany moves. States 1, 2, and 3 are winning states, because the player can remove 1,\\n2, or 3 sticks and win the game. State 4, in turn, is a losing state, because any move\\nleads to a state that is a winning state for the opponent.\\nMore generally, if there is a move that leads from the current state to a losing state,\\nit is a winning state, and otherwise it is a losing state. Using this observation, we can\\nclassify all states of a game starting with losing states where there are no possible\\nmoves. Figure 11.20 shows the classiﬁcation of states 0 ...15 (Wdenotes a winning\\nstate and Ldenotes a losing state).', metadata={'source': 'documents\\\\doc.pdf', 'page': 184}),\n",
       " Document(page_content='182 11 Mathematics\\nLWWW LWWW LWWW LWWW0 12 3 4 56 7 8 910 11 12 13 14 15\\nFig. 11.20 Classiﬁcation of states 0 ...15 in the stick game\\nFig. 11.21 State graph of\\nthe divisibility game1 2\\n3\\n4\\n5\\n67\\n8\\n9\\nFig. 11.22 Classiﬁcation of\\nstates 1 ...9i nt h e\\ndivisibility gameLW LW LW LW L12 3 4 56 7 8 9\\nIt is easy to analyze this game: a state kis a losing state if kis divisible by 4, and\\notherwise it is a winning state. An optimal way to play the game is to always choose\\na move after which the number of sticks in the heap is divisible by 4. Finally, there\\nare no sticks left and the opponent has lost. Of course, this strategy requires that the\\nnumber of sticks is not divisible by 4 when it is our move. If it is, there is nothing\\nwe can do, and the opponent will win the game if they play optimally.\\nLet us then consider another stick game, where in each state k, it is allowed to\\nremove any number xof sticks such that xis smaller than kand divides k.F o r\\nexample, in state 8 we may remove 1, 2, or 4 sticks, but in state 7 the only allowed\\nmove is to remove 1 stick. Figure 11.21 shows the states 1 ...9 of the game as a state\\ngraph , whose nodes are the states and edges are the moves between them:\\nThe ﬁnal state in this game is always state 1, which is a losing state, because there\\nare no valid moves. Figure 11.22 shows the classiﬁcation of states 1 ...9. It turns out\\nthat in this game, all even-numbered states are winning states, and all odd-numbered\\nstates are losing states.\\n11.5.2 Nim Game\\nThenim game is a simple game that has an important role in game theory, because\\nmany other games can be played using the same strategy. First, we focus on nim,\\nand after this, we generalize the strategy to other games.\\nThere are nheaps in nim, and each heap contains some number of sticks. The\\nplayers move alternately, and on each turn, the player chooses a heap that still contains\\nsticks and removes any number of sticks from it. The winner is the player who\\nremoves the last stick.', metadata={'source': 'documents\\\\doc.pdf', 'page': 185}),\n",
       " Document(page_content='11.5 Game Theory 183\\nThe states in nim are of the form [x1,x2,..., xn], where xidenotes the number\\nof sticks in heap i. For example, [10,12,5]is a state where there are three heaps\\nwith 10, 12, and 5 sticks. The state [0,0,..., 0]is a losing state, because it is not\\npossible to remove any sticks, and this is always the ﬁnal state.\\nAnalysis It turns out that we can easily classify any nim state by calculating the nim\\nsum s =x1⊕x2⊕···⊕ xn, where ⊕denotes the xor operation. The states whose\\nnim sum is 0 are losing states, and all other states are winning states. For example,\\nthe nim sum of [10,12,5]is 10⊕12⊕5=3, so the state is a winning state.\\nBut how is the nim sum related to the nim game? We can explain this by looking\\nat how the nim sum changes when the nim state changes.\\nLosing states: The ﬁnal state [0,0,..., 0]is a losing state, and its nim sum is 0, as\\nexpected. In other losing states, any move leads to a winning state, because when a\\nsingle value xichanges, the nim sum also changes, so the nim sum is different from\\n0 after the move.\\nWinning states: We can move to a losing state if there is any heap ifor which\\nxi⊕s<xi. In this case, we can remove sticks from heap iso that it will contain\\nxi⊕ssticks, which will lead to a losing state. There is always such a heap, where\\nxihas a one bit at the position of the leftmost one bit of s.\\nExample As an example, consider the state [10,12,5]. This state is a winning state,\\nbecause its nim sum is 3. Thus, there has to be a move which leads to a losing state.\\nNext we will ﬁnd out such a move.\\nThe nim sum of the state is as follows:\\n101010\\n121100\\n50101\\n30011\\nIn this case, the heap with 10 sticks is the only heap that has a one bit at the\\nposition of the leftmost one bit of the nim sum:\\n101010\\n121100\\n50101\\n30011\\nThe new size of the heap has to be 10 ⊕3=9, so we will remove just one stick.\\nAfter this, the state will be [9,12,5], which is a losing state:\\n91001\\n121100\\n50101\\n00000\\nMisère Game In amisère nim game, the goal of the game is opposite, so the player\\nwho removes the last stick loses the game. It turns out that the misère nim game can\\nbe optimally played almost like the standard nim game.', metadata={'source': 'documents\\\\doc.pdf', 'page': 186}),\n",
       " Document(page_content='184 11 Mathematics\\nFig. 11.23 Grundy numbers\\nof game states0 1 0\\n2 0 2\\nThe idea is to ﬁrst play the misère game like the standard game, but change the\\nstrategy at the end of the game. The new strategy will be introduced in a situation\\nwhere each heap would contain at most one stick after the next move. In the standard\\ngame, we should choose a move after which there is an even number of heaps with\\none stick. However, in the misère game, we choose a move so that there is an odd\\nnumber of heaps with one stick.\\nThis strategy works because a state where the strategy changes always appears in\\nthe game, and this state is a winning state, because it contains exactly one heap that\\nhas more than one stick so the nim sum is not 0.\\n11.5.3 Sprague–Grundy Theorem\\nTheSprague–Grundy theorem generalizes the strategy used in nim to all games that\\nfulﬁll the following requirements:\\n•There are two players who move alternately.\\n•The game consists of states, and the possible moves in a state do not depend on\\nwhose turn it is.\\n•The game ends when a player cannot make a move.\\n•The game surely ends sooner or later.\\n•The players have complete information about the states and allowed moves, and\\nthere is no randomness in the game.\\nGrundy Numbers The idea is to calculate for each game state a Grundy number\\nthat corresponds to the number of sticks in a nim heap. When we know the Grundy\\nnumbers of all states, we can play the game like the nim game.\\nThe Grundy number of a game state is calculated using the formula\\nmex({g1,g2,..., gn}),\\nwhere g1,g2,..., gnare the Grundy numbers of the states to which we can move\\nfrom the state, and the mex function gives the smallest nonnegative number that is\\nnot in the set. For example, mex ({0,1,3})=2. If a state has no possible moves, its\\nGrundy number is 0, because mex (∅)=0.\\nFor example, Fig. 11.23 shows a state graph of a game where each state is assigned\\nits Grundy number. The Grundy number of a losing state is 0, and the Grundy number\\nof a winning state is a positive number.', metadata={'source': 'documents\\\\doc.pdf', 'page': 187}),\n",
       " Document(page_content='11.5 Game Theory 185\\nFig. 11.24 Possible moves\\non the ﬁrst turn\\n@ * * * ***\\nFig. 11.25 Grundy numbers\\nof game states01 01\\n012\\n021 0\\n30 41\\n041 32\\nConsider a state whose Grundy number is x. We can think that it corresponds to\\na nim heap that has xsticks. In particular, if x>0, we can move to states whose\\nGrundy numbers are 0 ,1,..., x−1, which simulates removing sticks from a nim\\nheap. There is one difference, though it may be possible to move to a state whose\\nGrundy number is larger than xand “add” sticks to a heap. However, the opponent\\ncan always cancel any such move, so this does not change the strategy.\\nAs an example, consider a game where the players move a ﬁgure in a maze. Each\\nsquare of the maze is either ﬂoor or wall. On each turn, the player has to move the\\nﬁgure some number of steps left or up. The winner of the game is the player who\\nmakes the last move. Figure 11.24 shows a possible initial conﬁguration of the game,\\nwhere @ denotes the ﬁgure and * denotes a square where it can move. The states of\\nthe game are all ﬂoor squares of the maze. Figure 11.25 shows the Grundy numbers\\nof the states in this conﬁguration.\\nAccording to the Sprague–Grundy theorem, each state of the maze game corre-\\nsponds to a heap in the nim game. For example, the Grundy number of the lower-right\\nsquare is 2, so it is a winning state. We can reach a losing state and win the game by\\nmoving either four steps left or two steps up.\\nSubgames Assume that our game consists of subgames, and on each turn, the player\\nﬁrst chooses a subgame and then a move in the subgame. The game ends when it is\\nnot possible to make any move in any subgame. In this case, the Grundy number of\\na game equals the nim sum of the Grundy numbers of the subgames. The game can\\nthen be played like a nim game by calculating all Grundy numbers for subgames and\\nthen their nim sum.\\nAs an example, consider a game that consists of three mazes. On each turn, the\\nplayer chooses one of the mazes and then moves the ﬁgure in the maze. Figure 11.26\\nshows an initial conﬁguration of the game, and Fig. 11.27 shows the corresponding\\nGrundy numbers. In this conﬁguration, the nim sum of the Grundy numbers is 2 ⊕\\n3⊕3=2, so the ﬁrst player can win the game. One optimal move is to move two\\nsteps up in the ﬁrst maze, which produces the nim sum 0 ⊕3⊕3=0.', metadata={'source': 'documents\\\\doc.pdf', 'page': 188}),\n",
       " Document(page_content='186 11 Mathematics\\nFig. 11.26 Ag a m et h a t\\nconsists of three subgames\\n@ @ @\\nFig. 11.27 Grundy numbers\\nin subgames01 01\\n012\\n021 0\\n30 41\\n041 32012 3\\n100 1\\n2 012\\n3 12 0\\n40253012 34\\n1 0\\n21\\n3 2\\n4012 3\\nGrundy’s Game Sometimes a move in a game divides the game into subgames that\\nare independent of each other. In this case, the Grundy number of a game state is\\nmex({g1,g2,..., gn}),\\nwhere there are npossible moves and\\ngk=ak,1⊕ak,2⊕...⊕ak,m,\\nmeaning that move kdivides the game into msubgames whose Grundy numbers are\\nak,1,ak,2,..., ak,m.\\nAn example of such a game is Grundy’s game . Initially, there is a single heap\\nthat has nsticks. On each turn, the player chooses a heap and divides it into two\\nnonempty heaps such that the heaps are of different size. The player who makes the\\nlast move wins the game.\\nLetg(n)denote the Grundy number of a heap of size n. The Grundy number\\ncan be calculated by going through all ways to divide the heap into two heaps. For\\nexample, when n=8, the possibilities are 1 +7, 2+6, and 3 +5, so\\ng(8)=mex({g(1)⊕g(7),g(2)⊕g(6),g(3)⊕g(5)}).\\nIn this game, the value of g(n)is based on the values of g(1) ,..., g(n−1). The\\nbase cases are g(1)=g(2)=0, because it is not possible to divide the heaps of 1\\nand 2 sticks into smaller heaps. The ﬁrst Grundy numbers are:\\ng(1)=0\\ng(2)=0\\ng(3)=1\\ng(4)=0', metadata={'source': 'documents\\\\doc.pdf', 'page': 189}),\n",
       " Document(page_content='11.5 Game Theory 187\\ng(5)=2\\ng(6)=1\\ng(7)=0\\ng(8)=2\\nThe Grundy number for n=8 is 2, so it is possible to win the game. The winning\\nmove is to create heaps 1 +7, because g(1)⊕g(7)=0.', metadata={'source': 'documents\\\\doc.pdf', 'page': 190}),\n",
       " Document(page_content='Advanced Graph Algorithms\\nThis chapter discusses a selection of advanced graph algorithms.\\nSection 12.1 presents an algorithm for ﬁnding the strongly connected components\\nof a graph. After this, we will learn how to efﬁciently solve the 2SA T problem using\\nthe algorithm.\\nSection 12.2 focuses on Eulerian and Hamiltonian paths. An Eulerian path goes\\nthrough each edge of the graph exactly once, and a Hamiltonian path visits each node\\nexactly once. While the concepts look quite similar at ﬁrst glance, the computational\\nproblems related to them are very different.\\nSection 12.3 ﬁrst shows how we can determine the maximum ﬂow from a source\\nto a sink in a graph. After this, we will see how to reduce several other graph problems\\nto the maximum ﬂow problem.\\nSection 12.4 discusses properties of depth-ﬁrst search and problems related to\\nbiconnected graphs.\\n12.1 Strong Connectivity\\nA directed graph is called strongly connected if there is a path from any node to all\\nother nodes in the graph. For example, the left graph in Fig. 12.1 is strongly connected\\nwhile the right graph is not. The right graph is not strongly connected, because, for\\nexample, there is no path from node 2 to node 1.\\nA directed graph can always be divided into strongly connected components. Each\\nsuch component contains a maximal set of nodes such that there is a path from any\\nnode to all other nodes, and the components form an acyclic component graph that\\nrepresents the deep structure of the original graph. For example, Fig. 12.2 shows a\\ngraph, its strongly connected components and the corresponding component graph.\\nThe components are A={1,2},B={3,6,7},C={4}, and D={5}.', metadata={'source': 'documents\\\\doc.pdf', 'page': 191}),\n",
       " Document(page_content='190 12 Advanced Graph Algorithms\\nFig. 12.1 The left graph is\\nstrongly connected, the right\\ngraph is not1 2\\n3 41 2\\n3 4\\nFig. 12.2 A graph, its\\nstrongly connected\\ncomponents and the\\ncomponent graph73 2 1\\n6 5 4\\n73 2 1\\n6 5 4\\nBA\\nD C\\nA component graph is a directed acyclic graph, so it is easier to process than the\\noriginal graph. Since the graph does not contain cycles, we can always construct a\\ntopological sort and use dynamic programming to process it.\\n12.1.1 Kosaraju’s Algorithm\\nKosaraju’s algorithm is an efﬁcient method for ﬁnding the strongly connected com-\\nponents of a graph. The algorithm performs two depth-ﬁrst searches: the ﬁrst search\\nconstructs a list of nodes according to the structure of the graph, and the second\\nsearch forms the strongly connected components.\\nThe ﬁrst phase of Kosaraju’s algorithm constructs a list of nodes in the order in\\nwhich depth-ﬁrst search processes them. The algorithm goes through the nodes and\\nbegins a depth-ﬁrst search at each unprocessed node. Each node will be added to the\\nlist after it has been processed.\\nFor example, Fig. 12.3 shows the processing order of the nodes in our example\\ngraph. The notation x/ymeans that processing the node started at time xand ﬁnished\\nat time y. The resulting list is [4,5,2,1,6,7,3]\\nThe second phase of Kosaraju’s algorithm forms the strongly connected compo-\\nnents. First, the algorithm reverses every edge of the graph. This guarantees that\\nduring the second search, we will always ﬁnd valid strongly connected components.\\nFigure 12.4 shows the graph in our example after reversing the edges.', metadata={'source': 'documents\\\\doc.pdf', 'page': 192}),\n",
       " Document(page_content='12.1 Strong Connectivity 191\\n73 2 1\\n6 5 41/82 /79 /14\\n4/53 /61 1 /1210/13\\nFig. 12.3 The processing order of the nodes\\nFig. 12.4 Ag r a p hw i t h\\nreversed edges\\n73 2 1\\n6 5 4\\n73 2 1\\n6 5 4\\nstep 173 2 1\\n6 5 4\\nstep 2\\n73 2 1\\n6 5 4\\nstep 373 2 1\\n6 5 4\\nstep 4\\nFig. 12.5 Constructing the strongly connected components\\nAfter this, the algorithm goes through the list of nodes created by the ﬁrst search, in\\nreverse order. If a node does not belong to a component, the algorithm creates a new\\ncomponent by starting a depth-ﬁrst search that adds all new nodes found during the\\nsearch to the new component. Note that since all edges are reversed, the components\\ndo not “leak” to other parts of the graph.\\nFigure 12.5 shows how the algorithm processes our example graph. The process-\\ning order of the nodes is [3,7,6,1,2,5,4]. First, node 3 generates the component\\n{3,6,7}. Then, nodes 7 and 6 are skipped, because they already belong to a com-\\nponent. After this, node 1 generates the component {1,2}, and node 2 is skipped.\\nFinally, nodes 5 and 4 generate the components {5}and{4}.\\nThe time complexity of the algorithm is O(n+m), because the algorithm performs\\ntwo depth-ﬁrst searches.', metadata={'source': 'documents\\\\doc.pdf', 'page': 193}),\n",
       " Document(page_content='192 12 Advanced Graph Algorithms\\n12.1.2 2SAT Problem\\nIn the 2SAT problem , we are given a logical formula\\n(a1∨b1)∧(a2∨b2)∧···∧ (am∨bm),\\nwhere each aiandbiis either a logical variable ( x1,x2,..., xn) or a negation of\\na logical variable ( ¬x1,¬x2,...,¬xn). The symbols “ ∧” and “ ∨” denote logical\\noperators “and” and “or.” Our task is to assign each variable a value so that the\\nformula is true, or state that this is not possible.\\nFor example, the formula\\nL1=(x2∨¬x1)∧(¬x1∨¬x2)∧(x1∨x3)∧(¬x2∨¬x3)∧(x1∨x4)\\nis true when the variables are assigned as follows:\\n⎧\\n⎪⎪⎪⎨\\n⎪⎪⎪⎩x1=false\\nx2=false\\nx3=true\\nx4=true\\nHowever, the formula\\nL2=(x1∨x2)∧(x1∨¬x2)∧(¬x1∨x3)∧(¬x1∨¬x3)\\nis always false, regardless of how we assign the values. The reason for this is that\\nwe cannot choose a value for x1without creating a contradiction. If x1is false, both\\nx2and¬x2should be true which is impossible, and if x1is true, both x3and¬x3\\nshould be true which is also impossible.\\nAn instance of the 2SA T problem can be represented as an implication graph\\nwhose nodes correspond to variables xiand negations ¬xi, and edges determine\\nthe connections between the variables. Each pair (ai∨bi)generates two edges:\\n¬ai→biand¬bi→ai. This means that if aidoes not hold, bimust hold, and\\nvice versa. For example, Fig. 12.6 shows the implication graph of L1, and Fig. 12.7\\nshows the implication graph of L2.\\nThe structure of the implication graph tells us whether it is possible to assign the\\nvalues of the variables so that the formula is true. This can be done exactly when\\nthere are no nodes xiand¬xisuch that both nodes belong to the same strongly\\nFig. 12.6 The implication\\ngraph of L1¬x3 x2\\n¬x4 x1¬x1 x4\\n¬x2 x3', metadata={'source': 'documents\\\\doc.pdf', 'page': 194}),\n",
       " Document(page_content='12.1 Strong Connectivity 193\\nFig. 12.7 The implication\\ngraph of L2\\nx3 x2 ¬x2 ¬x3¬x1\\nx1\\nFig. 12.8 The component\\ngraph of L1A B C D\\nconnected component. If there are such nodes, the graph contains a path from xito\\n¬xiand also a path from ¬xitoxi, so both xiand¬xishould be true which is not\\npossible. For example, the implication graph of L1does not have nodes xiand¬xi\\nsuch that both nodes belong to the same strongly connected component, so there is a\\nsolution. Then, in the implication graph of L2all nodes belong to the same strongly\\nconnected component, so there are no solutions.\\nIf a solution exists, the values for the variables can be found by going through\\nthe nodes of the component graph in a reverse topological sort order. At each step,\\nwe process a component that does not contain edges that lead to an unprocessed\\ncomponent. If the variables in the component have not been assigned values, their\\nvalues will be determined according to the values in the component, and if they\\nalready have values, the values remain unchanged. The process continues until each\\nvariable has been assigned a value.\\nFigure 12.8 shows the component graph of L1. The components are A={ ¬ x4},\\nB={x1,x2,¬x3},C={ ¬ x1,¬x2,x3}, and D={x4}. When constructing the\\nsolution, we ﬁrst process the component Dwhere x4becomes true. After this, we\\nprocess the component Cwhere x1andx2become false and x3becomes true. All\\nvariables have been assigned values, so the remaining components AandBdo not\\nchange the values of the variables.\\nNote that this method works, because the implication graph has a special structure:\\nif there is a path from node xito node xjand from node xjto node ¬xj, then node\\nxinever becomes true. The reason for this is that there is also a path from node ¬xj\\nto node ¬xi, and both xiandxjbecome false.\\nA more difﬁcult problem is the 3SAT problem , where each part of the formula is\\nof the form (ai∨bi∨ci). This problem is NP-hard, so no efﬁcient algorithm for\\nsolving the problem is known.\\n12.2 Complete Paths\\nIn this section we discuss two special types of paths in graphs: an Eulerian path is a\\npath that goes through each edge exactly once, and a Hamiltonian path is a path that\\nvisits each node exactly once. While such paths look quite similar at ﬁrst glance, the\\ncomputational problems related to them are very different.', metadata={'source': 'documents\\\\doc.pdf', 'page': 195}),\n",
       " Document(page_content='194 12 Advanced Graph Algorithms\\nFig. 12.9 Ag r a p ha n da n\\nEulerian path 1 2\\n3\\n4 51 2\\n3\\n4 51.\\n2.\\n3.4.5.\\n6.\\nFig. 12.10 Ag r a p ha n da n\\nEulerian circuit1 2\\n3\\n4 51 2\\n3\\n4 51.2.3.\\n4.5.6.\\n12.2.1 Eulerian Paths\\nAnEulerian path is a path that goes exactly once through each edge of a graph.\\nFurthermore, if such a path starts and ends at the same node, it is called an Eulerian\\ncircuit . Figure 12.9 shows an Eulerian path from node 2 to node 5, and Fig. 12.10\\nshows an Eulerian circuit that starts and ends at node 1.\\nThe existence of Eulerian paths and circuits depends on the degrees of the nodes.\\nFirst, an undirected graph has an Eulerian path exactly when all the edges belong to\\nthe same connected component and\\n•the degree of each node is even, or\\n•the degree of exactly two nodes is odd, and the degree of all other nodes is even.\\nIn the ﬁrst case, each Eulerian path is also an Eulerian circuit. In the second case,\\nthe odd-degree nodes are the endpoints of an Eulerian path, which is not an Eulerian\\ncircuit. In Fig. 12.9 , nodes 1, 3, and 4 have degree 2, and nodes 2 and 5 have degree\\n3. Exactly two nodes have an odd degree, so there is an Eulerian path between nodes\\n2 and 5, but the graph does not have an Eulerian circuit. In Fig. 12.10 , all nodes have\\nan even degree, so the graph has an Eulerian circuit.\\nTo determine whether a directed graph has Eulerian paths, we focus on indegrees\\nand outdegrees of the nodes. A directed graph contains an Eulerian path exactly\\nwhen all the edges belong to the same strongly connected component and\\n•in each node, the indegree equals the outdegree, or\\n•in one node, the indegree is one larger than the outdegree, in another node, the\\noutdegree is one larger than the indegree, and in all other nodes, the indegree\\nequals the outdegree.\\nIn the ﬁrst case, each Eulerian path is also an Eulerian circuit, and in the second\\ncase, the graph has an Eulerian path that begins at the node whose outdegree is larger\\nand ends at the node whose indegree is larger. For example, in Fig. 12.11 , nodes 1,\\n3, and 4 have both indegree 1 and outdegree 1, node 2 has indegree 1 and outdegree', metadata={'source': 'documents\\\\doc.pdf', 'page': 196}),\n",
       " Document(page_content='12.2 Complete Paths 195\\nFig. 12.11 A directed graph\\nand an Eulerian path 1 2\\n3\\n4 51 2\\n3\\n4 51.\\n2.\\n3.4.5.\\n6.\\n2, and node 5 has indegree 2 and outdegree 1. Hence, the graph contains an Eulerian\\npath from node 2 to node 5.\\nConstruction Hierholzer’s algorithm is an efﬁcient method for constructing an\\nEulerian circuit for a graph. The algorithm consists of several rounds, each of which\\nadds new edges to the circuit. Of course, we assume that the graph contains an\\nEulerian circuit; otherwise Hierholzer’s algorithm cannot ﬁnd it.\\nThe algorithm begins with an empty circuit that contains only a single node and\\nthen extends the circuit step by step by adding subcircuits to it. The process continues\\nuntil all edges have been added to the circuit. The circuit is extended by ﬁnding a\\nnode xthat belongs to the circuit but has an outgoing edge that is not included in the\\ncircuit. Then, a new path from node xthat only contains edges that are not yet in the\\ncircuit is constructed. Sooner or later, the path will return to node x, which creates\\na subcircuit.\\nIf a graph does not have an Eulerian circuit but has an Eulerian path, we can still\\nuse Hierholzer’s algorithm to ﬁnd the path by adding an extra edge to the graph\\nand removing the edge after the circuit has been constructed. For example, in an\\nundirected graph, we add the extra edge between the two odd-degree nodes.\\nAs an example, Fig. 12.12 shows how Hierholzer’s algorithm constructs an\\nEulerian circuit in an undirected graph. First, the algorithm adds a subcircuit\\n1→2→3→1, then a subcircuit 2 →5→6→2, and ﬁnally a subcircuit\\n6→3→4→7→6. After this, since all edges have been added to the circuit, we\\nhave successfully constructed an Eulerian circuit.\\n12.2.2 Hamiltonian Paths\\nAHamiltonian path is a path that visits each node of a graph exactly once. Further-\\nmore, if a such a path begins and ends at the same node, it is called a Hamiltonian\\ncircuit . For example, Fig. 12.13 shows a graph that has both a Hamiltonian path and\\na Hamiltonian circuit.\\nProblems related to Hamiltonian paths are NP-hard: nobody knows a general\\nway to efﬁciently check if a graph has a Hamiltonian path or circuit. Of course, in\\nsome special cases we can be certain that a graph contains a Hamiltonian path. For\\nexample, if the graph is complete, i.e., there is an edge between all pairs of nodes, it\\nsurely contains a Hamiltonian path.\\nA simple way to search for a Hamiltonian path is to use a backtracking algorithm\\nthat goes through all possible ways to construct a path. The time complexity of such', metadata={'source': 'documents\\\\doc.pdf', 'page': 197}),\n",
       " Document(page_content='196 12 Advanced Graph Algorithms\\n1\\n2 3 4\\n5 6 7\\nstep 11\\n2 3 4\\n5 6 71.\\n2.3.\\nstep 2\\n1\\n2 3 4\\n5 6 71.\\n2.\\n3.4.5.6.\\nstep 31\\n2 3 4\\n5 6 71.\\n2.\\n3.4.5.\\n6.\\n7.8.9.10.\\nstep 4\\nFig. 12.12 Hierholzer’s algorithm\\n1 2\\n3\\n4 51 2\\n3\\n4 51.\\n2.3.4. 1 2\\n3\\n4 51.\\n2.\\n3.\\n4.5.\\nFig. 12.13 A graph, a Hamiltonian path and a Hamiltonian circuit\\nan algorithm is at least O(n!), because there are n!different ways to choose the\\norder of nnodes. Then, using dynamic programming, we can create a more efﬁcient\\nO(2nn2)time solution, which determines for each subset of nodes Sand each node\\nx∈Sif there is a path that visits all nodes of Sexactly once and ends at node x.\\n12.2.3 Applications\\nDe Bruijn Sequences ADe Bruijn sequence is a string that contains every string of\\nlength nexactly once as a substring, for a ﬁxed alphabet of kcharacters. The length\\nof such a string is kn+n−1 characters. For example, when n=3 and k=2, an\\nexample of a De Bruijn sequence is\\n0001011100 .\\nThe substrings of this string are all combinations of three bits: 000, 001, 010, 011,\\n100, 101, 110, and 111.', metadata={'source': 'documents\\\\doc.pdf', 'page': 198}),\n",
       " Document(page_content='12.2 Complete Paths 197\\nFig. 12.14 Constructing a\\nDe Bruijn sequence from an\\nEulerian path\\n00 1101\\n101 1\\n0 00 10 1\\nFig. 12.15 An open knight’s\\ntour on a 5 ×5 board14 1 1 16 25\\n12 17 2 510\\n32 0 724 15\\n18 13 22 9 6\\n21 819 14 23\\nA De Bruijn sequence always corresponds to an Eulerian path in a graph where\\neach node contains a string of n−1 characters, and each edge adds one character\\nto the string. For example, the graph in Fig. 12.14 corresponds to the scenario where\\nn=3 and k=2. To create a De Bruijn sequence, we start at an arbitrary node and\\nfollow an Eulerian path that visits each edge exactly once. When the characters in the\\nstarting node and on the edges are added together, the resulting string has kn+n−1\\ncharacters and is a valid De Bruijn sequence.\\nKnight’s Tours Aknight’s tour is a sequence of moves of a knight on an n×n\\nchessboard following the rules of chess such that the knight visits each square exactly\\nonce. A knight’s tour is called closed if the knight ﬁnally returns to the starting square\\nand otherwise it is called open . For example, Fig. 12.15 shows an open knight’s tour\\no na5 ×5 board.\\nA knight’s tour corresponds to a Hamiltonian path in a graph whose nodes repre-\\nsent the squares of the board, and two nodes are connected with an edge if a knight\\ncan move between the squares according to the rules of chess. A natural way to con-\\nstruct a knight’s tour is to use backtracking. Since there is a large number of possible\\nmoves, the search can be made more efﬁcient by using heuristics that attempt to\\nguide the knight so that a complete tour will be found quickly.\\nWarnsdorf’s rule is a simple and effective heuristic for ﬁnding a knight’s tour.\\nUsing the rule, it is possible to efﬁciently construct a tour even on a large board. The\\nidea is to always move the knight so that it ends up in a square where the number of\\npossible follow-up moves is as small as possible. For example, in Fig. 12.16 , there are\\nﬁve possible squares to which the knight can move (squares a...e). In this situation,\\nWarnsdorf’s rule moves the knight to square a, because after this choice, there is\\nonly a single possible move. The other choices would move the knight to squares\\nwhere there would be three moves available.', metadata={'source': 'documents\\\\doc.pdf', 'page': 199}),\n",
       " Document(page_content='198 12 Advanced Graph Algorithms\\nFig. 12.16 Using\\nWarndorf’s rule to construct\\na knight’s tour1\\n2a\\nb e\\nc d\\n12.3 Maximum Flows\\nIn the maximum ﬂow problem, we are given a directed weighted graph that contains\\ntwo special nodes: a source is a node with no incoming edges, and a sink is a node\\nwith no outgoing edges. Our task is to send as much ﬂow as possible from the source\\nto the sink. Each edge has a capacity that restricts the ﬂow that can go through the\\nedge, and in each intermediate node, the incoming and outgoing ﬂow has to be equal.\\nAs an example, consider the graph in Fig. 12.17 , where node 1 is the source and\\nnode 6 is the sink. The maximum ﬂow in this graph is 7, shown in Fig. 12.18 . The\\nnotation v/kmeans that a ﬂow of vunits is routed through an edge whose capacity\\niskunits. The size of the ﬂow is 7, because the source sends 3 +4 units of ﬂow\\nand the sink receives 5 +2 units of ﬂow. It is easy to see that this ﬂow is maximum,\\nbecause the total capacity of the edges leading to the sink is 7.\\nIt turns out that the maximum ﬂow problem is connected to another graph problem,\\ntheminimum cut problem, where our task is to remove a set of edges from the graph\\nsuch that there will be no path from the source to the sink after the removal and the\\ntotal weight of the removed edges is minimum.\\nFor example, consider again the graph in Fig. 12.17 . The minimum cut size is 7,\\nbecause it sufﬁces to remove the edges 2 →3 and 4 →5, as shown in Fig. 12.19 .\\nAfter removing the edges, there will be no path from the source to the sink. The size\\nof the cut is 6 +1=7, and the cut is minimum, because there is no valid cut whose\\nweight would be less than 7.\\nFig. 12.17 A graph with\\nsource 1 and sink 6\\n12 3\\n6\\n4 556\\n5\\n4\\n123 8\\nFig. 12.18 The maximum\\nﬂow of the graph is 7\\n12 3\\n6\\n4 53/56/6\\n5/5\\n4/4\\n1/12/23/3 1/8', metadata={'source': 'documents\\\\doc.pdf', 'page': 200}),\n",
       " Document(page_content='12.3 Maximum Flows 199\\nFig. 12.19 The minimum\\ncut of the graph is 7\\n12 3\\n6\\n4 556\\n5\\n4\\n123 8\\nFig. 12.20 Graph\\nrepresentation in the\\nFord–Fulkerson algorithm\\n12 3\\n6\\n4 55\\n06\\n0 5\\n0\\n4\\n0 1\\n02\\n030 80\\nIt is not a coincidence that the maximum ﬂow and minimum cut are equal in our\\nexample graph. Rather, it turns out that they are always equal, so the concepts are\\ntwo sides of the same coin. Next we will discuss the Ford–Fulkerson algorithm that\\ncan be used to ﬁnd the maximum ﬂow and minimum cut of a graph. The algorithm\\nalso helps us to understand why they are equal.\\n12.3.1 Ford–Fulkerson Algorithm\\nTheFord–Fulkerson algorithm ﬁnds the maximum ﬂow in a graph. The algorithm\\nbegins with an empty ﬂow, and at each step ﬁnds a path from the source to the\\nsink that generates more ﬂow. Finally, when the algorithm cannot increase the ﬂow\\nanymore, the maximum ﬂow has been found.\\nThe algorithm uses a special graph representation where each original edge has\\na reverse edge in another direction. The weight of each edge indicates how much\\nmore ﬂow we could route through it. At the beginning of the algorithm, the weight\\nof each original edge equals the capacity of the edge, and the weight of each reverse\\nedge is zero. Figure 12.20 shows the new representation for our example graph.\\nThe Ford–Fulkerson algorithm consists of several rounds. On each round, the\\nalgorithm ﬁnds a path from the source to the sink such that each edge on the path\\nhas a positive weight. If there is more than one possible path available, any of them\\ncan be chosen. After choosing the path, the ﬂow increases by xunits, where xis the\\nsmallest edge weight on the path. In addition, the weight of each edge on the path\\ndecreases by x, and the weight of each reverse edge increases by x.\\nThe idea is that increasing the ﬂow decreases the amount of ﬂow that can go\\nthrough the edges in the future. On the other hand, it is possible to cancel ﬂow later\\nusing the reverse edges if it turns out that it would be beneﬁcial to route the ﬂow\\nin another way. The algorithm increases the ﬂow as long as there is a path from the\\nsource to the sink through positive-weight edges. Then, if there are no such paths,\\nthe algorithm terminates and the maximum ﬂow has been found.\\nFigure 12.21 shows how the Ford–Fulkerson algorithm ﬁnds the maximum ﬂow\\nfor our example graph. In this case, there are four rounds. On the ﬁrst round, the', metadata={'source': 'documents\\\\doc.pdf', 'page': 201}),\n",
       " Document(page_content='200 12 Advanced Graph Algorithms\\nstep 1\\nstep 2\\nstep 3\\nstep 412 3\\n6\\n4 55\\n06\\n0 5\\n0\\n4\\n0 1\\n02\\n030 80 12 3\\n6\\n4 53\\n24\\n2 5\\n0\\n4\\n0 1\\n00\\n230 62\\n12 3\\n6\\n4 53\\n24\\n2 5\\n0\\n4\\n0 1\\n00\\n230 62 12 3\\n6\\n4 53\\n21\\n5 2\\n3\\n1\\n3 1\\n00\\n203 62\\n12 3\\n6\\n4 53\\n21\\n5 2\\n3\\n1\\n3 1\\n00\\n203 62 12 3\\n6\\n4 53\\n21\\n5 1\\n4\\n0\\n4 0\\n10\\n203 71\\n12 3\\n6\\n4 53\\n21\\n5 1\\n4\\n0\\n4 0\\n10\\n203 71 12 3\\n6\\n4 52\\n30\\n6 0\\n5\\n0\\n4 0\\n10\\n203 71\\nFig. 12.21 The Ford–Fulkerson algorithm\\nalgorithm chooses the path 1 →2→3→5→6. The minimum edge weight on\\nthis path is 2, so the ﬂow increases by 2 units. Then, the algorithm chooses three\\nother paths that increase the ﬂow by 3, 1, and 1 units. After this, there is no path with\\npositive-weight edges, so the maximum ﬂow is 2 +3+1+1=7.\\nFinding Paths The Ford–Fulkerson algorithm does not specify how we should\\nchoose the paths that increase the ﬂow. In any case, the algorithm will terminate\\nsooner or later and correctly ﬁnd the maximum ﬂow. However, the efﬁciency of the\\nalgorithm depends on how the paths are chosen. A simple way to ﬁnd paths is to\\nuse depth-ﬁrst search. Usually this works well, but in the worst case, each path only\\nincreases the ﬂow by one unit, and the algorithm is slow. Fortunately, we can avoid\\nthis situation by using one of the following techniques:\\nTheEdmonds–Karp algorithm chooses each path so that the number of edges on\\nthe path is as small as possible. This can be done by using breadth-ﬁrst search instead', metadata={'source': 'documents\\\\doc.pdf', 'page': 202}),\n",
       " Document(page_content='12.3 Maximum Flows 201\\nFig. 12.22 Nodes 1, 2, and\\n4 belong to the set A\\n12 3\\n6\\n4 52\\n30\\n6 0\\n5\\n0\\n4 0\\n10\\n203 71\\nof depth-ﬁrst search for ﬁnding paths. It can be proved that this guarantees that the\\nﬂow increases quickly, and the time complexity of the algorithm is O(m2n).\\nThecapacity scaling algorithm1uses depth-ﬁrst search to ﬁnd paths where each\\nedge weight is at least an integer threshold value. Initially, the threshold value is\\nsome large number, for example, the sum of all edge weights of the graph. Always\\nwhen a path cannot be found, the threshold value is divided by 2. The algorithm\\nterminates when the threshold value becomes 0. The time complexity of the algorithm\\nisO(m2logc), where cis the initial threshold value.\\nIn practice, the capacity scaling algorithm is easier to implement, because depth-\\nﬁrst search can be used for ﬁnding paths. Both algorithms are efﬁcient enough for\\nproblems that typically appear in programming contests.\\nMinimum Cuts It turns out that once the Ford–Fulkerson algorithm has found a\\nmaximum ﬂow, it has also determined a minimum cut. Consider the graph produced\\nby the algorithm, and let Abe the set of nodes that can be reached from the source\\nusing positive-weight edges. Now the minimum cut consists of the edges of the\\noriginal graph that start at some node in A, end at some node outside A, and whose\\ncapacity is fully used in the maximum ﬂow. For example, in Fig. 12.22 ,Aconsists of\\nnodes 1, 2, and 4, and the minimum cut edges are 2 →3 and 4 →5, whose weight\\nis 6+1=7.\\nWhy is the ﬂow produced by the algorithm maximum and why is the cut minimum?\\nThe reason is that a graph cannot contain a ﬂow whose size is larger than the weight\\nof any cut of the graph. Hence, always when a ﬂow and a cut are equal, they are a\\nmaximum ﬂow and a minimum cut.\\nTo see why the above holds, consider any cut of the graph such that the source\\nbelongs to A, the sink belongs to B, and there are some edges between the sets\\n(Fig. 12.23 ). The size of the cut is the sum of the weights of the edges that go from\\nAtoB. This is an upper bound for the ﬂow in the graph, because the ﬂow has to\\nproceed from AtoB. Thus, the size of a maximum ﬂow is smaller than or equal to\\nthe size of any cut in the graph. On the other hand, the Ford–Fulkerson algorithm\\nproduces a ﬂow whose size is exactly as large as the size of a cut in the graph. Thus,\\nthe ﬂow has to be a maximum ﬂow, and the cut has to be a minimum cut.\\n1This elegant algorithm is not very well known; a detailed description can be found in a textbook\\nby Ahuja, Magnanti, and Orlin [ 1].', metadata={'source': 'documents\\\\doc.pdf', 'page': 203}),\n",
       " Document(page_content='202 12 Advanced Graph Algorithms\\nFig. 12.23 Routing the ﬂow\\nfrom AtoB\\nA B\\nFig. 12.24 Two\\nedge-disjoint paths from\\nnode 1 to node 6 12 3\\n4 56\\n12 3\\n4 56\\nFig. 12.25 A node-disjoint\\npath from node 1 to node 6\\n12 3\\n4 56\\n12.3.2 Disjoint Paths\\nMany graph problems can be solved by reducing them to the maximum ﬂow problem.\\nOur ﬁrst example of such a problem is as follows: we are given a directed graph with\\na source and a sink, and our task is to ﬁnd the maximum number of disjoint paths\\nfrom the source to the sink.\\nEdge-Disjoint Paths We ﬁrst focus on the problem of ﬁnding the maximum number\\nofedge-disjoint paths from the source to the sink. This means that each edge may\\nappear in at most one path. For example, in Fig. 12.24 , the maximum number of\\nedge-disjoint paths is 2 (1 →2→4→3→6 and 1 →4→5→6).\\nIt turns out that the maximum number of edge-disjoint paths always equals the\\nmaximum ﬂow of the graph where the capacity of each edge is one. After the max-\\nimum ﬂow has been constructed, the edge-disjoint paths can be found greedily by\\nfollowing paths from the source to the sink.\\nNode-Disjoint Paths Then, consider the problem of ﬁnding the maximum number\\nofnode-disjoint paths from the source to the sink. In this case, every node, except for\\nthe source and sink, may appear in at most one path, which may reduce the maximum\\nnumber of disjoint paths. Indeed, in our example graph, the maximum number of\\nnode-disjoint paths is 1 (Fig. 12.25 ).\\nWe can reduce also this problem to the maximum ﬂow problem. Since each node\\ncan appear in at most one path, we have to limit the ﬂow that goes through the nodes.\\nA standard construction for this is to divide each node into two nodes such that', metadata={'source': 'documents\\\\doc.pdf', 'page': 204}),\n",
       " Document(page_content='12.3 Maximum Flows 203\\nFig. 12.26 A construction\\nthat limits the ﬂow through\\nthe nodes12 3\\n4 52 3\\n4 56\\nFig. 12.27 Maximum\\nmatching1\\n2\\n3\\n45\\n6\\n7\\n8\\nthe ﬁrst node has the incoming edges of the original node, the second node has the\\noutgoing edges of the original node, and there is a new edge from the ﬁrst node to\\nthe second node. Figure 12.26 shows the resulting graph and its maximum ﬂow in\\nour example.\\n12.3.3 Maximum Matchings\\nAmaximum matching of a graph is a maximum-size set of node pairs where each pair\\nis connected with an edge and each node belongs to at most one pair. While solving\\nthe maximum matching problem in a general graph requires tricky algorithms, the\\nproblem is much easier to solve if we assume that the graph is bipartite. In this case\\nwe can reduce the problem to the maximum ﬂow problem.\\nThe nodes of a bipartite graph can always be divided into two groups such that all\\nedges of the graph go from the left group to the right group. For example, Fig. 12.27\\nshows a maximum matching of a bipartite graph whose left group is {1,2,3,4}and\\nright group is {5,6,7,8}.\\nWe can reduce the bipartite maximum matching problem to the maximum ﬂow\\nproblem by adding two new nodes to the graph: a source and a sink. We also add\\nedges from the source to each left node and from each right node to the sink. After\\nthis, the size of a maximum ﬂow in the resulting graph equals the size of a maximum\\nmatching in the original graph. For example, Fig. 12.28 shows the reduction and the\\nmaximum ﬂow for our example graph.\\nHall’s Theorem Hall’s theorem can be used to ﬁnd out whether a bipartite graph has\\na matching that contains all left or right nodes. If the number of left and right nodes\\nis the same, Hall’s theorem tells us if it is possible to construct a perfect matching\\nthat contains all nodes of the graph.\\nAssume that we want to ﬁnd a matching that contains all left nodes. Let Xbe\\nany set of left nodes and let f(X)be the set of their neighbors. According to Hall’s', metadata={'source': 'documents\\\\doc.pdf', 'page': 205}),\n",
       " Document(page_content='204 12 Advanced Graph Algorithms\\nFig. 12.28 Maximum\\nmatching as a maximum ﬂow1\\n2\\n3\\n45\\n6\\n7\\n8\\nFig. 12.29 X={1,3}and\\nf(X)={5,6,8}1\\n2\\n3\\n45\\n6\\n7\\n8\\nFig. 12.30 X={2,4}and\\nf(X)={7}1\\n2\\n3\\n45\\n6\\n7\\n8\\ntheorem, a matching that contains all left nodes exists exactly when for every possible\\nsetX, the condition |X|≤| f(X)|holds.\\nLet us study Hall’s theorem in our example graph. First, let X={ 1,3}which\\nyields f(X)={5,6,8}(Fig. 12.29 ). The condition of Hall’s theorem holds, because\\n|X|= 2 and|f(X)|= 3. Then, let X={2,4}which yields f(X)={7}(Fig. 12.30 ).\\nIn this case, |X|= 2 and |f(X)|= 1, so the condition of Hall’s theorem does not\\nhold. This means that it is not possible to form a perfect matching for the graph. This\\nresult is not surprising, because we already know that the maximum matching of the\\ngraph is 3 and not 4.\\nIf the condition of Hall’s theorem does not hold, the set Xexplains why we cannot\\nform such a matching. Since Xcontains more nodes than f(X), there are no pairs for\\nall nodes in X. For example, in Fig. 12.30 , both nodes 2 and 4 should be connected\\nwith node 7, which is not possible.\\nK˝onig’s Theorem Aminimum node cover of a graph is a minimum set of nodes\\nsuch that each edge of the graph has at least one endpoint in the set. In a general\\ngraph, ﬁnding a minimum node cover is a NP-hard problem. However, if the graph\\nis bipartite, K˝onig’s theorem tells us that the size of a minimum node cover always\\nequals the size of a maximum matching. Thus, we can calculate the size of a minimum\\nnode cover using a maximum ﬂow algorithm.', metadata={'source': 'documents\\\\doc.pdf', 'page': 206}),\n",
       " Document(page_content='12.3 Maximum Flows 205\\nFig. 12.31 Am i n i m u m\\nnode cover1\\n2\\n3\\n45\\n6\\n7\\n8\\nFig. 12.32 Am a x i m u m\\nindependent set1\\n2\\n3\\n45\\n6\\n7\\n8\\nFig. 12.33 An example\\ngraph for constructing path\\ncovers1 2 3 4\\n5 6 7\\nFor example, since the maximum matching of our example graph is 3, K˝ onig’s\\ntheorem tells us that the size of a minimum node cover is also 3. Figure 12.31 shows\\nhow such a cover can be constructed.\\nThe nodes that do not belong to a minimum node cover form a maximum inde-\\npendent set . This is the largest possible set of nodes such that no two nodes in the set\\nare connected with an edge. Again, ﬁnding a maximum independent set in a general\\ngraph is a NP-hard problem, but in a bipartite graph we can use K˝ onig’s theorem\\nto solve the problem efﬁciently. Figure 12.32 shows a maximum independent set for\\nour example graph.\\n12.3.4 Path Covers\\nApath cover is a set of paths in a graph such that each node of the graph belongs\\nto at least one path. It turns out that in directed acyclic graphs, we can reduce the\\nproblem of ﬁnding a minimum path cover to the problem of ﬁnding a maximum ﬂow\\nin another graph.\\nNode-Disjoint Path Covers In a node-disjoint path cover, each node belongs to\\nexactly one path. As an example, consider the graph in Fig. 12.33 . A minimum\\nnode-disjoint path cover of this graph consists of three paths (Fig. 12.34 ).\\nWe can ﬁnd a minimum node-disjoint path cover by constructing a matching graph\\nwhere each node of the original graph is represented by two nodes: a left node and a', metadata={'source': 'documents\\\\doc.pdf', 'page': 207}),\n",
       " Document(page_content='206 12 Advanced Graph Algorithms\\nFig. 12.34 Am i n i m u m\\nnode-disjoint path cover1 5 6 7\\n2\\n3 4\\nFig. 12.35 A matching\\ngraph for ﬁnding a minimum\\nnode-disjoint path cover1\\n2\\n3\\n4\\n5\\n6\\n71\\n2\\n3\\n4\\n5\\n6\\n7\\nFig. 12.36 Am i n i m u m\\ngeneral path cover1 5 6 3 4\\n2 6 7\\nright node. There is an edge from a left node to a right node if there is such an edge\\nin the original graph. In addition, the matching graph contains a source and a sink,\\nand there are edges from the source to all left nodes and from all right nodes to the\\nsink. Each edge in the maximum matching of the matching graph corresponds to an\\nedge in the minimum node-disjoint path cover of the original graph. Thus, the size\\nof the minimum node-disjoint path cover is n−c, where nis the number of nodes\\nin the original graph, and cis the size of the maximum matching.\\nFor example, Fig. 12.35 shows the matching graph for the graph in Fig. 12.33 .\\nThe maximum matching is 4, so the minimum node-disjoint path cover consists of\\n7−4=3 paths.\\nGeneral Path Covers Ageneral path cover is a path cover where a node can belong to\\nmore than one path. A minimum general path cover may be smaller than a minimum\\nnode-disjoint path cover, because a node can be used multiple times in paths. Consider\\nagain the graph in Fig. 12.33 . The minimum general path cover of this graph consists\\nof two paths (Fig. 12.36 ).\\nA minimum general path cover can be found almost like a minimum node-disjoint\\npath cover. It sufﬁces to add some new edges to the matching graph so that there\\nis an edge a→balways when there is a path from atobin the original graph\\n(possibly through several nodes). Figure 12.37 shows the resulting matching graph\\nfor our example graph.', metadata={'source': 'documents\\\\doc.pdf', 'page': 208}),\n",
       " Document(page_content='12.3 Maximum Flows 207\\nFig. 12.37 A matching\\ngraph for ﬁnding a minimum\\ngeneral path cover1\\n2\\n3\\n4\\n5\\n6\\n71\\n2\\n3\\n4\\n5\\n6\\n7\\nFig. 12.38 Nodes 3 and 7\\nform a maximum antichain1 2 3 4\\n5 6 7\\nDilworth’s Theorem Anantichain is a set of nodes in a graph such that there is\\nno path from any node to another node using the edges of the graph. Dilworth’s\\ntheorem states that in a directed acyclic graph, the size of a minimum general path\\ncover equals the size of a maximum antichain. For example, in Fig. 12.38 , nodes\\n3 and 7 form an antichain of two nodes. This is a maximum antichain, because a\\nminimum general path cover of this graph has two paths (Fig. 12.36 ).\\n12.4 Depth-First Search Trees\\nWhen depth-ﬁrst search processes a connected graph, it also creates a rooted directed\\nspanning tree that can be called a depth-ﬁrst search tree . Then, the edges of the graph\\ncan be classiﬁed according to their roles during the search. In an undirected graph,\\nthere will be two types of edges: tree edges that belong to the depth-ﬁrst search tree\\nandback edges that point to already visited nodes. Note that a back edge always\\npoints to an ancestor of a node.\\nFor example, Fig. 12.39 shows a graph and its depth-ﬁrst search tree. The solid\\nedges are tree edges, and the dashed edges are back edges.\\nIn this section, we will discuss some applications for depth-ﬁrst search trees in\\ngraph processing.\\n12.4.1 Biconnectivity\\nA connected graph is called biconnected if it remains connected after removing\\nany single node (and its edges) from the graph. For example, in Fig. 12.40 , the left', metadata={'source': 'documents\\\\doc.pdf', 'page': 209}),\n",
       " Document(page_content='208 12 Advanced Graph Algorithms\\n1\\n2\\n34 5\\n6 71\\n2\\n34\\n5 6\\n7\\nFig. 12.39 A graph and its depth-ﬁrst search tree\\nFig. 12.40 The left graph is\\nbiconnected, the right graph\\nis not1 2 3\\n4 5 61 2\\n3\\n4 5\\nFig. 12.41 A graph with\\nthree articulation points and\\ntwo bridges 12\\n34 56\\n78\\nFig. 12.42 Finding bridges\\nand articulation points using\\ndepth-ﬁrst search\\n12\\n345\\n6 7\\n8\\ngraph is biconnected, but the right graph is not. The right graph is not biconnected,\\nbecause removing node 3 from the graph disconnects the graph by dividing it into\\ntwo components {1,4}and{2,5}.\\nA node is called an articulation point if removing the node from the graph dis-\\nconnects the graph. Thus, a biconnected graph does not have articulation points.\\nIn a similar way, an edge is called a bridge if removing the edge from the graph\\ndisconnects the graph. For example, in Fig. 12.41 , nodes 4, 5, and 7 are articulation\\npoints, and edges 4–5 and 7–8 are bridges.\\nWe can use depth-ﬁrst search to efﬁciently ﬁnd all articulation points and bridges\\nin a graph. First, to ﬁnd bridges, we begin a depth-ﬁrst search at an arbitrary node,\\nwhich builds a depth-ﬁrst search tree. For example, Fig. 12.42 shows a depth-ﬁrst\\nsearch tree for our example graph.\\nAn edge a→bcorresponds to a bridge exactly when it is a tree edge, and\\nthere is no back edge from the subtree of btoaor any ancestor of a. For example,\\nin Fig. 12.42 , edge 5 →4 is a bridge, because there is no back edge from nodes', metadata={'source': 'documents\\\\doc.pdf', 'page': 210}),\n",
       " Document(page_content='12.4 Depth-First Search Trees 209\\nFig. 12.43 Ag r a p ha n da n\\nEulerian subgraph1 2 3 4\\n5 6 7 8\\n1 2 3 4\\n5 6 7 8\\n{1,2,3,4}to node 5. However, edge 6 →7 is not a bridge, because there is a back\\nedge 7 →5, and node 5 is an ancestor of node 6.\\nFinding articulation points is a bit more difﬁcult, but we can again use the depth-\\nﬁrst search tree. First, if a node xis the root of the tree, it is an articulation point\\nexactly when it has two or more children. Then, if xis not the root, it is an articulation\\npoint exactly when it has a child whose subtree does not contain a back edge to an\\nancestor of x.\\nFor example, in Fig. 12.42 , node 5 is an articulation point, because it is the root\\nand has two children, and node 7 is an articulation point, because the subtree of its\\nchild 8 does not contain a back edge to an ancestor of 7. However, node 2 is not\\nan articulation point, because there is a back edge 3 → 4, and node 8 is not an\\narticulation point, because it does not have any children.\\n12.4.2 Eulerian Subgraphs\\nAnEulerian subgraph of a graph contains the nodes of the graph and a subset of\\nthe edges such that the degree of each node is even . For example, Fig. 12.43 shows\\na graph and its Eulerian subgraph.\\nConsider the problem of calculating the total number of Eulerian subgraphs for a\\nconnected graph. It turns out that there is a simple formula for this: there are always\\n2kEulerian subgraphs were kis the number of back edges in the depth-ﬁrst search\\ntree of the graph. Note that k=m−(n−1)where nis the number of nodes and m\\nis the number of edges.\\nThe depth-ﬁrst search tree helps to understand why this formula holds. Consider\\nany ﬁxed subset of back edges in the depth-ﬁrst search tree. To create an Eulerian\\nsubgraph that contains these edges, we need to choose a subset of the tree edges so\\nthat each node has an even degree. To do this, we process the tree from bottom to\\ntop and always include a tree edge in the subgraph exactly when it points to a node\\nwhose degree is even with the edge. Then, since the sum of degrees is even, also the\\ndegree of the root node will be even.', metadata={'source': 'documents\\\\doc.pdf', 'page': 211}),\n",
       " Document(page_content='Geometry\\nThis chapter discusses algorithm techniques related to geometry. The general goal\\nof the chapter is to ﬁnd ways to conveniently solve geometric problems, avoiding\\nspecial cases and tricky implementations.\\nSection 13.1 introduces the C++ complex number class which has useful tools for\\ngeometric problems. After this, we will learn to use cross products to solve various\\nproblems, such as testing whether two line segments intersect and calculating the\\ndistance from a point to a line. Finally, we discuss ways to calculate polygon areas\\nand explore special properties of Manhattan distances.\\nSection 13.2 focuses on sweep line algorithms which play an important role in\\ncomputational geometry. We will see how to use such algorithms for counting inter-\\nsection points, ﬁnding closest points, and constructing convex hulls.\\n13.1 Geometric T echniques\\nA challenge when solving geometric problems is how to approach the problem so\\nthat the number of special cases is as small as possible and there is a convenient way\\nto implement the solution. In this section, we will go through a set of tools that make\\nsolving geometric problems easier.\\n13.1.1 Complex Numbers\\nAcomplex number is a number of the form x+yi, where i=√−1i st h e imaginary\\nunit . A geometric interpretation of a complex number is that it represents a two-\\ndimensional point (x,y)or a vector from the origin to a point (x,y). For example,\\nFig. 13.1 illustrates the complex number 4 +2i.', metadata={'source': 'documents\\\\doc.pdf', 'page': 212}),\n",
       " Document(page_content='212 13 Geometry\\nFig. 13.1 Complex number\\n4+2iinterpreted as a point\\nand a vector(4,2)\\nThe C++ complex number class complex is useful when solving geometric\\nproblems. Using the class we can represent points and vectors as complex numbers,\\nand use the features of the class to manipulate them. To do this, let us ﬁrst deﬁne\\na coordinate type C. Depending on the situation, a suitable type is long long or\\nlong double . As a general rule, it is good to use integer coordinates whenever\\npossible, because calculations with integers are exact.\\nHere are possible coordinate type deﬁnitions:\\ntypedef long long C;\\ntypedef long double C;\\nAfter this, we can deﬁne a complex type Pthat represents a point or a vector:\\ntypedef complex<C> P;\\nFinally, the following macros refer to x and y coordinates:\\n#define X real()\\n#define Y imag()\\nFor example, the following code creates a point p=(4,2)and prints its x and y\\ncoordinates:\\nP p = {4,2};\\ncout << p.X << \"\" << p.Y << \"\\\\n\"; // 4 2\\nThen, the following code creates vectors v=(3,1)andu=(2,2), and after that\\ncalculates the sum s=v+u.\\nP v = {3,1};\\nP u = {2,2};\\nPs=v + u ;\\ncout << s.X << \"\" << s.Y << \"\\\\n\"; // 5 3', metadata={'source': 'documents\\\\doc.pdf', 'page': 213}),\n",
       " Document(page_content='13.1 Geometric Techniques 213\\nFunctions Thecomplex class also has functions that are useful in geometric prob-\\nlems. The following functions should only be used when the coordinate type is long\\ndouble (or another ﬂoating point type).\\nThe function abs(v)calculates the length |v|of a vector v=(x,y)using the\\nformula√\\nx2+y2. The function can also be used for calculating the distance between\\npoints (x1,y1)and(x2,y2), because that distance equals the length of the vector\\n(x2−x1,y2−y1). For example, the following code calculates the distance between\\npoints (4,2)and(3,−1)\\nP a = {4,2};\\nP b = {3,-1};\\ncout << abs(b-a) << \"\\\\n\"; // 3.16228\\nThe function arg(v)calculates the angle of a vector v=(x,y)with respect to\\nthe x-axis. The function gives the angle in radians, where rradians equals 180 r/π\\ndegrees. The angle of a vector that points to the right is 0, and angles decrease\\nclockwise and increase counterclockwise.\\nThe function polar (s,a)constructs a vector whose length is sand that points\\nto an angle a, given in radians. A vector can be rotated by an angle aby multiplying\\nit by a vector with length 1 and angle a.\\nThe following code calculates the angle of the vector (4,2), rotates it 1 /2 radians\\ncounterclockwise, and then calculates the angle again:\\nP v = {4,2};\\ncout << arg(v) << \"\\\\n\"; // 0.463648\\nv *= polar(1.0,0.5);\\ncout << arg(v) << \"\\\\n\"; // 0.963648\\n13.1.2 Points and Lines\\nThe cross product a ×bof vectors a=(x1,y1)andb=(x2,y2)is deﬁned to be\\nx1y2−x2y1. It tells us the direction to which bturns when it is placed directly after\\na. There are three cases illustrated in Fig. 13.2 :\\nab\\na×b>0ab\\na×b=0ab\\na×b<0\\nFig. 13.2 Interpretation of cross products', metadata={'source': 'documents\\\\doc.pdf', 'page': 214}),\n",
       " Document(page_content='214 13 Geometry\\nFig. 13.3 Testing the\\nlocation of a point\\ns1s2p\\n•a×b>0:bturns left\\n•a×b=0:bdoes not turn (or turns 180 degrees)\\n•a×b<0:bturns right\\nFor example, the cross product of vectors a=(4,2)andb=(1,2)is 4·2−2·\\n1=6, which corresponds to the ﬁrst scenario of Fig. 13.2 . The cross product can be\\ncalculated using the following code:\\nP a = {4,2};\\nP b = {1,2};\\nC p = (conj(a)*b).Y; // 6\\nThe above code works, because the function conj negates the y coordinate of\\na vector, and when the vectors (x1,−y1)and(x2,y2)are multiplied together, the y\\ncoordinate of the result is x1y2−x2y1.\\nNext we will go through some applications of cross products.\\nTesting Point Location Cross products can be used to test whether a point is located\\non the left or right side of a line. Assume that the line goes through points s1and\\ns2, we are looking from s1tos2and the point is p. For example, in Fig. 13.3 ,pis\\nlocated on the left side of the line.\\nThe cross product (p−s1)×(p−s2)tells us the location of the point p. If the\\ncross product is positive, pis located on the left side, and if the cross product is\\nnegative, pis located on the right side. Finally, if the cross product is zero, the points\\ns1,s2, and pare on the same line.\\nLine Segment Intersection Next, consider the problem of testing whether two line\\nsegments abandcdintersect. It turns out that if the line segments intersect, there\\nare three possible cases:\\nCase 1: The line segments are on the same line and they overlap each other. In\\nthis case, there is an inﬁnite number of intersection points. For example, in Fig. 13.4 ,\\nall points between candbare intersection points. To detect this case, we can use\\ncross products to test if all points are on the same line. If they are, we can then sort\\nthem and check whether the line segments overlap each other.\\nCase 2: The line segments have a common vertex that is the only intersection\\npoint. For example, in Fig. 13.5 the intersection point is b=c. This case is easy\\nto check, because there are only four possibilities for the intersection point: a=c,\\na=d,b=c, and b=d.', metadata={'source': 'documents\\\\doc.pdf', 'page': 215}),\n",
       " Document(page_content='13.1 Geometric Techniques 215\\nFig. 13.4 Case 1: the line\\nsegments are on the same\\nline and overlap each other\\nad\\ncb\\nFig. 13.5 Case 2: the line\\nsegments have a common\\nvertex\\nab=c\\nd\\nFig. 13.6 Case 3: the line\\nsegments have an\\nintersection point that is not\\nav e r t e x\\ncda\\nbp\\nFig. 13.7 Calculating the\\ndistance from pto the line\\ns1s2p\\nd\\nCase 3: There is exactly one intersection point that is not a vertex of any line\\nsegment. In Fig. 13.6 , the point pis the intersection point. In this case, the line\\nsegments intersect exactly when both points canddare on different sides of a line\\nthrough aandb, and points aandbare on different sides of a line through candd.\\nWe can use cross products to check this.\\nDistance from a Point to a Line Another property of cross products is that the area\\nof a triangle can be calculated using the formula\\n|(a−c)×(b−c)|\\n2,\\nwhere a,b, and care the vertices of the triangle. Using this fact, we can derive a\\nformula for calculating the shortest distance between a point and a line. For example,\\nin Fig. 13.7 ,dis the shortest distance between the point pand the line that is deﬁned\\nby the points s1ands2.', metadata={'source': 'documents\\\\doc.pdf', 'page': 216}),\n",
       " Document(page_content='216 13 Geometry\\nFig. 13.8 Point ais inside\\nand point bis outside the\\npolygon\\nab\\nFig. 13.9 Sending rays from\\npoints aandb\\nab\\nThe area of a triangle whose vertices are s1,s2, and pcan be calculated in two\\nways: it is both1\\n2|s2−s1|d(the standard formula taught in school) and1\\n2((s1−p)×\\n(s2−p))(the cross product formula). Thus, the shortest distance is\\nd=(s1−p)×(s2−p)\\n|s2−s1|.\\nPoint in a Polygon Finally, consider the problem of testing whether a point is located\\ninside or outside a polygon. For example, in Fig. 13.8 , point ais inside the polygon\\nand point bis outside the polygon.\\nA convenient way to solve the problem is to send a ray from the point to an\\narbitrary direction and calculate the number of times it touches the boundary of the\\npolygon. If the number is odd, the point is inside the polygon, and if the number is\\neven, the point is outside the polygon.\\nFor example, in Fig. 13.9 , the rays from atouch 1 and 3 times the boundary of the\\npolygon, so ais inside the polygon. In a similar way, the rays from btouch 0 and 2\\ntimes the boundary of the polygon, so bis outside the polygon.\\n13.1.3 Polygon Area\\nA general formula for calculating the area of a polygon, sometimes called the\\nshoelace formula , is as follows:\\n1\\n2|n−1∑\\ni=1(pi×pi+1)|=1\\n2|n−1∑\\ni=1(xiyi+1−xi+1yi)|.', metadata={'source': 'documents\\\\doc.pdf', 'page': 217}),\n",
       " Document(page_content='13.1 Geometric Techniques 217\\nFig. 13.10 A polygon\\nwhose area is 17/2\\n(4,1)(7,3)(5,5)\\n(2,4)\\n(4,3)\\nFig. 13.11 Calculating the\\narea of the polygon using\\ntrapezoids\\n(4,1)(7,3)(5,5)\\n(2,4)\\n(4,3)\\nHere the vertices are p1=(x1,y1),p2=(x2,y2) ,..., pn=(xn,yn)in such an\\norder that piandpi+1are adjacent vertices on the boundary of the polygon, and the\\nﬁrst and last vertex is the same, i.e., p1=pn.\\nFor example, the area of the polygon in Fig. 13.10 is\\n|(2·5−5·4)+(5·3−7·5)+(7·1−4·3)+(4·3−4·1)+(4·4−2·3)|\\n2=17/2.\\nThe idea behind the formula is to go through trapezoids whose one side is a side\\nof the polygon, and another side lies on the horizontal line y=0. For example,\\nFig. 13.11 shows one such trapezoid. The area of each trapezoid is\\n(xi+1−xi)yi+yi+1\\n2,\\nwhere the vertices of the polygon are piand pi+1.I fxi+1>xi, the area is positive,\\nand if xi+1<xi, the area is negative. Then, the area of the polygon is the sum of\\nareas of all such trapezoids, which yields the formula\\n⏐⏐⏐⏐⏐n−1∑\\ni=1(xi+1−xi)yi+yi+1\\n2⏐⏐⏐⏐⏐=1\\n2⏐⏐⏐⏐⏐n−1∑\\ni=1(xiyi+1−xi+1yi)⏐⏐⏐⏐⏐.\\nNote that the absolute value of the sum is taken, because the value of the sum may\\nbe positive or negative, depending on whether we walk clockwise or counterclock-\\nwise along the boundary of the polygon.', metadata={'source': 'documents\\\\doc.pdf', 'page': 218}),\n",
       " Document(page_content='218 13 Geometry\\nFig. 13.12 Calculating the\\npolygon area using Pick’s\\ntheorem\\n(4,1)(7,3)(5,5)\\n(2,4)\\n(4,3)\\nPick’s Theorem Pick’s theorem provides another way to calculate the area of a\\npolygon, assuming that all vertices of the polygon have integer coordinates. Pick’s\\ntheorem tells us that the area of the polygon is\\na+b/2−1,\\nwhere ais the number of integer points inside the polygon and bis the number of\\ninteger points on the boundary of the polygon. For example, the area of the polygon\\nin Fig. 13.12 is\\n6+7/2−1=17/2.\\n13.1.4 Distance Functions\\nAdistance function deﬁnes the distance between two points. The usual distance\\nfunction is the Euclidean distance where the distance between points (x1,y1)and\\n(x2,y2)is\\n√\\n(x2−x1)2+(y2−y1)2.\\nAn alternative distance function is the Manhattan distance where the distance\\nbetween points (x1,y1)and(x2,y2)is\\n|x1−x2|+| y1−y2|.\\nFor example, in Fig. 13.13 , the Euclidean distance between the points is\\n√\\n(5−2)2+(2−1)2=√\\n10\\nand the Manhattan distance is\\n|5−2|+| 2−1|= 4.', metadata={'source': 'documents\\\\doc.pdf', 'page': 219}),\n",
       " Document(page_content='13.1 Geometric Techniques 219\\n(2,1)(5,2)\\n(2,1)(5,2)\\nEuclidean distance Manhattan distance\\nFig. 13.13 Two distance functions\\nFig. 13.14 Regions within a\\ndistance of 1\\nEuclidean distance Manhattan distance\\nFig. 13.15 Points BandC\\nhave the maximum\\nManhattan distance\\nAC\\nBD\\nFig. 13.16 Maximum\\nManhattan distance after\\ntransforming the coordinates A\\nC\\nB\\nD\\nFigure 13.14 shows regions that are within a distance of 1 from the center point,\\nusing the Euclidean and Manhattan distances.\\nSome problems are easier to solve if Manhattan distances are used instead of\\nEuclidean distances. As an example, given a set of points in the two-dimensional\\nplane, consider the problem of ﬁnding two points whose Manhattan distance is maxi-\\nmum. For example, in Fig. 13.15 , we should select points BandCto get the maximum\\nManhattan distance 5.\\nA useful technique related to Manhattan distances is to transform the coordinates\\nso that a point (x,y)becomes (x+y,y−x). This rotates the point set 45◦and scales\\nit. For example, Fig. 13.16 shows the result of the transformation in our example\\nscenario.', metadata={'source': 'documents\\\\doc.pdf', 'page': 220}),\n",
       " Document(page_content='220 13 Geometry\\nThen, consider two points p1=(x1,y1)and p2=(x2,y2)whose transformed\\ncoordinates are p′\\n1=(x′\\n1,y′\\n1)andp′\\n2=(x′\\n2,y′\\n2). Now there are two ways to express\\nthe Manhattan distance between p1and p2:\\n|x1−x2|+| y1−y2|= max(|x′\\n1−x′\\n2|,|y′\\n1−y′\\n2|)\\nFor example, if p1=(1,0)and p2=(3,3), the transformed coordinates are\\np′\\n1=(1,−1)and p′\\n2=(6,0)and the Manhattan distance is\\n|1−3|+| 0−3|= max(|1−6|,|− 1−0|)=5.\\nThe transformed coordinates provide a simple way to operate with Manhattan\\ndistances, because we can consider x and y coordinates separately. In particular, to\\nmaximize the Manhattan distance, we should ﬁnd two points whose transformed\\ncoordinates maximize the value of\\nmax(|x′\\n1−x′\\n2|,|y′\\n1−y′\\n2|).\\nThis is easy, because either the horizontal or vertical difference of the transformed\\ncoordinates has to be maximum.\\n13.2 Sweep Line Algorithms\\nMany geometric problems can be solved using sweep line algorithms. The idea in\\nsuch algorithms is to represent an instance of the problem as a set of events that\\ncorrespond to points in the plane. Then, the events are processed in increasing order\\naccording to their x or y coordinates.\\n13.2.1 Intersection Points\\nGiven a set of nline segments, each of them being either horizontal or vertical,\\nconsider the problem of counting the total number of intersection points. For example,\\nin Fig. 13.17 , there are ﬁve line segments and three intersection points.\\nFig. 13.17 Five line\\nsegments with three\\nintersection points', metadata={'source': 'documents\\\\doc.pdf', 'page': 221}),\n",
       " Document(page_content='13.2 Sweep Line Algorithms 221\\nFig. 13.18 Events that\\ncorrespond to the line\\nsegments\\n1212\\n1233\\nIt is easy to solve the problem in O(n2)time, because we can go through all\\npossible pairs of line segments and check if they intersect. However, we can solve\\nthe problem more efﬁciently in O(nlogn)time using a sweep line algorithm and a\\nrange query data structure. The idea is to process the endpoints of the line segments\\nfrom left to right and focus on three types of events:\\n(1) horizontal segment begins\\n(2) horizontal segment ends\\n(3) vertical segment\\nFigure 13.18 shows the events in our example scenario.\\nAfter creating the events, we go through them from left to right and use a data\\nstructure that maintains the y coordinates of the active horizontal segments. At event\\n1, we add the y coordinate of the segment to the structure, and at event 2, we remove\\nthe y coordinate from the structure. Intersection points are calculated at event 3:\\nwhen processing a vertical segment between points y1andy2, we count the number\\nof active horizontal segments whose y coordinate is between y1andy2, and add this\\nnumber to the total number of intersection points.\\nTo store y coordinates of horizontal segments, we can use a binary indexed or\\nsegment tree, possibly with index compression. Processing each event takes O(logn)\\ntime, so the algorithm works in O(nlogn)time.\\n13.2.2 Closest Pair Problem\\nGiven a set of npoints, our next problem is to ﬁnd two points whose Euclidean\\ndistance is minimum. For example, Fig. 13.19 shows a set of points, where the closest\\npair is painted black.\\nThis is another example of a problem that can be solved in O(nlogn)time using\\na sweep line algorithm.1We go through the points from left to right and maintain\\n1Creating an efﬁcient algorithm for the closest pair problem was once an important open problem in\\ncomputational geometry. Finally, Shamos and Hoey [ 26] discovered a divide and conquer algorithm\\nthat works in O(nlogn)time. The sweep line algorithm presented here has common elements with\\ntheir algorithm, but it is easier to implement.', metadata={'source': 'documents\\\\doc.pdf', 'page': 222}),\n",
       " Document(page_content='222 13 Geometry\\nFig. 13.19 An instance of the closest pair problem\\nd\\nd\\nFig. 13.20 Region where the closest point must lie\\nFig. 13.21 Closest point\\nregion contains O(1)pointsd\\nd\\na value d: the minimum distance between two points seen so far. At each point, we\\nﬁnd its nearest point to the left. If the distance is less than d, it is the new minimum\\ndistance and we update the value of d.\\nIf the current point is (x,y)and there is a point to the left within a distance of\\nless than d, the x coordinate of such a point must be between [x−d,x]and the y\\ncoordinate must be between [y−d,y+d]. Thus, it sufﬁces to only consider points\\nthat are located in those ranges, which makes the algorithm efﬁcient. For example,\\nin Fig. 13.20 , the region marked with dashed lines contains the points that can be\\nwithin a distance of dfrom the active point.\\nThe efﬁciency of the algorithm is based on the fact that the region always contains\\nonly O(1)points. To see why this holds, consider Fig. 13.21 . Since the current\\nminimum distance between two points is d, each d/2×d/2 square may contain at\\nmost one point. Thus, there are at most eight points in the region.', metadata={'source': 'documents\\\\doc.pdf', 'page': 223}),\n",
       " Document(page_content='13.2 Sweep Line Algorithms 223\\nFig. 13.22 Convex hull of a point set\\nstep 1 step 2 step 3 step 4\\nstep 5 step 6 step 7 step 8\\nstep 9 step 10 step 11 step 12\\nstep 13 step 14 step 15 step 16\\nstep 17 step 18 step 19 step 20\\nFig. 13.23 Constructing the upper part of the convex hull using Andrew’s algorithm', metadata={'source': 'documents\\\\doc.pdf', 'page': 224}),\n",
       " Document(page_content='224 13 Geometry\\nWe can go through the points in the region in O(logn)time by maintaining a set\\nof points whose x coordinates are between [x−d,x]so that the points are sorted\\nin increasing order according to their y coordinates. The time complexity of the\\nalgorithm is O(nlogn), because we go through npoints and determine for each\\npoint its nearest point to the left in O(logn)time.\\n13.2.3 Convex Hull Problem\\nAconvex hull is the smallest convex polygon that contains all points of a given\\npoint set. Here convexity means that a line segment between any two vertices of the\\npolygon is completely inside the polygon. For example, Fig. 13.22 shows the convex\\nhull of a point set.\\nThere are many efﬁcient algorithms for constructing convex hulls. Perhaps the\\nsimplest among them is Andrew’s algorithm [2], which we will describe next. The\\nalgorithm ﬁrst determines the leftmost and rightmost points in the set, and then\\nconstructs the convex hull in two parts: ﬁrst the upper hull and then the lower hull.\\nBoth parts are similar, so we can focus on constructing the upper hull.\\nFirst, we sort the points primarily according to x coordinates and secondarily\\naccording to y coordinates. After this, we go through the points and add each point\\nto the hull. Always after adding a point to the hull, we make sure that the last line\\nsegment in the hull does not turn left. As long as it turns left, we repeatedly remove\\nthe second last point from the hull. Figure 13.23 shows how Andrew’s algorithm\\ncreates the upper hull for our example point set.', metadata={'source': 'documents\\\\doc.pdf', 'page': 225}),\n",
       " Document(page_content='String Algorithms\\nThis chapter deals with topics related to string processing.\\nSection 14.1 presents the trie structure which maintains a set of strings. After this,\\ndynamic programming algorithms for determining longest common subsequences\\nand edit distances are discussed.\\nSection 14.2 discusses the string hashing technique which is a general tool for\\ncreating efﬁcient string algorithms. The idea is to compare hash values of strings\\ninstead of their characters, which allows us to compare strings in constant time.\\nSection 14.3 introduces the Z-algorithm which determines for each string position\\nthe longest substring which is also a preﬁx of the string. The Z-algorithm is an\\nalternative for many string problems that can also be solved using hashing.\\nSection 14.4 discusses the sufﬁx array structure, which can be used to solve some\\nmore advanced string problems.\\n14.1 Basic Topics\\nThroughout the chapter, we assume that all strings are zero indexed. For example, a\\nstring sof length nconsists of characters s[0],s[1],..., s[n−1].\\nAsubstring is a sequence of consecutive characters in a string. We use the notation\\ns[a...b]to refer to a substring of sthat starts at position aand ends at position b.\\nApreﬁx is a substring that contains the ﬁrst character of a string, and a sufﬁx is a\\nsubstring that contains the last character of a string.\\nAsubsequence is any sequence of characters in a string in their original order. All\\nsubstrings are subsequences, but the converse is not true (Fig. 14.1).', metadata={'source': 'documents\\\\doc.pdf', 'page': 226}),\n",
       " Document(page_content='226 14 String Algorithms\\nFig. 14.1 NVELO is a\\nsubstring, NEP is a\\nsubsequenceENV EL OPE asubstrin g\\nENV EL OPE asubseq uence\\nFig. 14.2 At r i et h a t\\ncontains the strings CANAL ,\\nCANDY ,THE,a n dTHEREC T\\nA\\nN\\nA D\\nL YH\\nE\\nR\\nE\\n14.1.1 Trie Structure\\nAtrieis a rooted tree that maintains a set of strings. Each string in the set is stored as a\\ncharacter chain that starts at the root node. If two strings have a common preﬁx, they\\nalso have a common chain in the tree. As an example, the trie in Fig. 14.2 corresponds\\nto the set {CANAL ,CANDY ,THE ,THERE }. A circle in a node means that a string in\\nthe set ends at the node.\\nAfter constructing a trie, we can easily check whether it contains a given string\\nby following the chain that starts at the root node. We can also add a new string to\\nthe trie by ﬁrst following the chain and then adding new nodes if necessary. Both the\\noperations work in O(n)time where nis the length of the string.\\nA trie can be stored in an array\\nint trie[N][A];\\nwhere Nis the maximum number of nodes (the maximum total length of the strings\\nin the set) and Ais the size of the alphabet. The trie nodes are numbered 0 ,1,2,...\\nin such a way that the number of the root is 0, and trie[s][c]speciﬁes the next\\nnode in the chain when we move from node susing character c.\\nThere are several ways how we can extend the trie structure. For example, suppose\\nthat we are given queries that require us to calculate the number of strings in the set\\nthat have a certain preﬁx. We can do this efﬁciently by storing for each trie node the\\nnumber of strings whose chain goes through the node.', metadata={'source': 'documents\\\\doc.pdf', 'page': 227}),\n",
       " Document(page_content='14.1 Basic Topics 227\\nFig. 14.3 The values of the\\nlcs function for\\ndetermining the longest\\ncommon subsequence of\\nTOUR andOPERAT\\nO\\nU\\nROPERA\\n0\\n1\\n1\\n10\\n1\\n1\\n10\\n1\\n1\\n10\\n1\\n1\\n20\\n1\\n1\\n2\\n14.1.2 Dynamic Programming\\nDynamic programming can be used to solve many string problems. Next we will\\ndiscuss two examples of such problems.\\nLongest Common Subsequence The longest common subsequence of two strings\\nis the longest string that appears as a subsequence in both strings. For example, the\\nlongest common subsequence of TOUR andOPERA isOR.\\nUsing dynamic programming, we can determine the longest common subsequence\\nof two strings xandyinO(nm )time, where nandmdenote the lengths of the strings.\\nTo do this, we deﬁne a function lcs (i,j)that gives the length of the longest common\\nsubsequence of the preﬁxes x[0...i]andy[0...j]. Then, we can use the recurrence\\nlcs (i,j)={\\nlcs (i−1,j−1)+1 x[i]=y[j]\\nmax (lcs (i,j−1),lcs (i−1,j))otherwise .\\nThe idea is that if characters x[i]andy[j]are equal, we match them and increase\\nthe length of the longest common subsequence by one. Otherwise, we remove the\\nlast character from either xory, depending on which choice is optimal.\\nFor example, Fig. 14.3 shows the values of the lcs function in our example\\nscenario.\\nEdit Distances The edit distance (orLevenshtein distance ) between two strings\\ndenotes the minimum number of editing operations that transform the ﬁrst string\\ninto the second string. The allowed editing operations are as follows:\\n•insert a character (e.g., ABC→ABCA )\\n•remove a character (e.g., ABC→AC)\\n•modify a character (e.g., ABC→ADC)\\nFor example, the edit distance between LOVE andMOVIE is 2, because we can\\nﬁrst perform the operation LOVE →MOVE (modify) and then the operation MOVE\\n→MOVIE (insert).\\nWe can calculate the edit distance between two strings xandyinO(nm )time,\\nwhere nandmare the lengths of the strings. Let edit (i,j)denote the edit distance\\nbetween the preﬁxes x[0...i]andy[0...j]. The values of the function can be\\ncalculated using the recurrence', metadata={'source': 'documents\\\\doc.pdf', 'page': 228}),\n",
       " Document(page_content='228 14 String Algorithms\\nFig. 14.4 The values of the\\nedit function for\\ndetermining the edit distance\\nbetween LOVE andMOVIEL\\nO\\nV\\nEMOVIE\\n1\\n2\\n3\\n42\\n1\\n2\\n33\\n2\\n1\\n24\\n3\\n2\\n25\\n4\\n3\\n2\\nedit (a,b)=min (edit (a,b−1)+1,\\nedit (a−1,b)+1,\\nedit (a−1,b−1)+cost (a,b)),\\nwhere cost (a,b)=0i fx[a]=y[b], and otherwise cost (a,b)=1. The formula\\nconsiders three ways to edit the string x: insert a character at the end of x, remove\\nthe last character from x, or match/modify the last character of x. In the last case, if\\nx[a]=y[b], we can match the last characters without editing.\\nFor example, Fig. 14.4 shows the values of the edit function in our example\\nscenario.\\n14.2 String Hashing\\nUsing string hashing we can efﬁciently check whether two strings are equal by\\ncomparing their hash values. A hash value is an integer that is calculated from the\\ncharacters of the string. If two strings are equal, their hash values are also equal,\\nwhich makes it possible to compare strings based on their hash values.\\n14.2.1 Polynomial Hashing\\nA usual way to implement string hashing is polynomial hashing , which means that\\nthe hash value of a string sof length nis\\n(s[0]An−1+s[1]An−2+···+ s[n−1]A0)mod B,\\nwhere s[0],s[1],..., s[n−1]are interpreted as character codes, and Aand Bare\\nprechosen constants.\\nFor example, let us calculate the hash value of the string ABACB . The character\\ncodes of A,B, andCare 65, 66, and 67. Then, we need to ﬁx the constants; suppose\\nthat A=3 and B=97. Thus, the hash value is\\n(65·34+66·33+65·32+66·31+67·30)mod 97 =40.', metadata={'source': 'documents\\\\doc.pdf', 'page': 229}),\n",
       " Document(page_content='14.2 String Hashing 229\\nWhen polynomial hashing is used, we can calculate the hash value of any substring\\nof a string sinO(1)time after an O(n)time preprocessing. The idea is to construct\\nan array hsuch that h[k]contains the hash value of the preﬁx s[0...k]. The array\\nvalues can be recursively calculated as follows:\\nh[0]=s[0]\\nh[k]= (h[k−1]A+s[k])mod B\\nIn addition, we construct an array pwhere p[k]= Akmod B:\\np[0]=1\\np[k]= (p[k−1]A)mod B.\\nConstructing the above arrays takes O(n)time. After this, the hash value of any\\nsubstring s[a...b]can be calculated in O(1)time using the formula\\n(h[b]−h[a−1]p[b−a+1])mod B\\nassuming that a>0. If a=0, the hash value is simply h[b].\\n14.2.2 Applications\\nWe can efﬁciently solve many string problems using hashing, because it allows us\\nto compare arbitrary substrings of strings in O(1)time. In fact, we can often simply\\ntake a brute force algorithm and make it efﬁcient by using hashing.\\nPattern Matching A fundamental string problem is the pattern matching problem:\\ngiven a string sand a pattern p, ﬁnd the positions where poccurs in s. For example,\\nthe pattern ABC occurs at positions 0 and 5 in the string ABCABABCA (Fig. 14.5).\\nWe can solve the pattern matching problem in O(n2)time using a brute force\\nalgorithm that goes through all positions where pmay occur in sand compares\\nstrings character by character. Then, we can make the brute force algorithm efﬁcient\\nusing hashing, because each comparison of strings then only takes O(1)time. This\\nresults in an O(n)time algorithm.\\nDistinct Substrings Consider the problem of counting the number of distinct sub-\\nstrings of length kin a string. For example, the string ABABAB has two distinct\\nsubstrings of length 3: ABA andBAB. Using hashing, we can calculate the hash\\nvalue of each substring and reduce the problem to counting the number of distinct\\nintegers in a list, which can be done in O(nlogn)time.\\nFig. 14.5 The pattern ABC\\nappears two times in the\\nstring ABCABABCAAB CABAB CA012 3456 78', metadata={'source': 'documents\\\\doc.pdf', 'page': 230}),\n",
       " Document(page_content='230 14 String Algorithms\\nMinimal Rotation Arotation of a string can be created by repeatedly moving the\\nﬁrst character of the string to the end of the string. For example, the rotations of\\nATLAS areATLAS ,TLASA ,LASAT ,ASATL , andSATLA . Next we will consider the\\nproblem of ﬁnding the lexicographically minimal rotation of a string. For example,\\nthe minimal rotation of ATLAS isASATL .\\nWe can efﬁciently solve the problem by combining string hashing and binary\\nsearch . The key idea is that we can ﬁnd out the lexicographic order of two strings in\\nlogarithmic time. First, we calculate the length of the common preﬁx of the strings\\nusing binary search. Here hashing allows us to check in O(1)time whether two\\npreﬁxes of a certain length match. After this, we check the next character after the\\ncommon preﬁx, which determines the order of the strings.\\nThen, to solve the problem, we construct a string that contains two copies of\\nthe original string (e.g., ATLASATLAS ) and go through its substrings of length n\\nmaintaining the minimal substring. Since each comparison can be done in O(logn)\\ntime, the algorithm works in O(nlogn)time.\\n14.2.3 Collisions and Parameters\\nAn evident risk when comparing hash values is a collision , which means that two\\nstrings have different contents but equal hash values. In this case, an algorithm that\\nrelies on the hash values concludes that the strings are equal, but in reality they are\\nnot, and the algorithm may give incorrect results.\\nCollisions are always possible, because the number of different strings is larger\\nthan the number of different hash values. However, the probability of a collision is\\nsmall if the constants AandBare carefully chosen. A usual way is to choose random\\nconstants near 109, for example, as follows:\\nA=911382323\\nB=972663749\\nUsing such constants, the long long type can be used when calculating hash\\nvalues, because the products ABandBBwill ﬁt in long long . But is it enough to\\nhave about 109different hash values?\\nLet us consider three scenarios where hashing can be used:\\nScenario 1: Strings xand yare compared with each other. The probability of a\\ncollision is 1 /Bassuming that all hash values are equally probable.\\nScenario 2: A string xis compared with strings y1,y2,..., yn. The probability\\nof one or more collisions is\\n1−(1−1/B)n.\\nScenario 3: All pairs of strings x1,x2,..., xnare compared with each other. The\\nprobability of one or more collisions is\\n1−B·(B−1)·(B−2)··· (B−n+1)\\nBn.', metadata={'source': 'documents\\\\doc.pdf', 'page': 231}),\n",
       " Document(page_content='14.2 String Hashing 231\\nTable 14.1 Collision probabilities in hashing scenarios when n=106\\nConstant B Scenario 1 Scenario 2 Scenario 3\\n1030.00 1.00 1.00\\n1060.00 0.63 1.00\\n1090.00 0.00 1.00\\n10120.00 0.00 0.39\\n10150.00 0.00 0.00\\n10180.00 0.00 0.00\\nTable 14.1 shows the collision probabilities for different values of Bwhen n=\\n106. The table shows that in Scenarios 1 and 2, the probability of a collision is\\nnegligible when B≈109. However, in Scenario 3 the situation is very different: a\\ncollision will almost always happen when B≈109.\\nThe phenomenon in Scenario 3 is known as the birthday paradox : if there are n\\npeople in a room, the probability that some two people have the same birthday is\\nlarge even if nis quite small. In hashing, correspondingly, when all hash values are\\ncompared with each other, the probability that some two hash values are equal is\\nlarge.\\nWe can make the probability of a collision smaller by calculating multiple hash\\nvalues using different parameters. It is unlikely that a collision would occur in all\\nhash values at the same time. For example, two hash values with parameter B≈109\\ncorrespond to one hash value with parameter B≈1018, which makes the probability\\nof a collision very small.\\nSome people use constants B=232and B=264, which is convenient, because\\noperations with 32- and 64-bit integers are calculated modulo 232and 264. However,\\nthis is nota good choice, because it is possible to construct inputs that always generate\\ncollisions when constants of the form 2xare used [ 23].\\n14.3 Z-Algorithm\\nThe Z-array zof a string sof length ncontains for each k=0,1,..., n−1 the\\nlength of the longest substring of sthat begins at position kand is a preﬁx of s.\\nThus, z[k]= ptells us that s[0...p−1]equals s[k...k+p−1],b u ts[p]and\\ns[k+p]are different characters (or the length of the string is k+p).\\nFor example, Fig. 14.6 shows the Z-array of ABCABCABAB . In the array, for\\nexample, z[3]=5, because the substring ABCAB of length 5 is a preﬁx of s, but the\\nsubstring ABCABA of length 6 is not a preﬁx of s.', metadata={'source': 'documents\\\\doc.pdf', 'page': 232}),\n",
       " Document(page_content='232 14 String Algorithms\\nFig. 14.6 The Z-array of\\nABCABCABABAB CAB CABAB\\n–00500 2020012 3456 789\\nFig. 14.7 Scenario 1:\\nCalculating the value of z[3]AB CAB CABAB\\n–00 ???????012 3456 789\\nAB CAB CABAB\\n–005 ??????012 3456 789x y\\n14.3.1 Constructing the Z-Array\\nNext we describe an algorithm, called the Z-algorithm which efﬁciently constructs\\nthe Z-array in O(n)time.1The algorithm calculates the Z-array values from left\\nto right by both using information already stored in the array and by comparing\\nsubstrings character by character.\\nTo efﬁciently calculate the Z-array values, the algorithm maintains a range [x,y]\\nsuch that s[x...y]is a preﬁx of s, the value of z[x]has been determined, and yis\\nas large as possible. Since we know that s[0...y−x]ands[x...y]are equal, we\\ncan use this information when calculating subsequent array values. Suppose that we\\nhave calculated the values of z[0],z[1],..., z[k−1]and we want to calculate the\\nvalue of z[k]. There are three possible scenarios:\\nScenario 1 :y<k. In this case, we do not have information about the position k,\\nso we calculate the value of z[k]by comparing substrings character by character.\\nFor example, in Fig. 14.7, there is no [x,y]range yet, so we compare the substrings\\nstarting at positions 0 and 3 character by character. Since z[3]=5, the new [x,y]\\nrange becomes [3,7].\\nScenario 2 :y≥kandk+z[k−x]≤y. In this case we know that z[k]=z[k−\\nx], because s[0...y−x]ands[x...y]are equal and we stay inside the [x,y]\\nrange. For example, in Fig. 14.8, we conclude that z[4]=z[1]=0.\\nScenario 3 :y≥kandk+z[k−x]>y. In this case we know that z[k]≥y−\\nk+1. However, since we do not have information after the position y,w eh a v et o\\ncompare substrings character by character starting at positions y−k+1 and y+1.\\nFor example, in Fig. 14.9, we know that z[6]≥2. Then, since s[2]̸=s[8], it turns\\nout that, in fact, z[6]=2.\\n1Gusﬁeld [ 13] presents the Z-algorithm as the simplest known method for linear-time pattern match-\\ning and attributes the original idea to Main and Lorentz [ 22].', metadata={'source': 'documents\\\\doc.pdf', 'page': 233}),\n",
       " Document(page_content='14.3 Z-Algorithm 233\\nFig. 14.8 Scenario 2:\\nCalculating the value of z[4]\\nAB CAB CABAB\\n–005 ??????012 3456 789x y\\nAB CAB CABAB\\n–0050 ?????012 3456 789x y\\nFig. 14.9 Scenario 3:\\nCalculating the value of z[6]\\nAB CAB CABAB\\n–00500 ????012 3456 789x y\\nAB CAB CABAB\\n–00500 2 ???012 3456 789x y\\nThe resulting algorithm works in O(n)time, because always when two characters\\nmatch when comparing substrings character by character, the value of yincreases.\\nThus, the total work needed for comparing substrings is only O(n).\\n14.3.2 Applications\\nThe Z-algorithm provides an alternative way to solve many string problems that\\ncan be also solved using hashing. However, unlike hashing, the Z-algorithm always\\nworks and there is no risk of collisions. In practice, it is often a matter of taste whether\\nto use hashing or the Z-algorithm.\\nPattern Matching Consider again the pattern matching problem, where our task is\\nto ﬁnd the occurrences of a pattern pin a string s. We already solved the problem\\nusing hashing, but now we will see how the Z-algorithm handles the problem.\\nA recurrent idea in string processing is to construct a string that consists of multiple\\nindividual parts separated by special characters. In this problem, we can construct a\\nstring p#s, where pandsare separated by a special character #that does not occur\\nin the strings. Then, the Z-array of p#stells us the positions where poccurs in s,\\nbecause such positions contain the length of p.', metadata={'source': 'documents\\\\doc.pdf', 'page': 234}),\n",
       " Document(page_content='234 14 String Algorithms\\nFig. 14.10 Pattern matching\\nusing the Z-algorithmAB C#AB CABAB CA\\n–000300 20300 1012 3456 7891011 12\\nFig. 14.11 Finding borders\\nusing the Z-algorithmABA CABA CABA\\n–010701030 1012 3456 78910\\nFig. 14.12 The sufﬁx array\\nof the string ABAACBAB2603 7154012 3456 7\\nFig. 14.13 Another way to\\nrepresent the sufﬁx array2\\n6\\n0\\n3\\n7\\n1\\n5\\n4AACBAB\\nAB\\nABAACBAB\\nACBAB\\nB\\nBAACBAB\\nBAB\\nCBAB0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nFigure 14.10 shows the Z-array for s=ABCABABCA and p=ABC. Positions 4\\nand 9 contain the value 3, which means that poccurs in positions 0 and 5 in s.\\nFinding Borders Aborder is a string that is both a preﬁx and a sufﬁx of a string, but\\nnot the entire string. For example, the borders of ABACABACABA areA,ABA, and\\nABACABA . All borders of a string can be efﬁciently found using the Z-algorithm,\\nbecause a sufﬁx at position kis a border exactly when k+z[k]=nwhere nis the\\nlength of the string. For example, in Fig. 14.11 ,4+z[4]=11, which means that\\nABACABA is a border of the string.\\n14.4 Sufﬁx Arrays\\nThe sufﬁx array of a string describes the lexicographic order of its sufﬁxes. Each\\nvalue in the sufﬁx array is a starting position of a sufﬁx. For example, Fig. 14.12\\nshows the sufﬁx array of the string ABAACBAB .\\nIt is often convenient to represent the sufﬁx array vertically and also show the\\ncorresponding sufﬁxes (Fig. 14.13 ). However, note that the sufﬁx array itself only\\ncontains the starting positions of the sufﬁxes and not their characters.', metadata={'source': 'documents\\\\doc.pdf', 'page': 235}),\n",
       " Document(page_content='14.4 Sufﬁx Arrays 235\\n–––––––– 1211 3212initial labels final labels\\nround 0\\nlength 1\\n1, 2 2, 1 1, 1 1, 3 3, 2 2, 1 1, 2 2, 0 251365 24initial labels ﬁnal labels\\nround 1\\nlength 2\\n2, 1 5, 3 1, 6 3, 5 6, 2 5, 4 2, 0 4, 0 3614 8725initial labels ﬁnal labels\\nround 2\\nlength 4\\n3, 8 6, 7 1, 2 4, 5 8, 0 7, 0 2, 0 5, 0 3614 8725initial labels ﬁnal labels\\nround 3\\nlength 8\\nFig. 14.14 Constructing the labels for the string ABAACBAB\\n14.4.1 Preﬁx Doubling Method\\nA simple and efﬁcient way to create the sufﬁx array of a string is to use a preﬁx dou-\\nbling construction, which works in O(nlog2n)orO(nlogn)time, depending on\\nthe implementation.2The algorithm consists of rounds numbered 0 ,1,...,⌈log2n⌉,\\nand round igoes through substrings whose length is 2i. During a round, each sub-\\nstring xof length 2iis given an integer label l(x)such that l(a)=l(b)exactly when\\na=bandl(a)<l(b)exactly when a<b.\\nOn round 0, each substring consists of only one character, and we can, for example,\\nuse labels A=1,B=2, and so on. Then, on round i, where i>0, we use the labels\\nfor substrings of length 2i−1to construct labels for substrings of length 2i.T og i v ea\\nlabel l(x)for a substring xof length 2i, we divide xinto two halves aandbof length\\n2i−1whose labels are l(a)andl(b). (If the second half begins outside the string, we\\nassume that its label is 0.) First, we give xaninitial label that is a pair (l(a),l(b)).\\nThen, after all substrings of length 2ihave been given initial labels, we sort the initial\\nlabels and give ﬁnal labels that are consecutive integers 1 ,2,3, etc. The purpose of\\ngiving the labels is that after the last round, each substring has a unique label, and the\\nlabels show the lexicographic order of the substrings. Then, we can easily construct\\nthe sufﬁx array based on the labels.\\nFigure 14.14 shows the construction of the labels for ABAACBAB . For example,\\nafter round 1, we know that l(AB)=2 and l(AA)=1. Then, on round 2, the initial\\nlabel for ABAA is(2,1). Since there are two smaller initial labels ( (1,6)and (2,0)),\\nthe ﬁnal label is l(ABAA )=3. Note that in this example, each label is unique already\\n2The idea of preﬁx doubling is due to Karp, Miller, and Rosenberg [ 17]. There are also more\\nadvanced O(n)time algorithms for constructing sufﬁx arrays; Kärkkäinen and Sanders [ 16] provide\\na quite simple such algorithm.', metadata={'source': 'documents\\\\doc.pdf', 'page': 236}),\n",
       " Document(page_content='236 14 String Algorithms\\n2\\n6\\n0\\n3\\n7\\n1\\n5\\n4AACBAB\\nAB\\nABAACBAB\\nACBAB\\nB\\nBAACBAB\\nBAB\\nCBAB0\\n1\\n2\\n3\\n4\\n5\\n6\\n72\\n6\\n0\\n3\\n7\\n1\\n5\\n4AACBAB\\nAB\\nABAACBAB\\nACBAB\\nB\\nBAACBAB\\nBAB\\nCBAB0\\n1\\n2\\n3\\n4\\n5\\n6\\n72\\n6\\n0\\n3\\n7\\n1\\n5\\n4AACBAB\\nAB\\nABAACBAB\\nACBAB\\nB\\nBAACBAB\\nBAB\\nCBAB0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nFig. 14.15 Finding the occurrences of BAinABAACBAB using a sufﬁx array\\nafter round 2, because the ﬁrst four characters of the substrings completely determine\\ntheir lexicographical order.\\nThe resulting algorithm works in O(nlog2n)time, because there are O(logn)\\nrounds and we sort a list of npairs on each round. In fact, an O(nlogn)implemen-\\ntation is also possible, because we can use a linear-time sorting algorithm to sort the\\npairs. Still, a straightforward O(nlog2n)time implementation just using the C++\\nsort function is usually efﬁcient enough.\\n14.4.2 Finding Patterns\\nAfter constructing the sufﬁx array, we can efﬁciently ﬁnd the occurrences of any\\ngiven pattern in the string. This can be done in O(klogn)time, where nis the length\\nof the string and kis the length of the pattern. The idea is to process the pattern\\ncharacter by character and maintain a range in the sufﬁx array that corresponds to\\nthe preﬁx of the pattern processed so far. Using binary search, we can efﬁciently\\nupdate the range after each new character.\\nFor example, consider ﬁnding the occurrences of the pattern BAin the string\\nABAACBAB (Fig. 14.15 ). First, our search range is [0,7], which spans the entire\\nsufﬁx array. Then, after processing the character B, the range becomes [4,6]. Finally,\\nafter processing the character A, the range becomes [5,6]. Thus, we conclude that\\nBAhas two occurrences in ABAACBAB in positions 1 and 5.\\nCompared to string hashing and the Z-algorithm discussed earlier, the advantage\\nof the sufﬁx array is that we can efﬁciently process several queries that are related\\nto different patterns, and it is not necessary to know the patterns beforehand when\\nconstructing the sufﬁx array.\\n14.4.3 LCP Arrays\\nThe LCP array of a string gives for its each sufﬁx a LCP value : the length of the\\nlongest common preﬁx of the sufﬁx and the next sufﬁx in the sufﬁx array. Figure 14.16', metadata={'source': 'documents\\\\doc.pdf', 'page': 237}),\n",
       " Document(page_content='14.4 Sufﬁx Arrays 237\\nFig. 14.16 The LCP array\\nof the string ABAACBAB1\\n2\\n1\\n0\\n1\\n2\\n0\\n–AACBAB\\nAB\\nABAACBAB\\nACBAB\\nB\\nBAACBAB\\nBAB\\nCBAB0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nshows the LCP array for the string ABAACBAB . For example, the LCP value of the\\nsufﬁx BAACBAB is 2, because the longest common preﬁx of BAACBAB andBAB is\\nBA. Note that the last sufﬁx in the sufﬁx array does not have a LCP value.\\nNext we present an efﬁcient algorithm, due to Kasai et al. [ 18], for constructing\\nthe LCP array of a string, provided that we have already constructed its sufﬁx array.\\nThe algorithm is based on the following observation: Consider a sufﬁx whose LCP\\nvalue is x. If we remove the ﬁrst character from the sufﬁx and get another sufﬁx,\\nwe immediately know that its LCP value has to be at least x−1. For example, in\\nFig.14.16 , the LCP value of the sufﬁx BAACBAB is 2, so we know that the LCP\\nvalue of the sufﬁx AACBAB has to be at least 1. In fact, it happens to be exactly 1.\\nWe can use the above observation to efﬁciently construct the LCP array by calcu-\\nlating the LCP values in decreasing order of sufﬁx length. At each sufﬁx, we calculate\\nits LCP value by comparing the sufﬁx and the next sufﬁx in the sufﬁx array character\\nby character. Now we can use the fact that we know the LCP value of the sufﬁx\\nthat has one more character. Thus, the current LCP value has to be at least x−1,\\nwhere xis the previous LCP value, and we do not need to compare the ﬁrst x−1\\ncharacters of the sufﬁxes. The resulting algorithm works in O(n)time, because only\\nO(n)comparisons are done during the algorithm.\\nUsing the LCP array, we can efﬁciently solve some advanced string problems.\\nFor example, to calculate the number of distinct substrings in a string, we can simply\\nsubtract the sum of all values in the LCP array from the total number of substrings,\\ni.e., the answer to the problem is\\nn(n+1)\\n2−c,\\nwhere nis the length of the string and cis the sum of all values in the LCP array.\\nFor example, the string ABAACBAB has\\n8·9\\n2−7=29\\ndistinct substrings.', metadata={'source': 'documents\\\\doc.pdf', 'page': 238}),\n",
       " Document(page_content='Additional Topics\\nThis ﬁnal chapter presents a selection of advanced algorithms and data structures.\\nMastering the techniques of this chapter may sometimes help you to solve the most\\ndifﬁcult problem in a programming contest.\\nSection 15.1 discusses square root techniques for creating data structures and\\nalgorithms. Such solutions are often based on the idea of dividing a sequence of n\\nelements into O(√n)blocks, each of which consists of O(√n)elements.\\nSection 15.2 further explores the possibilities of segment trees. For example, we\\nwill see how to create a segment tree that supports both range queries and range\\nupdates at the same time.\\nSection 15.3 presents the treap data structure which allows us to efﬁciently split\\nan array into two parts and combine two arrays into a single array.\\nSection 15.4 focuses on optimizing dynamic programming solutions. First we will\\nlearn the convex hull trick which is used with linear functions, and after this we will\\ndiscuss the divide and conquer optimization and Knuth’s optimization.\\nSection 15.5 deals with miscellaneous algorithm design techniques, such as meet\\nin the middle and parallel binary search.\\n15.1 Square Root Techniques\\nA square root can be seen as a “poor man’s logarithm”: the complexity O(√n)is\\nbetter than O(n)but worse than O(logn). In any case, many data structures and\\nalgorithms involving square roots are fast and usable in practice. This section shows\\nsome examples of how square roots can be used in algorithm design.', metadata={'source': 'documents\\\\doc.pdf', 'page': 239}),\n",
       " Document(page_content='240 15 Additional Topics\\n15.1.1 Data Structures\\nSometimes we can create an efﬁcient data structure by dividing an array into blocks\\nof size√nand maintaining information about array values inside each block. For\\nexample, suppose that we should process two types of queries: modifying array values\\nand ﬁnding minimum values in ranges. We have previously seen that a segment tree\\ncan support both operations in O(logn)time, but next we will solve the problem in\\nanother simpler way where the operations take O(√n)time.\\nWe divide the array into blocks of√nelements, and maintain for each block the\\nminimum value inside it. For example, Fig. 15.1 shows an array of 16 elements that\\nis divided into blocks of 4 elements. When an array value changes, the corresponding\\nblock needs to be updated. This can be done in O(√n)time by going through the\\nvalues inside the block, as shown in Fig. 15.2 . Then, to calculate the minimum value\\nin a range, we divide the range into three parts such that the range consists of single\\nvalues and blocks between them. Figure 15.3 shows an example of such a division.\\nThe answer to the query is either a single value or the minimum value inside a block.\\nSince the number of single elements is O(√n)and the number of blocks is also\\nO(√n), the query takes O(√n)time.\\nHow efﬁcient is the resulting structure in practice? To ﬁnd this out, we conducted\\nan experiment where we created an array of nrandom int values and then processed\\nnrandom minimum queries. We implemented three data structures: a segment tree\\nwith O(logn)time queries, the square root structure described above with O(√n)\\ntime queries, and a plain array with O(n)time queries. Table 15.1 shows the results\\nof the experiment. It turns out that in this problem, the square root structure is quite\\nefﬁcient up to n=218; however, after this, it requires clearly more time than a\\nsegment tree.\\n5863 4 7 2 6 7 1 7 56 2 3 23 212\\nFig. 15.1 A square root structure for ﬁnding minimum values in ranges\\n5863 4 7 56 7 1 7 56 2 3 23 412\\nFig. 15.2 When an array value is updated, the value in the corresponding block has to be also\\nupdated\\n5863 4 7 2 6 7 1 7 56 2 3 23 212\\nFig. 15.3 To determine the minimum value in a range, the range is divided into single values and\\nblocks', metadata={'source': 'documents\\\\doc.pdf', 'page': 240}),\n",
       " Document(page_content='15.1 Square Root Techniques 241\\nTable 15.1 The running times of three data structures for range minimum queries: a segment tree\\n(O(logn)), a square root structure ( O(√n)), and a plain array ( O(n))\\nInput size n O(logn)Queries (s) O(√n)Queries (s) O(n)Queries (s)\\n2160.02 0.05 1.50\\n2170.03 0.16 6.02\\n2180.07 0.28 24.82\\n2190.14 1.14 >60\\n2200.31 2.11 >60\\n2210.66 9.27 >60\\nFig. 15.4 An instance of the\\nletter distance problemACEA\\nBDFD\\nEABC\\nCFEA\\n15.1.2 Subalgorithms\\nNext we discuss two problems that can be efﬁciently solved by creating two subal-\\ngorithms that are specialized for different kinds of situations during the algorithm.\\nWhile either of the subalgorithms could be used to solve the problem without the\\nother, we get an efﬁcient algorithm by combining them.\\nLetter Distances Our ﬁrst problem is as follows: We are given an n×ngrid whose\\neach square is assigned a letter. What is the minimum Manhattan distance between\\ntwo squares that have the same letter? For example, in Fig. 15.4 the minimum distance\\nis 2 between the two squares with letter “D.”\\nTo solve the problem, we can go through all letters that appear in the grid, and\\nfor each letter c, determine the minimum distance between two squares with letter\\nc. Consider two algorithms for processing a ﬁxed letter c:\\nAlgorithm 1 : Go through all pairs of squares that contain the letter cand determine\\nthe minimum distance pair among them. This algorithm works in O(k2)time, where\\nkis the number of squares with letter c.\\nAlgorithm 2 : Perform a breadth-ﬁrst search that simultaneously begins at each\\nsquare with letter c. The search takes O(n2)time.\\nBoth algorithms have certain worst-case situations. The worst case for Algorithm\\n1 is a grid where each square has the same color, in which case k=n2and the\\nalgorithm takes O(n4)time. Then, the worst case for Algorithm 2 is a grid where\\neach square has a distinct color. In this case, the algorithm is performed O(n2)times,\\nwhich takes O(n4)time.', metadata={'source': 'documents\\\\doc.pdf', 'page': 241}),\n",
       " Document(page_content='242 15 Additional Topics\\nFig. 15.5 A turn in the black\\nsquares game. The minimum\\ndistance from Xto a black\\nsquare is 3X\\nHowever, we can combine the algorithms so that they function as subalgorithms of\\na single algorithm. The idea is to decide for each color cseparately which algorithm\\nto use. Clearly, Algorithm 1 works well if kis small, and Algorithm 2 is best suited\\nfor cases where kis large. Thus, we can ﬁx a constant xand use Algorithm 1 if kis\\nat most x, and otherwise use Algorithm 2.\\nIn particular, by choosing x=√\\nn2=n, we get an algorithm that works in O(n3)\\ntime. First, each square that is processed using Algorithm 1 is compared with at most\\nnother squares, so processing those squares takes O(n3)time. Then, since there are\\nat most ncolors that appear in more than nsquares, Algorithm 2 is performed at\\nmost ntimes, and its total running time is also O(n3).\\nBlack Squares As another example, consider the following game: We are given an\\nn×ngrid where exactly one square is black and all other squares are white. On each\\nturn, one white square is chosen, and we should calculate the minimum Manhattan\\ndistance between this square and a black square. After this, the white square is\\npainted black. This process continues for n2−1 turns, after which all squares have\\nbeen painted black.\\nFor example, Fig. 15.5 shows a turn in the game. The minimum distance from the\\nchosen square Xto a black square is 3 (by going two steps down and one step right).\\nAfter this, the square is painted black.\\nWe can solve the problem by processing the turns in batches ofkturns. Before\\neach batch, we calculate for each square of the grid the minimum distance to a\\nblack square. This can be done in O(n2)time using breadth-ﬁrst search. Then,\\nwhen processing a batch, we keep a list of all squares that have been painted black\\nduring the current batch. Thus, the minimum distance to a black square is either the\\nprecalculated distance or a distance to one of the squares on the list. Since the list\\ncontains at most kvalues, it takes O(k)time to go through the list.\\nThen, by choosing k=√\\nn2=n, we get an algorithm that works in O(n3)time.\\nFirst, there are O(n)batches, so the total time used for breadth-ﬁrst searches is O(n3).\\nThen, the list of squares in a batch contains O(n)values, so calculating minimum\\ndistances for O(n2)squares also takes O(n3)time.\\nT uning Parameters In practice, it is not necessary to use the exact square root\\nvalue as the parameter, but rather we can ﬁne-tune the performance of an algorithm\\nby experimenting with different parameters and choosing the parameter that works\\nbest. Of course, the optimal parameter depends on the algorithm and also on the\\nproperties of the test data.', metadata={'source': 'documents\\\\doc.pdf', 'page': 242}),\n",
       " Document(page_content='15.1 Square Root Techniques 243\\nTable 15.2 Optimizing the value of the parameter kin the black squares algorithm\\nParameter k Running time (s)\\n200 5.74\\n500 2.41\\n1000 1.32\\n2000 1.02\\n5000 1.28\\n10000 2.13\\n20000 3.97\\nFig. 15.6 Some integer\\npartitions of a stick of\\nlength 77\\n3 4\\n1 3 1 2\\nTable 15.2 shows the results of an experiment where the O(n3)time algorithm for\\nthe black squares game was performed for different values of kwhen n=500. The\\norder in which the squares were painted black was randomly selected. In this case,\\nthe optimal parameter seems to be about k=2000.\\n15.1.3 Integer Partitions\\nSuppose that there is a stick whose length is n, and it is divided into some parts\\nwhose lengths are integers. For example, Fig. 15.6 shows some possible partitions\\nforn=7. What is the maximum number of distinct lengths in such a partition?\\nIt turns out that there are at most O(√n)distinct lengths. Namely an optimal\\nway to produce as many distinct lengths as possible is to include lengths 1 ,2,..., k.\\nThen, since\\n1+2+···+ k=k(k+1)\\n2,\\nwe can conclude that kcan be at most O(√n). Next, we will see how this observation\\ncan be used when designing algorithms.\\nKnapsack Problem Consider a knapsack problem where we are given a list of\\ninteger weights [w1,w2,..., wk]such that w1+w2+···+ wk=n, and our task\\nis to determine all possible weight sums that can be created. For example, Fig. 15.7\\nshows the possible sums using the weights [3,3,4].', metadata={'source': 'documents\\\\doc.pdf', 'page': 243}),\n",
       " Document(page_content='244 15 Additional Topics\\nFig. 15.7 The possible sums\\nusing the weights [3,3,4]✓✓ ✓ ✓ ✓✓0 12 3 4 56 7 8 910\\nUsing a standard knapsack algorithm (Sect. 6.2.3 ), we can solve the problem in\\nO(nk)time, so if k=O(n), the time complexity becomes O(n2). However, since\\nthere are at most O(√n)distinct weights, we can actually solve the problem more\\nefﬁciently by simultaneously processing all weights of a certain value. For example,\\nif the weights are [3,3,4], we ﬁrst process the two weights of value 3 and then the\\nweight of value 4. It is not difﬁcult to modify the standard knapsack algorithm so\\nthat processing each group of equal weights only takes O(n)time, which yields an\\nO(n√n)time algorithm.\\nString Construction As another example, suppose that we are given a string of\\nlength nand a dictionary of words whose total length is m. Our task is to count the\\nnumber of ways we can construct the string using the words. For example, there are\\nfour ways to construct the string ABAB using the words {A,B,AB}:\\n•A+B+A+B\\n•AB+A+B\\n•A+B+AB\\n•AB+AB\\nUsing dynamic programming, we can calculate for each k=0,1,..., nthe num-\\nber of ways to construct a preﬁx of length kof the string. One way to do this is to\\nuse a trie that contains reverses of all the words in the dictionary, which yields an\\nO(n2+m)time algorithm. However, another approach is to use string hashing and\\nthe fact that there are at most O(√m)distinct word lengths. Thus, we can restrict\\nourselves to word lengths that actually exist. This can be done by creating a set that\\ncontains all hash values of words, which results in an algorithm whose running time\\nisO(n√m+m)(using unordered_set ).\\n15.1.4 Mo’s Algorithm\\nMo’s algorithm1processes a set of range queries on a static array (i.e., the array values\\ndo not change between the queries). Each query requires us to calculate something\\nbased on the array values in a range [a,b]. Since the array is static, the queries can\\nbe processed in any order, and the trick in Mo’s algorithm is to use a special order\\nwhich guarantees that the algorithm works efﬁciently.\\nThe algorithm maintains an active range in the array, and the answer to a query\\nconcerning the active range is known at each moment. The algorithm processes the\\n1According to [ 5], Mo’s algorithm is named after Mo Tao, a Chinese competitive programmer.', metadata={'source': 'documents\\\\doc.pdf', 'page': 244}),\n",
       " Document(page_content='15.1 Square Root Techniques 245\\nFig. 15.8 Moving between\\ntwo ranges in Mo’s\\nalgorithm42 5 424 33 4\\n42 5 424 33 4\\nqueries one by one and always moves the endpoints of the active range by inserting\\nand removing elements. The array is divided into blocks of k=O(√n)elements,\\nand a query [a1,b1]is always processed before a query [a2,b2]if\\n•⌊a1/k⌋<⌊a2/k⌋or\\n•⌊a1/k⌋=⌊ a2/k⌋andb1<b2.\\nThus, all queries whose left endpoints are in a certain block are processed one after\\nanother sorted according to their right endpoints. Using this order, the algorithm only\\nperforms O(n√n)operations, because the left endpoint moves O(n)times O(√n)\\nsteps, and the right endpoint moves O(√n)times O(n)steps. Thus, both endpoints\\nmove a total of O(n√n)steps during the algorithm.\\nExample Consider a problem where we are given a set of array ranges, and we are\\nasked to calculate the number of distinct values in each range. In Mo’s algorithm,\\nthe queries are always sorted in the same way, but the way the answer to the query\\nis maintained depends on the problem.\\nTo solve the problem, we maintain an array count where count [x]indicates\\nthe number of times an element xoccurs in the active range. When we move from\\none query to another query, the active range changes. For example, consider the two\\nranges in Fig. 15.8 . When we move from the ﬁrst range to the second range, there will\\nbe three steps: the left endpoint moves one step to the right, and the right endpoint\\nmoves two steps to the right.\\nAfter each step, the array count needs to be updated. After adding an element\\nx, we increase the value of count [x]by 1, and if count [x]= 1 after this, we\\nalso increase the answer to the query by 1. Similarly, after removing an element\\nx, we decrease the value of count [x]by 1, and if count [x]= 0 after this, we\\nalso decrease the answer to the query by 1. Since each step requires O(1)time, the\\nalgorithm works in O(n√n)time.\\n15.2 Segment Trees Revisited\\nA segment tree is a versatile data structure that can be used to solve a large number\\nof problems. However, so far we have only seen a small part of the possibilities of\\nsegment trees. Now is time to discuss some more advanced variants of segment trees\\nthat allow us to solve more advanced problems.', metadata={'source': 'documents\\\\doc.pdf', 'page': 245}),\n",
       " Document(page_content='246 15 Additional Topics\\nUntil now, we have implemented the operations of a segment tree by walking\\nfrom bottom to top in the tree. For example, we have used the following function\\n(Sect. 9.2.2 ) to calculate the sum of values in a range [a,b]:\\nint sum( int a,int b) {\\na+ =n ;b+ =n ;\\nint s=0 ;\\nwhile (a <= b) {\\nif(a%2 == 1) s += tree[a++];\\nif(b%2 == 0) s += tree[b--];\\na/ =2 ;b/ =2 ;\\n}\\nreturn s;\\n}\\nHowever, in advanced segment trees, it is often necessary to implement the oper-\\nations from top to bottom as follows:\\nint sum( int a,int b,int k,int x,int y) {\\nif( b<x| |a>y ) return 0;\\nif( a< =x& &y< =b ) return tree[k];\\nint d = (x+y)/2;\\nreturn sum(a,b,2*k,x,d) + sum(a,b,2*k+1,d+1,y);\\n}\\nUsing this function, we can calculate the sum in a range [a,b]as follows:\\nint s = sum(a,b,1,0,n-1);\\nThe parameter kindicates the current position in tree . Initially kequals 1,\\nbecause we begin at the root of the tree. The range [x,y]corresponds to kand is\\ninitially [0,n−1]. When calculating the sum, if [x,y]is outside [a,b], the sum is\\n0, and if [x,y]is completely inside [a,b], the sum can be found in tree .I f[x,y]\\nis partially inside [a,b], the search continues recursively to the left and right half of\\n[x,y]. The left half is [x,d], and the right half is [d+1,y], where d=⌊x+y\\n2⌋.\\nFigure 15.9 shows how the search proceeds when calculating the value of\\nsum q(a,b). The gray nodes indicate nodes where the recursion stops and the sum\\ncan be found in tree . Also in this implementation, operations take O(logn)time,\\nbecause the total number of visited nodes is O(logn).\\n15.2.1 Lazy Propagation\\nUsing lazy propagation , we can build a segment tree that supports both range updates\\nand range queries in O(logn)time. The idea is to perform updates and queries from\\ntop to bottom and perform updates lazily so that they are propagated down the tree\\nonly when it is necessary.', metadata={'source': 'documents\\\\doc.pdf', 'page': 246}),\n",
       " Document(page_content='15.2 Segment Trees Revisited 247\\n5863 2 7 2 6 7 1 7 56 2 3 213 9 9 8 8 12 8 522 17 20 1339 3372\\na b\\nFig. 15.9 Traversing a segment tree from top to bottom\\n5863 2 7 2 6 7 1 7 56 2 3 213/0 9/0 9/0 8/0 8/0 12/0 8/0 5/022/0 17/0 20/0 13/039/0 33/072/0\\nFig. 15.10 A lazy segment tree for range updates and queries\\nThe nodes of a lazy segment tree contain two types of information. Like in an\\nordinary segment tree, each node contains the sum, minimum value, or some other\\nvalue related to the corresponding subarray. In addition, a node may contain informa-\\ntion about a lazy update which has not been propagated to its children. Lazy segment\\ntrees can support two types of range updates: each array value in the range is either\\nincreased by some value or assigned some value. Both operations can be imple-\\nmented using similar ideas, and it is even possible to construct a tree that supports\\nboth operations at the same time.\\nLet us consider an example where our goal is to construct a segment tree that\\nsupports two operations: increasing each value in [a,b]by a constant and calculating\\nthe sum of values in [a,b]. To achieve this goal, we construct a tree where each node\\nhas two values s/z:sdenotes the sum of values in the range, and zdenotes the value\\nof a lazy update, which means that all values in the range should be increased by z.\\nFigure 15.10 shows an example of such a tree, where z=0 in all nodes, meaning\\nthat there are no ongoing lazy updates.', metadata={'source': 'documents\\\\doc.pdf', 'page': 247}),\n",
       " Document(page_content='248 15 Additional Topics\\n5863 2 9 2 6 7 1 7 56 2 3 213/0 9/0 11/0 8/2 8/0 12/0 8/2 5/022/0 23/0 20/2 17/045/0 45/090/0\\na b\\nFig. 15.11 Increasing the values in the range [a,b]by 2\\n5863 2 9 2 6 7 1 7 56 2 3 213/0 9/0 11/0 8/2 8/2 12/2 8/2 5/022/0 23/0 28/0 17/045/0 45/090/0\\na b\\nFig. 15.12 Calculating the sum of values in the range [a,b]\\nWe implement the tree operations from top to bottom. To increase the values in\\na range [a,b]byu, we modify the nodes as follows: If the range [x,y]of a node is\\ncompletely inside [a,b], we increase the zvalue of the node by uand stop. Then, if\\n[x,y]partially belongs to [a,b], we continue our walk recursively in the tree, and\\nafter this calculate the new svalue for the node. As an example, Fig. 15.11 shows\\nour tree after increasing the range [a,b]by 2.\\nIn both updates and queries, lazy updates are propagated downwards when we\\nmove in the tree. Always before accessing a node, we check if it has an ongoing\\nlazy update. If it has, we update its svalue, propagate the update to its children, and\\nthen clear its zvalue. For example, Fig. 15.12 shows how our tree changes when we\\ncalculate the value of sum a(a,b). The rectangle contains the nodes whose values\\nchange when a lazy update is propagated downwards.', metadata={'source': 'documents\\\\doc.pdf', 'page': 248}),\n",
       " Document(page_content='15.2 Segment Trees Revisited 249\\nPolynomial Updates We can generalize the above segment tree so that it is possible\\nto update ranges using polynomials of the form\\np(u)=tkuk+tk−1uk−1+···+ t0.\\nIn this case, the update for a value at position iin[a,b]isp(i−a). For example,\\nadding the polynomial p(u)=u+1t o[a,b]means that the value at position a\\nincreases by 1, the value at position a+1 increases by 2, and so on.\\nTo support polynomial updates, each node is assigned k+2 values, where kequals\\nthe degree of the polynomial. The value sis the sum of the elements in the range,\\nand the values z0,z1,..., zkare the coefﬁcients of a polynomial that corresponds to\\na lazy update. Now, the sum of values in a range [x,y]equals\\ns+y−x∑\\nu=0(zkuk+zk−1uk−1+···+ z1u+z0),\\nand the value of such a sum can be efﬁciently calculated using sum formulas. For\\nexample, the term z0corresponds to the sum z0(y−x+1), and the term z1ucorre-\\nsponds to the sum\\nz1(0+1+···+ y−x)=z1(y−x)(y−x+1)\\n2.\\nWhen propagating an update in the tree, the indices of p(u)change, because in\\neach range [x,y], the values are calculated for u=0,1,..., y−x. However, we\\ncan easily handle this, because p′(u)=p(u+h)is a polynomial of equal degree as\\np(u). For example, if p(u)=t2u2+t1u+t0, then\\np′(u)=t2(u+h)2+t1(u+h)+t0=t2u2+(2ht2+t1)u+t2h2+t1h+t0.\\n15.2.2 Dynamic Trees\\nAn ordinary segment tree is static, which means that each node has a ﬁxed position\\nin the segment tree array and the structure requires a ﬁxed amount of memory. In a\\ndynamic segment tree , memory is allocated only for nodes that are actually accessed\\nduring the algorithm, which can save a large amount of memory.\\nThe nodes of a dynamic tree can be represented as structs:\\nstruct node {\\nint value;\\nint x, y;\\nnode *left, *right;\\nnode( int v,int x,int y) : value(v), x(x), y(y) {}\\n};', metadata={'source': 'documents\\\\doc.pdf', 'page': 249}),\n",
       " Document(page_content='250 15 Additional Topics\\nFig. 15.13 As p a r s e\\nsegment tree where the\\nelements at positions 3 and\\n10 have been modiﬁed[0,15]\\n[0,7]\\n[0,3]\\n[2,3]\\n[3,3][8,15]\\n[8,11]\\n[10,11]\\n[10,10]\\nHerevalue is the value of the node, [x,y]is the corresponding range, and left\\nandright point to the left and right subtree. Nodes can be created as follows:\\n// create a node with value 2 and range [0,7]\\nnode *x = new node(2,0,7);\\n// change value\\nx->value = 5;\\nSparse Segment Trees A dynamic segment tree is a useful structure when the under-\\nlying array is sparse , i.e., the range [0,n−1]of allowed indices is large, but most\\narray values are zeros. While an ordinary segment tree would use O(n)memory,\\na dynamic segment tree only uses O(klogn)memory, where kis the number of\\noperations performed.\\nAsparse segment tree initially has only one node [0,n−1]whose value is zero,\\nwhich means that every array value is zero. After updates, new nodes are dynamically\\nadded to the tree. Any path from the root node to a leaf contains O(logn)nodes, so\\neach segment tree operation adds at most O(logn)new nodes to the tree. Thus, after\\nkoperations, the tree contains O(klogn)nodes. For example, Fig. 15.13 shows a\\nsparse segment tree where n=16, and the elements at positions 3 and 10 have been\\nmodiﬁed.\\nNote that if we know all elements that will be updated during the algorithm\\nbeforehand, a dynamic segment tree is not necessary, because we can use an ordinary\\nsegment tree with index compression (Sect. 9.2.3 ). However, this is not possible when\\nthe indices are generated during the algorithm.\\nPersistent Segment Trees Using a dynamic implementation, we can also create a\\npersistent segment tree that stores the modiﬁcation history of the tree. In such an\\nimplementation, we can efﬁciently access all versions of the tree that have existed\\nduring the algorithm. When the modiﬁcation history is available, we can perform\\nqueries in any previous tree like in an ordinary segment tree, because the full structure', metadata={'source': 'documents\\\\doc.pdf', 'page': 250}),\n",
       " Document(page_content='15.2 Segment Trees Revisited 251\\nstep 1 step 2 step 3\\nFig. 15.14 A modiﬁcation history of a segment tree: the initial tree and two updates\\nstep 1 step 2 step 3\\nFig. 15.15 A compact way to store the modiﬁcation history\\nof each tree is stored. We can also create new trees based on previous trees and modify\\nthem independently.\\nConsider the sequence of updates in Fig. 15.14 , where marked nodes change and\\nother nodes remain the same. After each update, most nodes of the tree remain the\\nsame, so a compact way to store the modiﬁcation history is to represent each historical\\ntree as a combination of new nodes and subtrees of previous trees. Figure 15.15 shows\\nhow the modiﬁcation history can be stored. The structure of each previous tree can\\nbe reconstructed by following the pointers starting at the corresponding root node.\\nSince each operation adds only O(logn)new nodes to the tree, it is possible to store\\nthe full modiﬁcation history of the tree.\\n15.2.3 Data Structures in Nodes\\nInstead of single values, the nodes of a segment tree can also contain data structures\\nthat maintain information about the corresponding ranges. As an example, suppose\\nthat we should be able to efﬁciently count the number of occurrences of an element\\nxin a range [a,b]. To do this, we can create a segment tree where each node is\\nassigned a data structure that can be asked how many times any element xappears\\nin the corresponding range. After this, the answer to a query can be calculated by\\ncombining the results from nodes that belong to the range.\\nThe remaining task is to choose a suitable data structure for the problem. A good\\nchoice is a map structure whose keys are array elements and values indicate how\\nmany times each element occurs in a range. Figure 15.16 shows an array and the\\ncorresponding segment tree. For example, the root node of the tree tells us that\\nelement 1 appears 4 times in the array.', metadata={'source': 'documents\\\\doc.pdf', 'page': 251}),\n",
       " Document(page_content='252 15 Additional Topics\\n3 12 3 1112\\n3\\n11\\n12\\n13\\n11\\n11\\n11\\n12\\n113\\n1123\\n111\\n212\\n11123\\n11212\\n31123\\n422\\nFig. 15.16 A segment tree for calculating the number of occurrences of an element in an array\\nrange\\n7\\n6\\n1\\n68\\n7\\n5\\n23\\n9\\n7\\n18\\n5\\n3\\n8\\n761613 720\\n875215 722\\n397112 820\\n853813 112415 13 6 828 1442\\n11 14 10925 19442627161753 3386\\nFig. 15.17 A two-dimensional array and the corresponding segment tree for calculating sums of\\nrectangular subarrays', metadata={'source': 'documents\\\\doc.pdf', 'page': 252}),\n",
       " Document(page_content='15.2 Segment Trees Revisited 253\\nEach query in the above segment tree works in O(log2n)time, because each node\\nhas a map structure whose operations take O(logn)time. The tree uses O(nlogn)\\nmemory, because it has O(logn)levels, and each level contains nelements that have\\nbeen distributed in the map structures.\\n15.2.4 Two-Dimensional Trees\\nAtwo-dimensional segment tree allows us to process queries related to rectangular\\nsubarrays on a two-dimensional array. The idea is to create a segment tree that\\ncorresponds to the columns of the array and then assign each node of this structure\\na segment tree that corresponds to the rows of the array.\\nFor example, Fig. 15.17 shows a two-dimensional segment tree that supports two\\nqueries: calculating the sum of values in a subarray and updating a single array value.\\nBoth the queries take O(log2n)time, because O(logn)nodes in the main segment\\ntree are accessed, and processing each node takes O(logn)time. The structure uses\\na total of O(n2)memory, because the main segment tree has O(n)nodes, and each\\nnode has a segment tree of O(n)nodes.\\n15.3 Treaps\\nAtreap is a binary tree that can store the contents of an array in such a way that we\\ncan efﬁciently split an array into two arrays and merge two arrays into an array. Each\\nnode in a treap has two values: a weight and a value . Each node’s weight is smaller\\nor equal than the weights of its children, and the node is located in the array after all\\nnodes in its left subtree and before all nodes in its right subtree.\\nFigure 15.18 shows an example of an array and the corresponding treap. For\\nexample, the root node has weight 1 and value D. Since its left subtree contains three\\nnodes, this means that the array element at position 3 has value D.\\n15.3.1 Splitting and Merging\\nWhen a new node is added to the treap, it is assigned a random weight. This guar-\\nantees that the tree is balanced (its height is O(logn)) with high probability, and its\\noperations can be performed efﬁciently.\\nSplitting The splitting operation of a treap creates two treaps which divide the array\\ninto two arrays so that the ﬁrst kelements belong to the ﬁrst array and the rest of\\nthe elements belong to the second array. To do this, we create two new treaps that\\nare initially empty and traverse the original treap starting at the root node. At each', metadata={'source': 'documents\\\\doc.pdf', 'page': 253}),\n",
       " Document(page_content='254 15 Additional Topics\\nFig. 15.18 An array and the\\ncorresponding treapSA N D W IC H0 12 3 4 56 7\\n9S3A\\n6N1D\\n8W\\n9I4C\\n5H\\nFig. 15.19 Splitting an array\\ninto two arraysSA N D W IC H0 12 3 4 0 12\\n9S3A\\n6N1D\\n8W9I4C\\n5H\\nstep, if the current node belongs to the left treap, the node and its left subtree are\\nadded to the left treap and we recursively process its right subtree. Similarly, if the\\ncurrent node belongs to the right treap, the node and its right subtree are added to the\\nright treap and we recursively process its left subtree. Since the height of the treap\\nisO(logn), this operation works in O(logn)time.\\nFor example, Fig. 15.19 shows how to divide our example array into two arrays so\\nthat the ﬁrst array contains the ﬁrst ﬁve elements of the original array and the second\\narray contains the last three elements. First, node D belongs to the left treap, so we\\nadd node D and its left subtree to the left treap. Then, node C belongs to the right\\ntreap, and we add node C and its right subtree to the right treap. Finally, we add node\\nW to the left treap and node I to the right treap.\\nMerging The merging operation of two treaps creates a single treap that concatenates\\nthe arrays. The two treaps are processed simultaneously, and at each step, the treap\\nwhose root has the smallest weight is selected. If the root of the left treap has the\\nsmallest weight, the root and its left subtree are moved to the new treap and its right\\nsubtree becomes the new root of the left treap. Similarly, if the root of the right treap', metadata={'source': 'documents\\\\doc.pdf', 'page': 254}),\n",
       " Document(page_content='15.3 Treaps 255\\nFig. 15.20 Merging two\\narrays into an array, before\\nmergingIC H S A N D W0 12 0 12 3 4\\n9S3A\\n6N1D\\n8W9I4C\\n5H\\nFig. 15.21 Merging two\\narrays into an array, after\\nmergingI CHSA N D W0 12 3 4 56 7\\n9S3A\\n6N1D\\n8W\\n9I4C\\n5H\\nhas the smallest weight, the root and its right subtree are moved to the new treap and\\nits left subtree becomes the new root of the right treap. Since the height of the treap\\nisO(logn), this operation works in O(logn)time.\\nFor example, we may now swap the order of the two arrays in our example scenario\\nand then concatenate the arrays again. Figure 15.20 shows the arrays before merging,\\nand Fig. 15.21 shows the ﬁnal result. First, node D and its right subtree is added to\\nthe new treap. Then, node A and its right subtree become the left subtree of node\\nD. After this, node C and its left subtree become the left subtree of node A. Finally,\\nnode H and node S are added to the new treap.\\n15.3.2 Implementation\\nNext we will learn a convenient way to implement a treap. First, here is a struct that\\nstores a treap node:', metadata={'source': 'documents\\\\doc.pdf', 'page': 255}),\n",
       " Document(page_content='256 15 Additional Topics\\nstruct node {\\nnode *left, *right;\\nint weight, size, value;\\nnode( int v) {\\nleft = right = NULL;\\nweight = rand();\\nsize = 1;\\nvalue = v;\\n}\\n};\\nThe ﬁeld size contains the size of the subtree of the node. Since a node can be\\nNULL , the following function is useful:\\nint size(node *treap) {\\nif(treap == NULL) return 0;\\nreturn treap->size;\\n}\\nThe following function split implements the splitting operation. The function\\nrecursively splits the treap treap into treaps left andright so that the left treap\\ncontains the ﬁrst knodes and the right treap contains the remaining nodes.\\nvoid split(node *treap, node *&left, node *&right, int k) {\\nif(treap == NULL) {\\nleft = right = NULL;\\n}else {\\nif(size(treap->left) < k) {\\nsplit(treap->right, treap->right, right,\\nk-size(treap->left)-1);\\nleft = treap;\\n}else {\\nsplit(treap->left, left, treap->left, k);\\nright = treap;\\n}\\ntreap->size = size(treap->left)+size(treap->right)+1;\\n}\\n}\\nThen, the following function merge implements the merging operation. This\\nfunction creates a treap treap that contains ﬁrst the nodes of the treap left and\\nthen the nodes of the treap right .', metadata={'source': 'documents\\\\doc.pdf', 'page': 256}),\n",
       " Document(page_content='15.3 Treaps 257\\nvoid merge(node *&treap, node *left, node *right) {\\nif(left == NULL) treap = right;\\nelse if (right == NULL) treap = left;\\nelse {\\nif(left->weight < right->weight) {\\nmerge(left->right, left->right, right);\\ntreap = left;\\n}else {\\nmerge(right->left, left, right->left);\\ntreap = right;\\n}\\ntreap->size = size(treap->left)+size(treap->right)+1;\\n}\\n}\\nFor example, the following code creates a treap that corresponds to the array\\n[1,2,3,4]. Then it divides it into two treaps of size 2 and swaps their order to create\\na new treap that corresponds to the array [3,4,1,2].\\nnode *treap = NULL;\\nmerge(treap, treap, new node(1));\\nmerge(treap, treap, new node(2));\\nmerge(treap, treap, new node(3));\\nmerge(treap, treap, new node(4));\\nnode *left, *right;\\nsplit(treap, left, right, 2);\\nmerge(treap, right, left);\\n15.3.3 Additional Techniques\\nThe splitting and merging operations of treaps are very powerful, because we can\\nfreely “cut and paste” arrays in logarithmic time using them. Treaps can be also\\nextended so that they work almost like segment trees. For example, in addition to\\nmaintaining the size of each subtree, we can also maintain the sum of its values, the\\nminimum value, and so on.\\nOne special trick related to treaps is that we can efﬁciently reverse an array. This\\ncan be done by swapping the left and right child of each node in the treap. For\\nexample, Fig. 15.22 shows the result after reversing the array in Fig. 15.18 .T od o\\nthis efﬁciently, we can introduce a ﬁeld that indicates if we should reverse the subtree\\nof the node, and process swapping operations lazily.', metadata={'source': 'documents\\\\doc.pdf', 'page': 257}),\n",
       " Document(page_content='258 15 Additional Topics\\nFig. 15.22 Reversing an\\narray using a treapHC I W D N AS0 12 3 4 56 7\\n9S3A\\n6N1D\\n8W\\n9I4C\\n5H\\nFig. 15.23 The minimum\\nfunction value at point x=4\\nisf2(4)=16/3\\nf1f2f3f4\\nx=4\\n15.4 Dynamic Programming Optimization\\nThis section discusses techniques for optimizing dynamic programming solutions.\\nFirst, we focus on the convex hull trick, which can be used to efﬁciently ﬁnd minimum\\nvalues of linear functions. After this, we discuss two other techniques that are based\\non properties of cost functions.\\n15.4.1 Convex Hull Trick\\nTheconvex hull trick allows us to efﬁciently ﬁnd the minimum function value at a\\ngiven point xamong a set of nlinear functions of the form f(x)=ax+b. For exam-\\nple, Fig. 15.23 shows functions f1(x)=x+2,f2(x)=x/3+4,f3(x)=x/6+5,\\nand f4(x)=−x/4+7. The minimum value at point x=4i s f2(4)=16/3.\\nThe idea is to divide the x-axis into ranges where a certain function has the\\nminimum value. It turns out that each function will have at most one range, and we\\ncan store the ranges in a sorted list that will contain at most nranges. For example,', metadata={'source': 'documents\\\\doc.pdf', 'page': 258}),\n",
       " Document(page_content='15.4 Dynamic Programming Optimization 259\\nFig. 15.24 The ranges\\nwhere f1,f2,a n d f4have\\nthe minimum value\\nf1f2f3f4\\nf1 f2 f4\\nFig. 15.24 shows the ranges in our example scenario. First, f1has the minimum\\nvalue, then f2has the minimum value, and ﬁnally f4has the minimum value. Note\\nthat f3never has the minimum value.\\nGiven a list of ranges, we can ﬁnd the minimum function value at point xin\\nO(logn)time using binary search. For example, since point x=4 belongs to the\\nrange of f2in Fig. 15.24 , we immediately know that the minimum function value at\\npoint x=4i s f2(4)=16/3. Thus, we can process a set of kqueries in O(klogn)\\ntime. Moreover, if the queries are given in increasing order, we can process them in\\nO(k)time by just iterating through the ranges from left to right.\\nThen, how to determine the ranges? If the functions are given in decreasing order\\nof their slopes, we can easily ﬁnd the ranges, because we can maintain a stack that\\ncontains the ranges, and the amortized cost for processing each function is O(1).I f\\nthe functions are given in an arbitrary order, we need to use a more sophisticated set\\nstructure and processing each function takes O(logn)time.\\nExample Suppose that there are nconsecutive concerts. The ticket for concert i\\ncosts pieuros, and if we attend the concert, we get a discount coupon whose value\\nisdi(0<di<1). We can later use the coupon to buy a ticket for dipeuros where\\npis the original price. It is also known that di≥di+1for all consecutive concerts i\\nandi+1. We deﬁnitely want to attend the last concert, and we can also attend other\\nconcerts. What is the minimum total price for this?\\nWe can easily solve the problem using dynamic programming by calculating for\\neach concert ia value ui: the minimum price for attending concert iand possibly\\nsome previous concerts. A simple way to ﬁnd the optimal choice for the previous\\nconcert is to go through all previous concerts in O(n)time, which results in an O(n2)\\ntime algorithm. However, we can use the convex hull trick to ﬁnd the optimal choice\\ninO(logn)time and get an O(nlogn)time algorithm.\\nThe idea is to maintain a set of linear functions, which initially only contains the\\nfunction f(x)=x, which means that we do not have a discount coupon. To calculate\\nthe value uifor a concert, we ﬁnd a function fin our set that minimizes the value of\\nf(pi), which can be done in O(logn)time using the convex hull trick. Then, we add\\na function f(x)=dix+uito our set, and we can use it to attend another concert\\nlater. The resulting algorithm works in O(nlogn)time.', metadata={'source': 'documents\\\\doc.pdf', 'page': 259}),\n",
       " Document(page_content='260 15 Additional Topics\\nFig. 15.25 An optimal way\\nto divide a sequence into\\nthree blocks23122 34112 3456 78\\nNote that if it is additionally known that pi≤pi+1for all consecutive concerts\\niandi+1, we can solve the problem more efﬁciently in O(n)time, because we\\ncan process the ranges from left to right and ﬁnd each optimal choice in amortized\\nconstant time instead of using binary search.\\n15.4.2 Divide and Conquer Optimization\\nThedivide and conquer optimization can be applied to certain dynamic programming\\nproblems where a sequence s1,s2,..., snofnelements has to be divided into k\\nsubsequences of consecutive elements. A cost function cost (a,b)is given, which\\ndetermines the cost of creating a subsequence sa,sa+1,..., sb. The total cost of a\\ndivision is the sum of the individual costs of the subsequences, and our task is to ﬁnd\\na division that minimizes the total cost.\\nAs an example, suppose that we have a sequence of positive integers and\\ncost (a,b)=(sa+sa+1+···+ sb)2. Figure 15.25 shows an optimal way to divide\\na sequence into three subsequences using this cost function. The total cost of the divi-\\nsion is (2+3+1)2+(2+2+3)2+(4+1)2=110.\\nWe can solve the problem by deﬁning a function solve (i,j)which gives the\\nminimum total cost of dividing the ﬁrst ielements s1,s2,..., siinto jsubsequences.\\nClearly, solve (n,k)equals the answer to the problem. To calculate a value of\\nsolve (i,j), we have to ﬁnd a position 1 ≤p≤ithat minimizes the value of\\nsolve (p−1,j−1)+cost (p,i).\\nFor example, in Fig. 15.25 , an optimal choice for solve (8,3)isp=7. A simple\\nway to ﬁnd an optimal position is to check all positions 1 ,2,..., i, which takes\\nO(n)time. By calculating all values of solve (i,j)like this, we get a dynamic\\nprogramming algorithm that works in O(n2k)time. However, using the divide and\\nconquer optimization, we can improve the time complexity to O(nklogn).\\nThe divide and conquer optimization can be used if the cost function satisﬁes the\\nquadrangle inequality\\ncost (a,c)+cost (b,d)≤cost (a,d)+cost (b,c)\\nfor all a≤b≤c≤d. Letpos (i,j)denote the smallest position pthat minimizes\\nthe cost of a division for solve (i,j). If the above inequality holds, it is guaranteed\\nthatpos (i,j)≤pos (i+1,j)for all values of iandj, which allows us to calculate\\nthe values of solve (i,j)more efﬁciently.', metadata={'source': 'documents\\\\doc.pdf', 'page': 260}),\n",
       " Document(page_content='15.4 Dynamic Programming Optimization 261\\nThe idea is to create a function calc (j,a,b,x,y)that calculates all values of\\nsolve (i,j)fora≤i≤band a ﬁxed jusing the information that x≤pos (i,j)≤\\ny. The function ﬁrst calculates the value of solve (z,j)where z=⌊ (a+b)/2⌋.\\nThen it performs recursive calls calc (j,a,z−1,x,p)andcalc (j,z+1,b,p,y)\\nwhere p=pos (z,j). Here the fact that pos (i,j)≤pos (i+1,j)is used to limit\\nthe search range. To calculate all values of solve (i,j), we perform a function call\\ncalc (j,1,n,1,n)for each j=1,2,..., k. Since each such function call takes\\nO(nlogn)time, the resulting algorithm works in O(nklogn)time.\\nFinally, let us prove that the squared sum cost function in our example satisﬁes the\\nquadrangle inequality. Let sum (a,b)denote the sum of values in range [a,b], and\\nletx=sum (b,c),y=sum (a,c)−sum (b,c), and z=sum (b,d)−sum (b,c).\\nUsing this notation, the quadrangle inequality becomes\\n(x+y)2+(x+z)2≤(x+y+z)2+x2,\\nwhich is equal to\\n0≤2yz.\\nSince yandzare nonnegative values, this completes the proof.\\n15.4.3 Knuth’s Optimization\\nKnuth’s optimization2can be used in certain dynamic programming problems where\\nwe are asked to divide a sequence s1,s2,..., snofnelements into single elements\\nusing splitting operations. A cost function cost (a,b)gives the cost of processing a\\nsequence sa,sa+1,..., sb, and our task is to ﬁnd a solution that minimizes the total\\nsum of the splitting costs.\\nFor example, suppose that cost (a,b)=sa+sa+1+···+ sb. Figure 15.26\\nshows an optimal way to process a sequence in this case. The total cost of this\\nsolution is 19 +9+10+5=43.\\nWe can solve the problem by deﬁning a function solve (i,j)which gives the\\nminimum cost of dividing the sequence si,si+1,..., sjinto single elements. Then,\\nsolve (1,n)gives the answer to the problem. To determine a value of solve (i,j),\\nwe have to ﬁnd a position i≤p<jthat minimizes the value of\\ncost (i,j)+solve (i,p)+solve (p+1,j).\\nIf we check all positions between iandj, we get a dynamic programming algorithm\\nthat works in O(n3)time. However, using Knuth’s optimization, we can calculate\\nthe values of solve (i,j)more efﬁciently in O(n2)time.\\n2Knuth [ 20] used his optimization to construct optimal binary search trees; later, Yao [ 32] general-\\nized the optimization to other similar problems.', metadata={'source': 'documents\\\\doc.pdf', 'page': 261}),\n",
       " Document(page_content='262 15 Additional Topics\\nFig. 15.26 An optimal way\\nto divide an array into single\\nelements2 7 3 2 5 cost: 19\\n2 7 3 2 5 cost: 9\\n2 7 3 2 5 cost: 10\\n2 7 3 2 5 cost: 5\\n2 7 3 2 5\\nKnuth’s optimization is applicable if\\ncost (b,c)≤cost (a,d)\\nand\\ncost (a,c)+cost (b,d)≤cost (a,d)+cost (b,c)\\nfor all values of a≤b≤c≤d. Note that the latter inequality is the quadrangle\\ninequality that was also used in the divide and conquer optimization. Let pos (i,j)\\ndenote the smallest position pthat minimizes the cost for solve (i,j). If the above\\ninequalities hold, we know that\\npos (i,j−1)≤pos (i,j)≤pos (i+1,j).\\nNow we can perform nrounds 1 ,2,..., n, and on round kcalculate the values of\\nsolve (i,j)where j−i+1=k, i.e., we process the subsequences in increasing\\norder of length. Since we know that pos (i,j)has to be between pos (i,j−1)\\nandpos (i+1,j), we can perform each round in O(n)time, and the total time\\ncomplexity of the algorithm becomes O(n2).\\n15.5 Miscellaneous\\nThis section presents a selection of miscellaneous algorithm design techniques. We\\ndiscuss the meet in the middle technique, a dynamic programming algorithm for\\ncounting subsets, the parallel binary search technique, and an ofﬂine solution to the\\ndynamic connectivity problem.', metadata={'source': 'documents\\\\doc.pdf', 'page': 262}),\n",
       " Document(page_content='15.5 Miscellaneous 263\\n15.5.1 Meet in the Middle\\nThemeet in the middle technique divides the search space into two parts of about\\nequal size, performs a separate search for both of the parts, and ﬁnally combines the\\nresults of the searches. Meet in the middle allows us to speed up certain O(2n)time\\nalgorithms so that they work in only O(2n/2)time. Note that O(2n/2)is much faster\\nthan O(2n), because 2n/2=√\\n2n. Using an O(2n)algorithm we can process inputs\\nwhere n≈20, but using an O(2n/2)algorithm the bound is n≈40.\\nSuppose that we are given a set of nintegers and our task is to determine whether\\nthe set has a subset with sum x. For example, given the set {2,4,5,9}andx=15,\\nwe can choose the subset {2,4,9}, because 2 +4+9=15. We can easily solve the\\nproblem in O(2n)time by going through every possible subset, but next we will\\nsolve the problem more efﬁciently in O(2n/2)time using meet in the middle.\\nThe idea is to divide our set into two sets AandBsuch that both sets contain about\\nhalf of the numbers. We perform two searches: the ﬁrst search generates all subsets\\nofAand stores their sums to a list SA, and the second search creates a similar list\\nSBforB. After this, it sufﬁces to check if we can choose one element from SAand\\nanother element from SBsuch that their sum is x, which is possible exactly when\\nthe original set contains a subset with sum x.\\nFor example, let us see how the set {2,4,5,9}is processed. First, we divide the\\nset into sets A={2,4}andB={5,9}. After this, we create lists SA=[0,2,4,6]\\nandSB=[0,5,9,14]. Since SAcontains the sum 6 and SBcontains the sum 9, we\\nconclude that the original set has a subset with sum 6 +9=15.\\nWith a good implementation, we can create the lists SAandSBinO(2n/2)time\\nin such a way that the lists are sorted. After this, we can use a two pointers algorithm\\nto check in O(2n/2)time if the sum xcan be created from SAandSB. Thus, the total\\ntime complexity of the algorithm is O(2n/2).\\n15.5.2 Counting Subsets\\nLetX={0...n−1}, and each subset S⊂Xis assigned an integer value [S]. Our\\ntask is to calculate for each S\\nsum (S)=∑\\nA⊂Svalue [A],\\ni.e., the sum of values of subsets of S.\\nFor example, suppose that n=3 and the values are as follows:\\n•value [∅] = 3\\n•value [{0}] = 1\\n•value [{1}] = 4\\n•value [{0,1}] = 5•value [{2}] = 5\\n•value [{0,2}] = 1\\n•value [{1,2}] = 3\\n•value [{0,1,2}] = 3', metadata={'source': 'documents\\\\doc.pdf', 'page': 263}),\n",
       " Document(page_content='264 15 Additional Topics\\nIn this case, for example,\\nsum ({0,2})=value [∅] + value [{0}] +value [{2}] +value [{0,2}]\\n=3+1+5+1=10.\\nNext we will see how to solve the problem in O(2nn)time using dynamic pro-\\ngramming and bit operations. The idea is to consider subproblems where it is limited\\nwhich elements may be removed from S.\\nLetpartial (S,k)denote the sum of values of subsets of Swith the restriction\\nthat only elements 0 ...kmay be removed from S. For example,\\npartial ({0,2},1)=value [{2}] +value [{0,2}],\\nbecause we only may remove elements 0 ...1. Note that we can calculate any value\\nofsum (S)using partial , because\\nsum (S)=partial (S,n−1).\\nTo use dynamic programming, we have to ﬁnd a recurrence for partial . First,\\nthe base cases are\\npartial (S,−1)=value [S],\\nbecause no elements can be removed from S. Then, in the general case we can\\ncalculate the values as follows:\\npartial (S,k)={\\npartial (S,k−1) k/∈S\\npartial (S,k−1)+partial (S\\\\{k},k−1)k∈S\\nHere we focus on the element k.I fk∈S, there are two options: we can either keep\\nkin the subset or remove it from the subset.\\nImplementation There is a particularly clever way to implement a dynamic pro-\\ngramming solution using bit operations. Namely we can declare an array\\nint sum[1<<N];\\nthat will contain the sum of each subset. The array is initialized as follows:\\nfor (int s = 0; s < (1<<n); s++) {\\nsum[s] = value[s];\\n}', metadata={'source': 'documents\\\\doc.pdf', 'page': 264}),\n",
       " Document(page_content='15.5 Miscellaneous 265\\nFig. 15.27 An instance of\\nthe road building problem1 2\\n3 4\\nday 11 2\\n3 4\\nday 2\\n1 2\\n3 4\\nday 31 2\\n3 4\\nday 4\\nThen, we can ﬁll the array as follows:\\nfor (int k=0 ;k<n ;k + + ){\\nfor (int s=0 ;s<( 1 < < n ) ;s + + ){\\nif(s&(1<<k)) sum[s] += sum[s^(1<<k)];\\n}\\n}\\nThis code calculates the values of partial (S,k)fork=0...n−1 to the array\\nsum . Since partial (S,k)is always based on partial (S,k−1), we can reuse\\nthe array sum , which yields a very efﬁcient implementation.\\n15.5.3 Parallel Binary Search\\nParallel binary search is a technique that allows us to make some binary search based\\nalgorithms more efﬁcient. The general idea is to perform several binary searches\\nsimultaneously, instead of doing the searches separately.\\nAs an example, consider the following problem: There are ncities numbered\\n1,2,..., n. Initially there are no roads between the cities. Then, during mdays, each\\nday a new road is built between two cities. Finally, we are given kqueries of the form\\n(a,b), and our task is to determine for each query the earliest moment when cities a\\nandbare connected. We can assume that all requested pairs of cities are connected\\nafter mdays.\\nFigure 15.27 shows an example scenario where there are four cities. Suppose that\\nthe queries are q1=(1,4)andq2=(2,3). The answer for q1is 2, because cities 1\\nand 4 are connected after day 2, and the answer for q2is 4, because cities 2 and 3\\nare connected after day 4.', metadata={'source': 'documents\\\\doc.pdf', 'page': 265}),\n",
       " Document(page_content='266 15 Additional Topics\\nLet us ﬁrst consider an easier problem where we have only one query (a,b).I n\\nthis case, we can use a union-ﬁnd structure to simulate the process of adding roads\\nto the network. After each new road, we check if cities aandbare connected and\\nstop the search if they are. Both adding a road and checking if cities are connected\\ntake O(logn)time, so the algorithm works in O(mlogn)time.\\nHow could we generalize this solution to kqueries? Of course we could process\\neach query separately, but such an algorithm would take O(kmlogn)time, which\\nwould be slow if both kandmare large. Next we will see how we can solve the\\nproblem more efﬁciently using parallel binary search.\\nThe idea is to assign each query a range [x,y]which means that the cities are\\nconnected for the ﬁrst time no earlier than after xdays and no later than after y\\ndays. Initially, each range is [1,m]. Then, we simulate log mtimes the process of\\nadding all roads to the network using a union-ﬁnd structure. For each query, we check\\nat moment u=⌊ (x+y)/2⌋if the cities are connected. If they are, the new range\\nbecomes [x,u], and otherwise the range becomes [u+1,y]. After log mrounds,\\neach range only contains a single moment which is the answer to the query.\\nDuring each round, we add mroads to the network in O(mlogn)time and check\\nwhether kpairs of cities are connected in O(klogn)time. Thus, since there are log m\\nrounds, the resulting algorithm works in O((m+k)lognlogm)time.\\n15.5.4 Dynamic Connectivity\\nSuppose that there is a graph of nnodes and medges. Then, we are given qqueries,\\neach of which is either “add an edge between nodes aandb” or “remove the edge\\nbetween nodes aandb.” Our task is to efﬁciently report the number of connected\\ncomponents in the graph after each query.\\nFigure 15.28 shows an example of the process. Initially, the graph has three com-\\nponents. Then, the edge 2–4 is added, which joins two components. After this, the\\nedge 4–5 is added and the edge 2–5 is removed, but the number of components\\nremains the same. Then, the edge 1–3 is added, which joins two components, and\\nﬁnally, the edge 2–4 is removed, which divides a component into two components.\\nIf edges would only be added to the graph, the problem would be easy to solve\\nusing a union-ﬁnd data structure, but the removal operations make the problem much\\nmore difﬁcult. Next we will discuss a divide and conquer algorithm for solving the\\nofﬂine version of the problem where all queries are known beforehand, and we are\\nallowed to report the results in any order. The algorithm presented here is based on\\nthe work by Kopeliovich [ 21].\\nThe idea is to create a timeline where each edge is represented by an interval\\nthat shows the insertion and removal time of the edge. The timeline spans a range\\n[0,q+1], and an edge that is added on step aand removed on step bis represented\\nby an interval [a,b]. If an edge belongs to the initial graph, a=0, and if an edge is\\nnever removed, b=q+1. Figure 15.29 shows the timeline in our example scenario.\\nTo process the intervals, we create a graph that has nnodes and no edges, and\\nuse a recursive function that is called with range [0,q+1]. The function works as', metadata={'source': 'documents\\\\doc.pdf', 'page': 266}),\n",
       " Document(page_content='15.5 Miscellaneous 267\\n1 2\\n3 45\\nthe initial graph\\nnumber of components: 31 2\\n3 45\\nstep 1: add ed ge2 – 4\\nnumber of components: 2\\n1 2\\n3 45\\nstep 2: add ed ge4 – 5\\nnumber of components: 21 2\\n3 45\\nstep 3: remove ed ge2 – 5\\nnumber of components: 2\\n1 2\\n3 45\\nstep 4: add ed ge1 – 3\\nnumber of components: 11 2\\n3 45\\nstep 5: remove ed ge2 – 4\\nnumber of components: 2\\nFig. 15.28 The dynamic connectivity problem\\nFig. 15.29 Timeline of edge\\ninsertions and removals0 12 3 4 56\\n1–2\\n2–5\\n2–4\\n4–5\\n1–3\\nfollows for a range [a,b]: First, if [a,b]is completely inside the interval of an edge,\\nand the edge does not belong to the graph, it is added to the graph. Then, if the\\nsize of [a,b] is 1, we report the number of connected components, and otherwise\\nwe recursively process ranges [a,k]and[k,b]where k=⌊ (a+b)/2⌋. Finally, we\\nremove all edges that were added at the beginning of processing the range [a,b].\\nAlways when an edge is added or removed, we also update the number of com-\\nponents. This can be done using a union-ﬁnd data structure, because we always\\nremove the edge that was added last. Thus, it sufﬁces to implement an undo oper-', metadata={'source': 'documents\\\\doc.pdf', 'page': 267}),\n",
       " Document(page_content='268 15 Additional Topics\\nation for the union-ﬁnd structure, which is possible by storing information about\\noperations in a stack. Since each edge is added and removed at most O(logq)times\\nand each operation works in O(logn)time, the total running time of the algorithm\\nisO((m+q)logqlogn).\\nNote that in addition to counting the number of components, we may maintain\\nany information that can combined with the union-ﬁnd data structure. For example,\\nwe may maintain the number of nodes in the largest component or the bipartiteness\\nof each component. The technique can also be generalized to other data structures\\nthat support insertion and undo operations.', metadata={'source': 'documents\\\\doc.pdf', 'page': 268}),\n",
       " Document(page_content='Sum Formulas\\nEach sum of the form\\nn∑\\nx=1xk=1k+2k+3k+···+ nk,\\nwhere kis a positive integer has a closed-form formula that is a polynomial of degree\\nk+1. For example,1\\nn∑\\nx=1x=1+2+3+···+ n=n(n+1)\\n2\\nand\\nn∑\\nx=1x2=12+22+32+...+n2=n(n+1)(2n+1)\\n6.\\nAnarithmetic progression is a sequence of numbers where the difference between\\nany two consecutive numbers is constant. For example,\\n3,7,11,15\\nis an arithmetic progression with constant 4. The sum of an arithmetic progression\\ncan be calculated using the formula\\na+···+ b\\ued19\\ued18\\ued17\\ued1a\\nnnumbers=n(a+b)\\n2\\n1There is even a general formula for such sums, called Faulhaber’s formula , but it is too complex\\nto be presented here.Mathematical Background', metadata={'source': 'documents\\\\doc.pdf', 'page': 269}),\n",
       " Document(page_content='270 Appendix A: Mathematical Background\\nwhere ais the ﬁrst number, bis the last number, and nis the amount of numbers.\\nFor example,\\n3+7+11+15=4·(3+15)\\n2=36.\\nThe formula is based on the fact that the sum consists of nnumbers and the value of\\neach number is (a+b)/2 on average.\\nAgeometric progression is a sequence of numbers where the ratio between any\\ntwo consecutive numbers is constant. For example,\\n3,6,12,24\\nis a geometric progression with constant 2. The sum of a geometric progression can\\nbe calculated using the formula\\na+ak+ak2+···+ b=bk−a\\nk−1\\nwhere ais the ﬁrst number, bis the last number, and the ratio between consecutive\\nnumbers is k. For example,\\n3+6+12+24=24·2−3\\n2−1=45.\\nThis formula can be derived as follows. Let\\nS=a+ak+ak2+···+ b.\\nBy multiplying both sides by k, we get\\nkS=ak+ak2+ak3+···+ bk,\\nand solving the equation\\nkS−S=bk−a\\nyields the formula.\\nA special case of a sum of a geometric progression is the formula\\n1+2+4+8+···+ 2n−1=2n−1.\\nAharmonic sum is a sum of the form\\nn∑\\nx=11\\nx=1+1\\n2+1\\n3+···+1\\nn.', metadata={'source': 'documents\\\\doc.pdf', 'page': 270}),\n",
       " Document(page_content='Appendix A: Mathematical Background 271\\nAn upper bound for a harmonic sum is log2(n)+1. Namely, we can modify each\\nterm 1 /kso that kbecomes the nearest power of two that does not exceed k.F o r\\nexample, when n=6, we can estimate the sum as follows:\\n1+1\\n2+1\\n3+1\\n4+1\\n5+1\\n6≤1+1\\n2+1\\n2+1\\n4+1\\n4+1\\n4.\\nThis upper bound consists of log2(n)+1 parts (1, 2 ·1/2, 4·1/4, etc.), and the value\\nof each part is at most 1.\\nSets\\nAsetis a collection of elements. For example, the set\\nX={2,4,7}\\ncontains elements 2, 4, and 7. The symbol ∅denotes an empty set, and |S|denotes\\nthe size of a set S, i.e., the number of elements in the set. For example, in the above\\nset,|X|= 3. If a set Scontains an element x, we write x∈S, and otherwise we\\nwrite x/∈S. For example, in the above set, 4 ∈Xand 5 /∈X.\\nNew sets can be constructed using set operations:\\n•Theintersection A ∩Bconsists of elements that are in both AandB. For example,\\nifA={1,2,5}andB={2,4}, then A∩B={2}.\\n•Theunion A ∪Bconsists of elements that are in AorBor both. For example, if\\nA={3,7}andB={2,3,8}, then A∪B={2,3,7,8}.\\n•Thecomplement ¯Aconsists of elements that are not in A. The interpretation of a\\ncomplement depends on the universal set , which contains all possible elements.\\nFor example, if A={ 1,2,5,7}and the universal set is {1,2,..., 10}, then\\n¯A={3,4,6,8,9,10}.\\n•Thedifference A \\\\B=A∩¯Bconsists of elements that are in Abut not in B. Note\\nthat Bcan contain elements that are not in A. For example, if A={2,3,7,8}\\nandB={3,5,8}, then A\\\\B={2,7}.\\nIf each element of Aalso belongs to S, we say that Ais asubset ofS, denoted by\\nA⊂S. A set Salways has 2|S|subsets, including the empty set. For example, the\\nsubsets of the set {2,4,7}are\\n∅,{2},{4},{7},{2,4},{2,7},{4,7}and{2,4,7}.\\nSome often used sets are N(natural numbers), Z(integers), Q(rational numbers),\\nand R(real numbers). The set Ncan be deﬁned in two ways, depending on the\\nsituation: either N={0,1,2,...}orN={1,2,3, ...}.', metadata={'source': 'documents\\\\doc.pdf', 'page': 271}),\n",
       " Document(page_content='272 Appendix A: Mathematical Background\\nTa b l e A. 1 Logical operators\\nA B ¬A ¬B A∧B A∨B A⇒B A⇔B\\n0 0 1 1 0 0 1 1\\n0 1 1 0 0 1 1 0\\n1 0 0 1 0 1 0 0\\n1 1 0 0 1 1 1 1\\nThere are several notations for deﬁning sets. For example,\\nA={2n:n∈Z}\\nconsists of all even integers, and\\nB={x∈R:x>2}\\nconsists of all real numbers that are greater than two.\\nLogic\\nThe value of a logical expression is either true (1) or false (0). The most important\\nlogical operators are ¬(negation ),∧(conjunction ),∨(disjunction ),⇒(implication ),\\nand⇔(equivalence ). Table A.1 shows the meanings of these operators.\\nThe expression ¬Ahas the opposite value of A. The expression A∧Bis true if\\nboth AandBare true, and the expression A∨Bis true if AorBor both are true.\\nThe expression A⇒Bis true if whenever Ais true, also Bis true. The expression\\nA⇔Bis true if AandBare both true or both false.\\nApredicate is an expression that is true or false depending on its parameters.\\nPredicates are usually denoted by capital letters. For example, we can deﬁne a pred-\\nicate P(x)that is true exactly when xis a prime number. Using this deﬁnition, P(7)\\nis true but P(8)is false.\\nAquantiﬁer connects a logical expression to the elements of a set. The most\\nimportant quantiﬁers are ∀(for all ) and∃(there is ). For example,\\n∀x(∃y(y<x))\\nmeans that for each element xin the set, there is an element yin the set such that\\nyis smaller than x. This is true in the set of integers, but false in the set of natural\\nnumbers.\\nUsing the notation described above, we can express many kinds of logical propo-\\nsitions. For example,\\n∀x((x>1∧¬P(x))⇒ (∃a(∃b(a>1∧b>1∧x=ab))))', metadata={'source': 'documents\\\\doc.pdf', 'page': 272}),\n",
       " Document(page_content='Appendix A: Mathematical Background 273\\nmeans that if a number xis larger than 1 and not a prime number, then there are\\nnumbers aandbthat are larger than 1 and whose product is x. This proposition is\\ntrue in the set of integers.\\nFunctions\\nThe function ⌊x⌋rounds the number xdown to an integer, and the function ⌈x⌉\\nrounds the number xup to an integer. For example,\\n⌊3/2⌋= 1 and⌈3/2⌉= 2.\\nThe functions min (x1,x2,..., xn)and max (x1,x2,..., xn)give the smallest and\\nlargest of values x1,x2,..., xn. For example,\\nmin (1,2,3)=1 and max (1,2,3)=3.\\nThefactorial n !can be deﬁned by\\nn∏\\nx=1x=1·2·3·...·n\\nor recursively\\n0!= 1\\nn!=n·(n−1)!\\nTheFibonacci numbers arise in many situations. They can be deﬁned recursively\\nas follows:\\nf(0)=0\\nf(1)=1\\nf(n)=f(n−1)+f(n−2)\\nThe ﬁrst Fibonacci numbers are\\n0,1,1,2,3,5,8,13,21,34,55,...\\nThere is also a closed-form formula for calculating Fibonacci numbers, which is\\nsometimes called Binet’s formula :\\nf(n)=(1+√\\n5)n−(1−√\\n5)n\\n2n√\\n5.', metadata={'source': 'documents\\\\doc.pdf', 'page': 273}),\n",
       " Document(page_content='274 Appendix A: Mathematical Background\\nLogarithms\\nThelogarithm of a number xis denoted logb(x), where bis the base of the logarithm.\\nIt is deﬁned so that logb(x)=aexactly when ba=x. The natural logarithm ln(x)\\nof a number xis a logarithm whose base is e≈2.71828.\\nA useful property of logarithms is that logb(x)equals the number of times we\\nhave to divide xbybbefore we reach the number 1. For example, log2(32)=5\\nbecause 5 divisions by 2 are needed:\\n32→16→8→4→2→1\\nThe logarithm of a product is\\nlogb(xy)=logb(x)+logb(y),\\nand consequently,\\nlogb(xn)=n·logb(x).\\nIn addition, the logarithm of a quotient is\\nlogb(x\\ny)\\n=logb(x)−logb(y).\\nAnother useful formula is\\nlogu(x)=logb(x)\\nlogb(u),\\nusing which it is possible to calculate logarithms to any base if there is a way to\\ncalculate logarithms to some ﬁxed base.\\nNumber Systems\\nUsually, numbers are written in base 10, which means that the digits 0 ,1,..., 9 are\\nused. However, there are also other number systems, like the base 2 binary system that\\nhas only two digits 0 and 1. In general, in a base bsystem, the integers 0 ,1,..., b−1\\nare used as digits.\\nWe can convert a base 10 number to base bby dividing the number by buntil it\\nbecomes zero. The remainders in reverse order correspond to the digits in base b.\\nFor example, let us convert the number 17 to base 3:\\n•17/3=5 (remainder 2)\\n•5/3=1 (remainder 2)\\n•1/3=0 (remainder 1)', metadata={'source': 'documents\\\\doc.pdf', 'page': 274}),\n",
       " Document(page_content='Appendix A: Mathematical Background 275\\nThus, the number 17 in base 3 is 122. Then, to convert a base bnumber to base 10,\\nit sufﬁces to multiply each digit by bk, where kis the zero-based position of the digit\\nstarting from the right, and sum the results together. For example, we can convert\\nthe base 3 number 122 back to base 10 as follows:\\n1·32+2·31+2·30=17\\nThe number of digits of an integer xin base bcan be calculated using the formula\\n⌊logb(x)+1⌋. For example, ⌊log3(17)+1⌋= 3.', metadata={'source': 'documents\\\\doc.pdf', 'page': 275}),\n",
       " Document(page_content='References\\n1. R. K. Ahuja, T. L. Magnanti, and J. B. Orlin. Network Flows: Theory, Algorithms, and Appli-\\ncations , Pearson, 1993.\\n2. A. M. Andrew. Another efﬁcient algorithm for convex hulls in two dimensions. Information\\nProcessing Letters , 9(5):216–219, 1979.\\n3. M. A. Bender and M. Farach-Colton. The LCA problem revisited. Latin American Symposium\\non Theoretical Informatics , 88–94, 2000.\\n4. J. Bentley and D. Wood. An optimal worst case algorithm for reporting intersections of rectan-\\ngles. IEEE Transactions on Computers , C-29(7):571–577, 1980.\\n5. Codeforces: On ”Mo’s algorithm”, http://codeforces.com/blog/entry/20032\\n6. T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to Algorithms ,M I T\\nPress, 2009 (3rd edition).\\n7. K. Diks et al. Looking for a Challenge? The Ultimate Problem Set from the University of Warsaw\\nProgramming Competitions , University of Warsaw, 2012.\\n8. D. Fanding. A faster algorithm for shortest-path – SPFA. Journal of Southwest Jiaotong Uni-\\nversity , 2, 1994.\\n9. P . M. Fenwick. A new data structure for cumulative frequency tables. Software: Practice and\\nExperience , 24(3):327–336, 1994.\\n10. J. Fischer and V . Heun. Theoretical and practical improvements on the RMQ-problem, with\\napplications to LCA and LCE. Annual Symposium on Combinatorial Pattern Matching , 36–48,\\n2006.\\n11. F. Le Gall. Powers of tensors and fast matrix multiplication. International Symposium on Sym-\\nbolic and Algebraic Computation , 296–303, 2014.\\n12. A. Grønlund and S. Pettie. Threesomes, degenerates, and love triangles. Annual Symposium on\\nFoundations of Computer Science , 621–630, 2014.\\n13. D. Gusﬁeld. Algorithms on Strings, Trees and Sequences: Computer Science and Computational\\nBiology , Cambridge University Press, 1997.\\n14. S. Halim and F. Halim. Competitive Programming 3: The New Lower Bound of Programming\\nContests , 2013.\\n15. The International Olympiad in Informatics Syllabus, https://people.ksp.sk/~misof/ioi-syllabus/', metadata={'source': 'documents\\\\doc.pdf', 'page': 276}),\n",
       " Document(page_content='278 References\\n16. J. Kärkkäinen and P . Sanders. Simple linear work sufﬁx array construction. International Col-\\nloquium on Automata, Languages, and Programming , 943–955, 2003.\\n17. R. M. Karp, R. E. Miller, and A. L. Rosenberg. Rapid identiﬁcation of repeated patterns in\\nstrings, trees and arrays. Annual ACM Symposium on Theory of Computing , 125–135, 1972.\\n18. T. Kasai, G. Lee, H. Arimura, S. Arikawa, and K. Park. Linear-time longest-common-preﬁx\\ncomputation in sufﬁx arrays and its applications. Annual Symposium on Combinatorial Pattern\\nMatching , 181–192, 2001.\\n19. J. Kleinberg and É. Tardos. Algorithm Design , Pearson, 2005.\\n20. D. E. Knuth. Optimum binary search trees. Acta Informatica 1(1):14–25, 1971.\\n21. S. Kopeliovich. Ofﬂine solution of connectivity and 2-edge-connectivity problems for fully\\ndynamic graphs. MSc thesis, Saint Petersburg State University, 2012.\\n22. M. G. Main and R. J. Lorentz. An O(nlogn)algorithm for ﬁnding all repetitions in a string.\\nJournal of Algorithms , 5(3):422–432, 1984.\\n23. J. Pachocki and J. Radoszewski. Where to use and how not to use polynomial string hashing.\\nOlympiads in Informatics , 7(1):90–100, 2013.\\n24. D. Pearson. A polynomial-time algorithm for the change-making problem. Operations Research\\nLetters , 33(3):231–234, 2005.\\n25. 27-Queens Puzzle: Massively Parallel Enumeration and Solution Counting. https://github.com/\\npreusser/q27\\n26. M. I. Shamos and D. Hoey. Closest-point problems. Annual Symposium on Foundations of\\nComputer Science , 151–162, 1975.\\n27. S. S. Skiena. The Algorithm Design Manual , Springer, 2008 (2nd edition).\\n28. S. S. Skiena and M. A. Revilla. Programming Challenges: The Programming Contest Training\\nManual , Springer, 2003.\\n29. D. D. Sleator and R. E. Tarjan. A data structure for dynamic trees. Journal of Computer and\\nSystem Sciences , 26(3):362–391, 1983.\\n30. P . Sta´ nczyk. Algorytmika praktyczna w konkursach Informatycznych. MSc thesis, University\\nof Warsaw, 2006.\\n31. V . Strassen. Gaussian elimination is not optimal. Numerische Mathematik , 13(4):354–356,\\n1969.\\n32. F. F. Yao. Efﬁcient dynamic programming using quadrangle inequalities. Annual ACM Sympo-\\nsium on Theory of Computing , 429–435, 1980.', metadata={'source': 'documents\\\\doc.pdf', 'page': 277}),\n",
       " Document(page_content='Index\\nSymbols\\n2SA T problem, 192\\n2SUM problem, 112\\n3SA T problem, 193\\nA\\nAdjacency list, 80\\nAdjacency matrix, 81\\nAmortized analysis, 111\\nAncestor, 137\\nAnd operation, 21\\nAndrew’s algorithm, 224\\nAntichain, 207\\nArithmetic progression, 1,269\\nArticulation point, 208\\nB\\nBacktracking, 18\\nBellman–Ford algorithm, 88\\nBiconnected graph, 207\\nBiconnectivity, 207\\nBinary indexed tree, 122\\nBinary search, 46\\nBinary search tree, 55\\nBinet’s formula, 5,273\\nBinomial coefﬁcient, 157\\nBinomial distribution, 177\\nBipartite graph, 80\\nBipartiteness check, 87\\nBirthday paradox, 231\\nBit mask, 22\\nBit representation, 20\\nBit shift, 22\\nBit-parallel algorithm, 107\\nBitset, 24\\nBorder, 234Breadth-First Search (BFS), 85\\nBridge, 208\\nBubble sort, 38\\nBurnside’s lemma, 163\\nC\\nCatalan number, 159\\nCayley’s formula, 164\\nCentroid, 144\\nCentroid decomposition, 144\\nChild, 131\\nChinese remainder theorem, 155\\nClosest pair, 221\\nCoin change problem, 63\\nCollatz conjecture, 5\\nCollision, 230\\nCombinatorics, 156\\nComparison function, 43\\nComparison operator, 42\\nComplement, 3,271\\nComplete graph, 79\\nComplex number, 211\\nComponent, 78\\nComponent graph, 189\\nConditional probability, 175\\nConjunction, 4,272\\nConnected graph, 78\\nConnectivity check, 86\\nConstant factor, 31\\nConstant-time algorithm, 30\\nConvex function, 116\\nConvex hull, 224\\nConvex hull trick, 258\\nCoprime, 153\\nCounting sort, 41\\nCross product, 213', metadata={'source': 'documents\\\\doc.pdf', 'page': 278}),\n",
       " Document(page_content='280 Index\\nCubic algorithm, 30\\nCut, 198\\nCycle, 78\\nCycle detection, 86,94,99\\nD\\nData structure, 51\\nDe Bruijn sequence, 196\\nDegree, 78\\nDepth-First Search (DFS), 83\\nDepth-ﬁrst search tree, 207\\nDeque, 54\\nDerangement, 162\\nDiameter, 134\\nDifference, 3,271\\nDifference array, 129\\nDijkstra’s algorithm, 89\\nDilworth’s theorem, 207\\nDiophantine equation, 155\\nDirected Acyclic Graph (DAG) 94\\nDirected graph, 78\\nDisjoint paths, 202\\nDisjunction, 4,272\\nDistance function, 218\\nDistribution, 177\\nDivide and conquer optimization, 260\\nDivisibility, 148\\nDivisor, 148\\nDynamic array, 51\\nDynamic connectivity, 266\\nDynamic programming, 63\\nDynamic programming optimization, 258\\nDynamic segment tree, 249\\nE\\nEdge, 78\\nEdge list, 82\\nEdit distance, 227\\nEdmonds–Karp algorithm, 200\\nEquivalence, 4,272\\nEuclid’s algorithm, 151\\nEuclidean distance, 218\\nEuler tour tree, 141\\nEuler’s theorem, 154\\nEuler’s totient function, 153\\nEulerian circuit, 194\\nEulerian path, 194\\nEulerian subgraph, 209\\nExpected value, 176\\nExtended Euclid’s algorithm, 152F\\nFactor, 148\\nFactorial, 5,273\\nFaulhaber’s formula, 1,269\\nFenwick tree, 122\\nFermat’s little theorem, 154\\nFibonacci number, 5,167,273\\nFloating point number, 13\\nFlow, 198\\nFloyd’s algorithm, 99\\nFloyd–Warshall algorithm, 92\\nFord–Fulkerson algorithm, 199\\nFunctional graph, 97\\nG\\nGame state, 181\\nGame theory, 181\\nGeometric distribution, 177\\nGeometric progression, 2,270\\nGeometry, 211\\nGraph, 78\\nGraph coloring, 180\\nGreatest common divisor, 151\\nGreedy algorithm, 45\\nGrundy number, 184\\nGrundy’s game, 186\\nH\\nHall’s theorem, 203\\nHamiltonian circuit, 195\\nHamiltonian path, 195\\nHamming distance, 107\\nHarmonic sum, 2,150,270\\nHash table, 55\\nHash value, 228\\nHashing, 228\\nHeap, 58\\nHeavy-Light decomposition, 145\\nHierholzer’s algorithm, 195\\nI\\nIdentity matrix, 166\\nImplication, 4,272\\nIn-order, 133\\nInclusion-exclusion, 161\\nIndegree, 79\\nIndependence, 175\\nIndependent set, 205\\nIndex compression, 128\\nInput and output, 10\\nInteger, 12', metadata={'source': 'documents\\\\doc.pdf', 'page': 279}),\n",
       " Document(page_content='Index 281\\nInteger partition, 243\\nIntersection, 3,271\\nIntersection point, 220\\nInversion, 38\\nIterator, 53\\nK\\nK˝onig’s theorem, 204\\nKnapsack, 71,243\\nKnight’s tour, 197\\nKnuth’s optimization, 261\\nKosaraju’s algorithm, 190\\nKruskal’s algorithm, 101\\nL\\nLas V egas algorithm, 179\\nLazy propagation, 246\\nLazy segment tree, 246\\nLCP array, 236\\nLeaf, 131\\nLevenshtein distance, 227\\nLine segment intersection, 214\\nLinear algorithm, 30\\nLinear recurrence, 167\\nLogarithm, 6,274\\nLogarithmic algorithm, 30\\nLogic, 4,272\\nLongest border, 234\\nLongest common subsequence, 227\\nLongest increasing subsequence, 69\\nLosing state, 181\\nLowest common ancestor, 140\\nLowest common multiple, 151\\nM\\nMacro, 14\\nManhattan distance, 218\\nMap, 57\\nMarkov chain, 178\\nMatching, 203\\nMatrix, 164\\nMatrix exponentiation, 167\\nMatrix multiplication, 165,180\\nMatrix sum, 165\\nMaximum ﬂow, 198\\nMaximum independent set, 205\\nMaximum matching, 203\\nMaximum spanning tree, 100\\nMaximum subarray sum, 32\\nMeet in the middle, 263\\nMemoization, 66Merge sort, 39\\nMex function, 184\\nMinimal rotation, 230\\nMinimum cut, 198,201\\nMinimum node cover, 204\\nMinimum spanning tree, 100\\nMisère game, 183\\nMo’s algorithm, 244\\nModular arithmetic, 12\\nModular exponentiation, 153\\nModular multiplicative inverse, 154\\nMonte Carlo algorithm, 179\\nMultinomial coefﬁcient, 158\\nMultiset, 57\\nN\\nNatural logarithm, 6,274\\nNearest smaller elements, 113\\nNegation, 4,272\\nNegative cycle, 89\\nNeighbor, 78\\nNim game, 182\\nNim sum, 183\\nNim theory, 181\\nNode, 78\\nNode cover, 204\\nNot operation, 21\\nNP-hard problem, 31\\nNumber theory, 147\\nO\\nOr operation, 21\\nOrder statistic, 179\\nOutdegree, 79\\nP\\nParallel binary search, 265\\nParent, 131\\nParenthesis expression, 159\\nPascal’s triangle, 157\\nPath, 78\\nPath compression, 105\\nPath cover, 205\\nPattern matching, 229,233\\nPerfect matching, 203\\nPermutation, 16\\nPersistent segment tree, 250\\nPick’s theorem, 218\\nPoint, 211\\nPoint in a polygon, 216\\nPoint location, 214', metadata={'source': 'documents\\\\doc.pdf', 'page': 280}),\n",
       " Document(page_content='282 Index\\nPoint-line distance, 215\\nPolicy-based set, 59\\nPolygon area, 216\\nPolynomial algorithm, 31\\nPolynomial hashing, 228\\nPost-order, 133\\nPrüfer code, 164\\nPre-order, 133\\nPredicate, 4,272\\nPreﬁx, 225\\nPreﬁx doubling method, 235\\nPreﬁx sum array, 120\\nPrim’s algorithm, 106\\nPrimality test, 148\\nPrime, 148\\nPrime decomposition, 148\\nPriority queue, 58\\nProbability, 173\\nProbability event, 174\\nQ\\nQuadrangle inequality, 260\\nQuadratic algorithm, 30\\nQuantiﬁer, 4,272\\nQueen problem, 18,35\\nQueue, 54\\nR\\nRandom variable, 175\\nRandomized algorithm, 179\\nRange, 53\\nRange query, 119\\nRange update, 129\\nReachability, 110\\nRecursion, 15\\nRegular graph, 79\\nRemainder, 12\\nRoot, 131\\nRooted tree, 131\\nRotating coordinates, 219\\nRotation, 230\\nS\\nScaling algorithm, 201\\nSegment tree, 125,245\\nSet, 3,55,271\\nShoelace formula, 216\\nShortest path, 87\\nSieve of Eratosthenes, 150Signed number, 20\\nSliding window, 114\\nSliding window minimum, 114\\nSorting, 37\\nSorting algorithm, 37\\nSpanning tree, 100\\nSparse segment tree, 250\\nSparse table algorithm, 121\\nSPFA algorithm, 89\\nSprague–Grundy theorem, 184\\nSquare matrix, 165\\nSquare root algorithm, 239\\nStack, 54\\nStrassen’s algorithm, 166\\nString hashing, 228\\nStrongly connected component, 189\\nStrongly connected graph, 189\\nSubalgorithm, 241\\nSubsequence, 225\\nSubset, 3,15,271\\nSubstring, 225\\nSubtree, 131\\nSuccessor, 97\\nSuccessor graph, 97\\nSufﬁx, 225\\nSufﬁx array, 234\\nSweep line algorithm, 44,220\\nT\\nTernary search, 115\\nTiling, 74\\nTime complexity, 27\\nTopological sorting, 94\\nTranspose, 165\\nTreap, 253\\nTree, 78,131\\nTree query, 137\\nTree traversal array, 138\\nTrie, 226\\nTwo pointers method, 111\\nTwo-dimensional segment tree, 253\\nU\\nUniform distribution, 177\\nUnion, 3,271\\nUnion-ﬁnd structure, 103\\nUniversal set, 3,271\\nUnsigned number, 20', metadata={'source': 'documents\\\\doc.pdf', 'page': 281}),\n",
       " Document(page_content='Index 283\\nV\\nV ector, 52,165,211\\nW\\nWarnsdorf’s rule, 197\\nWeighted graph, 78Winning state, 181\\nX\\nXor operation, 21\\nZ\\nZ-algorithm, 231\\nZ-array, 231', metadata={'source': 'documents\\\\doc.pdf', 'page': 282}),\n",
       " Document(page_content='', metadata={'source': 'documents\\\\Research Proposal Report.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=chunk_data(docs=doc)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x00000178F7DDFAC0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x00000178F6806350>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, http_client=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings=OpenAIEmbeddings(api_key=os.environ['OpenAI_API_KEY'])\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors=embeddings.embed_query(\"How are you\")\n",
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(\n",
    "    api_key=\"f95d2ada-e127-46bf-8028-d2dfc45da265\",\n",
    "    environment=\"gcp-starter\"\n",
    ")\n",
    "index_name=\"langchainvector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=Pinecone.from_documents(doc,embeddings,index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_query(query,k=2):\n",
    "    matching_results=index.similarity_search(query,k=k)\n",
    "    return matching_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(model_name=\"text-davinci-003\",temperature=0.5)\n",
    "chain=load_qa_chain(llm,chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_answers(query):\n",
    "    doc_search=retrieve_query(query)\n",
    "    print(doc_search)\n",
    "    response=chain.run(input_documents=doc_search,question=query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='48 4 Sorting and Searching\\nFig. 4.15 An alternative\\nway to implement binary\\nsearch. We scan the array\\nfrom left to right jumping\\nover elements\\nSecond Method Another way to implement binary search is to go through the array\\nfrom left to right making jumps . The initial jump length is n/2, and the jump length\\nis halved on each round: ﬁrst n/4, then n/8, then n/16, etc., until ﬁnally the length\\nis 1. On each round, we make jumps until we would end up outside the array or in\\nan element whose value exceeds the target value. After the jumps, either the desired\\nelement has been found or we know that it does not appear in the array. Figure 4.15\\nillustrates the technique in our example scenario.\\nThe following code implements the search:\\nint k=0 ;\\nfor (int b = n/2; b >= 1; b /= 2) {\\nwhile (k+b < n && array[k+b] <= x) k += b;\\n}\\nif(array[k] == x) {\\n// x found at index k\\n}\\nDuring the search, the variable bcontains the current jump length. The time\\ncomplexity of the algorithm is O(log n), because the code in the while loop is\\nperformed at most twice for each jump length.\\n4.3.2 Finding Optimal Solutions\\nSuppose that we are solving a problem and have a function valid (x)that returns\\ntrue ifxis a valid solution and false otherwise. In addition, we know that\\nvalid (x)isfalse when x<kandtrue when x≥k. In this situation, we can\\nuse binary search to efﬁciently ﬁnd the value of k.\\nThe idea is to binary search for the largest value of xfor which valid (x)is\\nfalse . Thus, the next value k=x+1 is the smallest possible value for which\\nvalid (k)istrue . The search can be implemented as follows:', metadata={'page': 55.0, 'source': 'documents\\\\doc.pdf'}), Document(page_content='46 4 Sorting and Searching\\nFig. 4.12 An optimal\\nschedule for the tasks\\nFig. 4.13 Improving the\\nsolution by swapping tasks\\nXand Y\\nFor example, suppose that the tasks are as follows:\\ntask duration deadline\\nA 42\\nB 31 0\\nC 28\\nD 41 5\\nFigure 4.12 shows an optimal schedule for the tasks in our example scenario.\\nUsing this schedule, Cyields 6 points, Byields 5 points, Ayields −7 points, and D\\nyields 2 points, so the total score is 6.\\nIt turns out that the optimal solution to the problem does not depend on the\\ndeadlines at all, but a correct greedy strategy is to simply perform the tasks sorted\\nby their durations in increasing order. The reason for this is that if we ever perform\\ntwo tasks one after another such that the ﬁrst task takes longer than the second task,\\nwe can obtain a better solution if we swap the tasks.\\nFor example, in Fig. 4.13 , there are two tasks Xand Ywith durations aand\\nb. Initially, Xis scheduled before Y. However, since a>b, the tasks should be\\nswapped. Now Xgives bpoints less and Ygives apoints more, so the total score\\nincreases by a−b>0. Thus, in an optimal solution, a shorter task must always\\ncome before a longer task, and the tasks must be sorted by their durations.\\n4.3 Binary Search\\nBinary search is an O(log n)time algorithm that can be used, for example, to efﬁ-\\nciently check whether a sorted array contains a given element. In this section, we\\nﬁrst focus on the implementation of binary search, and after that, we will see how\\nbinary search can be used to ﬁnd optimal solutions for problems.', metadata={'page': 53.0, 'source': 'documents\\\\doc.pdf'})]\n",
      " O(log n)\n"
     ]
    }
   ],
   "source": [
    "our_query=\"Time complexity of Binary search\"\n",
    "answer=retrieve_answers(our_query)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
